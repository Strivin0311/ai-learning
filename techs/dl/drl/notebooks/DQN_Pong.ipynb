{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93c730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "import collections\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba8924",
   "metadata": {},
   "source": [
    "### step0. load the original env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdd9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"ALE/Pong-v5\"\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "env = gym.make(env_name) # 1 means no frame skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155960e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x185a7ce2610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b806af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210, 160, 3), dtype('uint8'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample().shape, env.observation_space.sample().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7979b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c679fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6b1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy_func=None, step_limit=10000):\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    for step in range(step_limit):\n",
    "        if policy_func:\n",
    "            action = policy_func(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            env.close()\n",
    "            print(f\"Game over in {step+1} steps\")\n",
    "            return\n",
    "    env.close()\n",
    "    print(f\"Time reached limit as {step_limit} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f2dd3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time reached limit as 1000 steps\n"
     ]
    }
   ],
   "source": [
    "play(gym.make(env_name, render_mode=\"human\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e019f68",
   "metadata": {},
   "source": [
    "### step1. build the wrapped env to suit training nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdf7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipWrapper(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4, buffer_len=2):\n",
    "        \"\"\"return the max-pixel frame of the last 'buffer_len' frames only every 'skip' step\"\"\"\n",
    "        super(MaxAndSkipWrapper, self).__init__(env)\n",
    "        self.obs_buffer = collections.deque(maxlen=buffer_len)\n",
    "        self.skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        terminated, truncated = None, None\n",
    "        for _ in range(self.skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            self.obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self.obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obs_buffer.clear()\n",
    "        obs, info = self.env.reset()\n",
    "        self.obs_buffer.append(obs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b334341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetWrapper(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Skip the first FIRE action when to reset\"\"\"\n",
    "        super(FireResetWrapper, self).__init__(env)\n",
    "        # check the actions, about whether the second action is 'FIRE', \n",
    "        # where the first action is always 'NOOP'\n",
    "        # and check if there's another action more than these two\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "    def reset(self):\n",
    "        # directly pass through the first 'FIRE' action\n",
    "        self.env.reset()\n",
    "        obs, reward, terminated, truncated, info = self.env.step(1) # 'FIRE'\n",
    "        if terminated:\n",
    "            self.env.reset()\n",
    "        obs, reward, terminated, truncated, info = self.env.step(2) # 'RIGHT'\n",
    "        if terminated:\n",
    "            self.env.reset()\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b223b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameReshapeWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"reshape the img from (210,160,3) t0 (84,84,1)\"\"\"\n",
    "        super(FrameReshapeWrapper, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84,84,1), dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        \"\"\"change original obs to new one\"\"\"\n",
    "        img = obs.astype(np.float32)\n",
    "        img = img[:,:,0] * 0.299 + img[:,:,1] * 0.587 + img[:,:,2] * 0.114 # shape=(210,160,1)\n",
    "        img = cv2.resize(img, (84,110), interpolation=cv2.INTER_AREA)\n",
    "        img = img[18:102, :]\n",
    "        img = np.reshape(img, [84,84,1])\n",
    "        return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ded7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelTransWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"transpose the channel dim from 2 to 0\"\"\"\n",
    "        super(ChannelTransWrapper, self).__init__(env)\n",
    "        ori_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(ori_shape[-1], ori_shape[0], ori_shape[1]), # HWC => CHW\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return np.moveaxis(obs, 2, 0) # HWC => CHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8833e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None, n_steps=4):\n",
    "        \"\"\"extend the channel dim from C to C*n_steps to store n_steps consecutive frames\"\"\"\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box( # CHW => [C*n_steps]HW\n",
    "            env.observation_space.low.repeat(n_steps, axis=0),\n",
    "            env.observation_space.high.repeat(n_steps, axis=0),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
    "        obs, info = self.env.reset()\n",
    "        return self.observation(obs), info\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = obs\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf47901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeWrapper(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        \"\"\"normalize [0, 255] to [0.0, 1.0]\"\"\"\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8474ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wrapped_env(env_name, render_mode=None):\n",
    "    env = gym.make(env_name, frameskip=1, render_mode=render_mode)\n",
    "    # refine\n",
    "    env = MaxAndSkipWrapper(env)\n",
    "    env = FireResetWrapper(env)\n",
    "    # reshape\n",
    "    env = FrameReshapeWrapper(env)\n",
    "    env = ChannelTransWrapper(env)\n",
    "    # buffer \n",
    "    env = BufferWrapper(env)\n",
    "    # normalize\n",
    "    env = NormalizeWrapper(env)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fa8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env = make_wrapped_env(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0139f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 1.0, (4, 84, 84), float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18f5f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over in 901 steps\n"
     ]
    }
   ],
   "source": [
    "play(make_wrapped_env(env_name, render_mode=\"human\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80df78",
   "metadata": {},
   "source": [
    "### step2. build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c5c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, dueling=False):\n",
    "        super().__init__()\n",
    "        # convolutional layer\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_shape = self._get_conv_out_shape(input_shape) # flatten shape\n",
    "        # linear layer\n",
    "        self.dueling = dueling\n",
    "        if dueling: # Dueling DQN: Q(s,a) = V(s) + A(s,a)\n",
    "            self.fc_val = nn.Sequential( # V(s)\n",
    "                nn.Linear(conv_out_shape, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 1)\n",
    "            )\n",
    "            self.fc_adv = nn.Sequential( # A(s,a)\n",
    "                nn.Linear(conv_out_shape, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, n_actions)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(conv_out_shape, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, n_actions)\n",
    "            )\n",
    "        \n",
    "    def _get_conv_out_shape(self, input_shape):\n",
    "        # pseudoly pass the conv layer and get the output shape\n",
    "        conv_out = self.conv(torch.zeros(1, *input_shape))\n",
    "        return int(np.prod(conv_out.size()))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        \"\"\"output is the Q value of each action for this observation obs\"\"\"\n",
    "        conv_out = self.conv(obs)\n",
    "        fc_in = conv_out.view(obs.size()[0], -1) # flatten\n",
    "        if self.dueling:\n",
    "            val = self.fc_val(fc_in)\n",
    "            adv = self.fc_adv(fc_in)\n",
    "            return val + adv - adv.mean() # - adv.mean() to ensure advantage function has 0 mean value\n",
    "        else:\n",
    "            fc_out = self.fc(fc_in)\n",
    "        \n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0086fe9",
   "metadata": {},
   "source": [
    "### step3. build the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ca1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=[\n",
    "    'state', 'action', 'reward', 'done', 'next_state'\n",
    "])\n",
    "        \n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        \"\"\"The buffer to store the latest 'buffer_size' experiences to sample the training data\n",
    "        where one experience is a 5 elem tuple: (state, action, reward, is_done, next_state)\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, exp):\n",
    "        self.buffer.append(exp)\n",
    "    \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in idxs])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9620f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env_name, gamma=0.99, buffer_size=10000, device=\"cuda:0\", \n",
    "                 lr=1e-4, double=False, dueling=False,\n",
    "                 epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=1e5\n",
    "                ):\n",
    "        \"\"\"DQN Agent with epsilon greedy search policy\"\"\"\n",
    "        self.gamma, self.double, self.dueling = gamma, double, dueling\n",
    "        self.eps_start, self.eps_end, self.eps_decay = epsilon_start, epsilon_end, epsilon_decay\n",
    "        self.eps = self.eps_start\n",
    "        self.device = device\n",
    "        self.env = make_wrapped_env(env_name)\n",
    "        self.exp_buffer = ExperienceBuffer(buffer_size)\n",
    "        self.net, self.target_net = self._make_net(), self._make_net()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self._reset_env()\n",
    "        \n",
    "    def __str__(self):\n",
    "        if not self.double and not self.dueling:\n",
    "            return \"DQNAgent with basic DQN\"\n",
    "        if self.double and self.dueling:\n",
    "            return \"DQNAgent with Dueling Double DQN\"\n",
    "        if self.double:\n",
    "            return \"DQNAgent with Double DQN\"\n",
    "        if self.dueling:\n",
    "            return \"DQNAgent with Dueling DQN\"\n",
    "        \n",
    "        return \"DQNAgent with DQN\"\n",
    "        \n",
    "        \n",
    "    def _make_net(self):\n",
    "        input_shape = self.env.observation_space.shape\n",
    "        n_actions = self.env.action_space.n\n",
    "        \n",
    "        return DQN(input_shape, n_actions, dueling=self.dueling).to(self.device)\n",
    "        \n",
    "    def _reset_env(self):\n",
    "        self.state, info = self.env.reset()\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def update_epsilon(self, eps_idx):\n",
    "        self.eps = max(self.eps_end, self.eps_start - eps_idx/self.eps_decay)\n",
    "        \n",
    "    def play_step(self):\n",
    "        \"\"\"step the env using epsilon greedy search\n",
    "            and update the current state and append a new experience to buffer,\n",
    "            and if the episode ends, return the episode reward\"\"\"\n",
    "        epi_reward = None\n",
    "        \n",
    "        # get action using epsilon greedy search\n",
    "        action = self.policy(self.state)\n",
    "        # step the env\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        # append the experience to the buffer\n",
    "        self.exp_buffer.append(Experience(\n",
    "            self.state, action, reward, terminated, next_state\n",
    "        ))\n",
    "        # update the state\n",
    "        self.state = next_state\n",
    "        if terminated:\n",
    "            epi_reward = self.total_reward\n",
    "            self._reset_env()\n",
    "            \n",
    "        return epi_reward\n",
    "    \n",
    "    def cal_loss(self, batch):\n",
    "        # batch sampled data\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        states_v = torch.tensor(states).to(self.device) # shape = (batch, 4, 84, 84)\n",
    "        actions_v = torch.tensor(actions, dtype=torch.int64).to(self.device) # shape = (batch,)\n",
    "        rewards_v = torch.tensor(rewards).to(self.device) # shape = (batch,)\n",
    "        done_mask = torch.tensor(dones, dtype=torch.bool).to(self.device) # shape = (batch,)\n",
    "        next_states_v = torch.tensor(next_states).to(self.device) # shape = (batch, 4, 84, 84)\n",
    "        \n",
    "        # current q action values\n",
    "        q_action_values_pdf_v = self.net(states_v) # shape = (batch, n_actions)\n",
    "        q_action_values_v = q_action_values_pdf_v.gather( # shape = (batch,)\n",
    "            1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        # next q action values\n",
    "        if self.double: # double DQN\n",
    "            _, next_actions_v = self.net(next_states_v).max(1) # choose next action from the net\n",
    "            next_q_action_values_v = self.target_net(next_states_v).gather( # but use the value from the target net\n",
    "                1, next_actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        else: # basic DQN\n",
    "            next_q_action_values_v, _ = self.target_net(next_states_v).max(1) # use the best action values all from the target net\n",
    "        next_q_action_values_v[done_mask] = 0.0 # the terminated state has no next q action values\n",
    "        next_q_action_values_v = next_q_action_values_v.detach() # prevent to update the target net's parameter\n",
    "        \n",
    "        # calculate the Bellman MSE loss: [Q(s,a) - (r + max_a' Q(s',a'))]^2\n",
    "        target_action_values_v = rewards_v + self.gamma * next_q_action_values_v\n",
    "        loss = nn.MSELoss()(\n",
    "            q_action_values_v,\n",
    "            target_action_values_v\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def load_ckpt(self, ckpt_path):\n",
    "        self.net.load_state_dict(torch.load(ckpt_path, map_location=lambda storage, loc: storage))\n",
    "    \n",
    "    def save_ckpt(self, idx, reward):\n",
    "        save_path = os.path.join(\"ckpt\", str(idx) + \"_reward{:.2f}\".format(reward) +\".pth\")\n",
    "        torch.save(self.net.state_dict(), save_path)\n",
    "        \n",
    "    def sync(self):\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "        \n",
    "    def policy(self, state):\n",
    "        if np.random.rand() < self.eps:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state_v = torch.tensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values_v = self.net(state_v)\n",
    "            _, action_v = torch.max(q_values_v, dim=1)\n",
    "            action = int(action_v.item())\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28b7b9",
   "metadata": {},
   "source": [
    "### step4. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, agent, writer, reward_bound=16.0, replay_start_size=10000, sync_frames=1000):\n",
    "    total_rewards, frame_idx, ts_frame, ts, best_mean_reward = [], 0, 0, time.time(), None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # update the epsilon greedy rate\n",
    "        frame_idx += 1\n",
    "        agent.update_epsilon(frame_idx)\n",
    "        # let agent play a step to test the performance with getting some experience\n",
    "        reward = agent.play_step()\n",
    "        if reward: # the last step in an episode\n",
    "            # log and report\n",
    "            total_rewards.append(reward)\n",
    "            fps = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame, ts = frame_idx, time.time()\n",
    "            trained_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            print( # log\n",
    "                f\"frame {frame_idx}, epsiode {len(total_rewards)}, mean reward {mean_reward:.2f}, \" + \n",
    "                f\"eps {agent.eps:.2f}, fps {fps:.2f}, \" + \n",
    "                f\"trained time {trained_time}\"\n",
    "            ) # report\n",
    "            writer.add_scalar(\"epsilon\", agent.eps, frame_idx)\n",
    "            writer.add_scalar(\"fps\", fps, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "            # save checkpoint for the current best mean performance\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                agent.save_ckpt(frame_idx, mean_reward)\n",
    "                best_mean_reward = mean_reward\n",
    "            # check if solved with outperformance reward bound\n",
    "            if best_mean_reward > reward_bound:\n",
    "                print(f\"The agent '{str(agent)}' has solved the game '{env_name}' \" + \n",
    "                      f\"in {frame_idx} frames, {len(total_rewards)} episodes\"\n",
    "                )\n",
    "                break\n",
    "        \n",
    "        # check if the experience is enough to replay to train\n",
    "        if len(agent.exp_buffer) < replay_start_size:\n",
    "            continue\n",
    "        # check if it's time to sync target net with net\n",
    "        if frame_idx % sync_frames == 0:\n",
    "            agent.sync()\n",
    "        \n",
    "        # sample the experience batch to train\n",
    "        batch = agent.exp_buffer.sample()\n",
    "        # forward\n",
    "        agent.optimizer.zero_grad()\n",
    "        loss_v = agent.cal_loss(batch)\n",
    "        # backward\n",
    "        loss_v.backward()\n",
    "        # optimize\n",
    "        agent.optimizer.step()\n",
    "        # log loss\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), frame_idx)\n",
    "        \n",
    "    # close the resources \n",
    "    agent.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27158c09",
   "metadata": {},
   "source": [
    "#### 4.1 train the basic DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ea12efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][0]\n",
    "writer = SummaryWriter(comment=\"-\"+ str(agent)  + \"-\" + env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2895736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 899, epsiode 1, mean reward -21.00, eps 0.99, fps 730.69, trained time 00:00:01\n",
      "frame 1735, epsiode 2, mean reward -20.50, eps 0.98, fps 711.44, trained time 00:00:02\n",
      "frame 2585, epsiode 3, mean reward -20.67, eps 0.97, fps 698.01, trained time 00:00:03\n",
      "frame 3647, epsiode 4, mean reward -20.25, eps 0.96, fps 686.79, trained time 00:00:05\n",
      "frame 4527, epsiode 5, mean reward -20.40, eps 0.95, fps 679.26, trained time 00:00:06\n",
      "frame 5499, epsiode 6, mean reward -20.50, eps 0.95, fps 692.86, trained time 00:00:07\n",
      "frame 6413, epsiode 7, mean reward -20.43, eps 0.94, fps 677.04, trained time 00:00:09\n",
      "frame 7175, epsiode 8, mean reward -20.50, eps 0.93, fps 638.76, trained time 00:00:10\n",
      "frame 8046, epsiode 9, mean reward -20.56, eps 0.92, fps 595.82, trained time 00:00:11\n",
      "frame 8808, epsiode 10, mean reward -20.60, eps 0.91, fps 624.78, trained time 00:00:13\n",
      "frame 9630, epsiode 11, mean reward -20.64, eps 0.90, fps 612.59, trained time 00:00:14\n",
      "frame 10392, epsiode 12, mean reward -20.67, eps 0.90, fps 103.30, trained time 00:00:21\n",
      "frame 11234, epsiode 13, mean reward -20.69, eps 0.89, fps 55.36, trained time 00:00:37\n",
      "frame 12314, epsiode 14, mean reward -20.57, eps 0.88, fps 55.96, trained time 00:00:56\n",
      "frame 13433, epsiode 15, mean reward -20.47, eps 0.87, fps 55.59, trained time 00:01:16\n",
      "frame 14333, epsiode 16, mean reward -20.50, eps 0.86, fps 53.66, trained time 00:01:33\n",
      "frame 15343, epsiode 17, mean reward -20.53, eps 0.85, fps 56.46, trained time 00:01:51\n",
      "frame 16370, epsiode 18, mean reward -20.44, eps 0.84, fps 56.55, trained time 00:02:09\n",
      "frame 17282, epsiode 19, mean reward -20.47, eps 0.83, fps 56.71, trained time 00:02:25\n",
      "frame 18201, epsiode 20, mean reward -20.45, eps 0.82, fps 56.37, trained time 00:02:41\n",
      "frame 19173, epsiode 21, mean reward -20.48, eps 0.81, fps 58.31, trained time 00:02:58\n",
      "frame 20222, epsiode 22, mean reward -20.41, eps 0.80, fps 59.77, trained time 00:03:15\n",
      "frame 21012, epsiode 23, mean reward -20.43, eps 0.79, fps 59.24, trained time 00:03:29\n",
      "frame 21834, epsiode 24, mean reward -20.46, eps 0.78, fps 58.77, trained time 00:03:43\n",
      "frame 22922, epsiode 25, mean reward -20.48, eps 0.77, fps 55.87, trained time 00:04:02\n",
      "frame 23744, epsiode 26, mean reward -20.50, eps 0.76, fps 55.23, trained time 00:04:17\n",
      "frame 24655, epsiode 27, mean reward -20.52, eps 0.75, fps 56.08, trained time 00:04:33\n",
      "frame 25831, epsiode 28, mean reward -20.50, eps 0.74, fps 57.91, trained time 00:04:54\n",
      "frame 26678, epsiode 29, mean reward -20.52, eps 0.73, fps 59.20, trained time 00:05:08\n",
      "frame 27500, epsiode 30, mean reward -20.53, eps 0.72, fps 57.81, trained time 00:05:22\n",
      "frame 28540, epsiode 31, mean reward -20.48, eps 0.71, fps 56.27, trained time 00:05:41\n",
      "frame 29717, epsiode 32, mean reward -20.44, eps 0.70, fps 55.11, trained time 00:06:02\n",
      "frame 30679, epsiode 33, mean reward -20.45, eps 0.69, fps 55.37, trained time 00:06:19\n",
      "frame 31669, epsiode 34, mean reward -20.44, eps 0.68, fps 55.06, trained time 00:06:37\n",
      "frame 32510, epsiode 35, mean reward -20.46, eps 0.67, fps 54.31, trained time 00:06:53\n",
      "frame 33409, epsiode 36, mean reward -20.44, eps 0.67, fps 55.65, trained time 00:07:09\n",
      "frame 34510, epsiode 37, mean reward -20.38, eps 0.65, fps 54.99, trained time 00:07:29\n",
      "frame 35512, epsiode 38, mean reward -20.34, eps 0.64, fps 55.82, trained time 00:07:47\n",
      "frame 36523, epsiode 39, mean reward -20.33, eps 0.63, fps 58.59, trained time 00:08:04\n",
      "frame 37432, epsiode 40, mean reward -20.35, eps 0.63, fps 58.64, trained time 00:08:20\n",
      "frame 38828, epsiode 41, mean reward -20.27, eps 0.61, fps 56.61, trained time 00:08:44\n",
      "frame 40106, epsiode 42, mean reward -20.21, eps 0.60, fps 55.40, trained time 00:09:07\n",
      "frame 41212, epsiode 43, mean reward -20.23, eps 0.59, fps 57.61, trained time 00:09:27\n",
      "frame 42548, epsiode 44, mean reward -20.20, eps 0.57, fps 55.92, trained time 00:09:51\n",
      "frame 43563, epsiode 45, mean reward -20.22, eps 0.56, fps 56.36, trained time 00:10:09\n",
      "frame 44717, epsiode 46, mean reward -20.20, eps 0.55, fps 56.68, trained time 00:10:29\n",
      "frame 45953, epsiode 47, mean reward -20.17, eps 0.54, fps 56.43, trained time 00:10:51\n",
      "frame 47133, epsiode 48, mean reward -20.15, eps 0.53, fps 54.50, trained time 00:11:12\n",
      "frame 48448, epsiode 49, mean reward -20.14, eps 0.52, fps 54.64, trained time 00:11:37\n",
      "frame 49819, epsiode 50, mean reward -20.16, eps 0.50, fps 54.37, trained time 00:12:02\n",
      "frame 51075, epsiode 51, mean reward -20.16, eps 0.49, fps 53.79, trained time 00:12:25\n",
      "frame 52392, epsiode 52, mean reward -20.13, eps 0.48, fps 53.30, trained time 00:12:50\n",
      "frame 53757, epsiode 53, mean reward -20.11, eps 0.46, fps 55.46, trained time 00:13:14\n",
      "frame 55235, epsiode 54, mean reward -20.09, eps 0.45, fps 54.29, trained time 00:13:42\n",
      "frame 56726, epsiode 55, mean reward -20.09, eps 0.43, fps 53.01, trained time 00:14:10\n",
      "frame 58133, epsiode 56, mean reward -20.07, eps 0.42, fps 53.42, trained time 00:14:36\n",
      "frame 59635, epsiode 57, mean reward -20.05, eps 0.40, fps 55.11, trained time 00:15:03\n",
      "frame 60898, epsiode 58, mean reward -20.02, eps 0.39, fps 54.62, trained time 00:15:26\n",
      "frame 62731, epsiode 59, mean reward -19.93, eps 0.37, fps 54.15, trained time 00:16:00\n",
      "frame 64366, epsiode 60, mean reward -19.92, eps 0.36, fps 54.70, trained time 00:16:30\n",
      "frame 65746, epsiode 61, mean reward -19.90, eps 0.34, fps 53.92, trained time 00:16:56\n",
      "frame 66913, epsiode 62, mean reward -19.90, eps 0.33, fps 52.63, trained time 00:17:18\n",
      "frame 68617, epsiode 63, mean reward -19.87, eps 0.31, fps 52.43, trained time 00:17:50\n",
      "frame 70469, epsiode 64, mean reward -19.80, eps 0.30, fps 56.10, trained time 00:18:23\n",
      "frame 72041, epsiode 65, mean reward -19.78, eps 0.28, fps 57.06, trained time 00:18:51\n",
      "frame 74114, epsiode 66, mean reward -19.70, eps 0.26, fps 56.98, trained time 00:19:27\n",
      "frame 75659, epsiode 67, mean reward -19.70, eps 0.24, fps 56.56, trained time 00:19:55\n",
      "frame 77277, epsiode 68, mean reward -19.69, eps 0.23, fps 56.94, trained time 00:20:23\n",
      "frame 79441, epsiode 69, mean reward -19.57, eps 0.21, fps 56.54, trained time 00:21:01\n",
      "frame 80934, epsiode 70, mean reward -19.57, eps 0.19, fps 57.11, trained time 00:21:28\n",
      "frame 82497, epsiode 71, mean reward -19.55, eps 0.18, fps 56.25, trained time 00:21:55\n",
      "frame 84003, epsiode 72, mean reward -19.57, eps 0.16, fps 54.79, trained time 00:22:23\n",
      "frame 85538, epsiode 73, mean reward -19.56, eps 0.14, fps 53.70, trained time 00:22:51\n",
      "frame 87331, epsiode 74, mean reward -19.54, eps 0.13, fps 52.06, trained time 00:23:26\n",
      "frame 89209, epsiode 75, mean reward -19.49, eps 0.11, fps 56.53, trained time 00:23:59\n",
      "frame 91468, epsiode 76, mean reward -19.45, eps 0.09, fps 56.81, trained time 00:24:39\n",
      "frame 94198, epsiode 77, mean reward -19.32, eps 0.06, fps 56.02, trained time 00:25:28\n",
      "frame 96195, epsiode 78, mean reward -19.28, eps 0.04, fps 56.56, trained time 00:26:03\n",
      "frame 98538, epsiode 79, mean reward -19.23, eps 0.02, fps 56.42, trained time 00:26:44\n",
      "frame 101221, epsiode 80, mean reward -19.09, eps 0.02, fps 55.86, trained time 00:27:32\n",
      "frame 103771, epsiode 81, mean reward -18.94, eps 0.02, fps 55.34, trained time 00:28:19\n",
      "frame 105603, epsiode 82, mean reward -18.91, eps 0.02, fps 54.31, trained time 00:28:52\n",
      "frame 108304, epsiode 83, mean reward -18.59, eps 0.02, fps 53.05, trained time 00:29:43\n",
      "frame 111135, epsiode 84, mean reward -18.27, eps 0.02, fps 55.96, trained time 00:30:34\n",
      "frame 113850, epsiode 85, mean reward -17.98, eps 0.02, fps 56.37, trained time 00:31:22\n",
      "frame 116525, epsiode 86, mean reward -17.87, eps 0.02, fps 55.86, trained time 00:32:10\n",
      "frame 118890, epsiode 87, mean reward -17.79, eps 0.02, fps 55.29, trained time 00:32:53\n",
      "frame 121636, epsiode 88, mean reward -17.67, eps 0.02, fps 56.15, trained time 00:33:41\n",
      "frame 124462, epsiode 89, mean reward -17.54, eps 0.02, fps 56.08, trained time 00:34:32\n",
      "frame 127220, epsiode 90, mean reward -17.28, eps 0.02, fps 55.58, trained time 00:35:22\n",
      "frame 129168, epsiode 91, mean reward -17.27, eps 0.02, fps 56.21, trained time 00:35:56\n",
      "frame 132208, epsiode 92, mean reward -17.04, eps 0.02, fps 55.71, trained time 00:36:51\n",
      "frame 135076, epsiode 93, mean reward -16.80, eps 0.02, fps 55.95, trained time 00:37:42\n",
      "frame 136999, epsiode 94, mean reward -16.44, eps 0.02, fps 55.91, trained time 00:38:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 140138, epsiode 95, mean reward -16.20, eps 0.02, fps 55.75, trained time 00:39:13\n",
      "frame 142139, epsiode 96, mean reward -15.85, eps 0.02, fps 55.97, trained time 00:39:48\n",
      "frame 143979, epsiode 97, mean reward -15.49, eps 0.02, fps 55.98, trained time 00:40:21\n",
      "frame 146500, epsiode 98, mean reward -15.22, eps 0.02, fps 55.66, trained time 00:41:07\n",
      "frame 149372, epsiode 99, mean reward -15.12, eps 0.02, fps 56.11, trained time 00:41:58\n",
      "frame 151157, epsiode 100, mean reward -14.78, eps 0.02, fps 56.17, trained time 00:42:30\n",
      "frame 152792, epsiode 101, mean reward -14.36, eps 0.02, fps 56.23, trained time 00:42:59\n",
      "frame 154714, epsiode 102, mean reward -14.00, eps 0.02, fps 55.77, trained time 00:43:33\n",
      "frame 157587, epsiode 103, mean reward -13.73, eps 0.02, fps 56.08, trained time 00:44:24\n",
      "frame 159943, epsiode 104, mean reward -13.66, eps 0.02, fps 56.32, trained time 00:45:06\n",
      "frame 162433, epsiode 105, mean reward -13.55, eps 0.02, fps 55.99, trained time 00:45:51\n",
      "frame 165693, epsiode 106, mean reward -13.27, eps 0.02, fps 55.89, trained time 00:46:49\n",
      "frame 168718, epsiode 107, mean reward -13.00, eps 0.02, fps 56.07, trained time 00:47:43\n",
      "frame 171696, epsiode 108, mean reward -12.74, eps 0.02, fps 56.13, trained time 00:48:36\n",
      "frame 173979, epsiode 109, mean reward -12.38, eps 0.02, fps 56.24, trained time 00:49:17\n",
      "frame 176272, epsiode 110, mean reward -12.04, eps 0.02, fps 55.65, trained time 00:49:58\n",
      "frame 178824, epsiode 111, mean reward -11.71, eps 0.02, fps 56.02, trained time 00:50:43\n",
      "frame 181088, epsiode 112, mean reward -11.35, eps 0.02, fps 55.92, trained time 00:51:24\n",
      "frame 183712, epsiode 113, mean reward -11.01, eps 0.02, fps 55.60, trained time 00:52:11\n",
      "frame 185544, epsiode 114, mean reward -10.64, eps 0.02, fps 55.93, trained time 00:52:44\n",
      "frame 187687, epsiode 115, mean reward -10.29, eps 0.02, fps 55.81, trained time 00:53:22\n",
      "frame 189745, epsiode 116, mean reward -9.92, eps 0.02, fps 56.26, trained time 00:53:59\n",
      "frame 191663, epsiode 117, mean reward -9.53, eps 0.02, fps 56.20, trained time 00:54:33\n",
      "frame 194118, epsiode 118, mean reward -9.22, eps 0.02, fps 56.37, trained time 00:55:16\n",
      "frame 195790, epsiode 119, mean reward -8.81, eps 0.02, fps 55.55, trained time 00:55:47\n",
      "frame 197810, epsiode 120, mean reward -8.43, eps 0.02, fps 56.11, trained time 00:56:23\n",
      "frame 199881, epsiode 121, mean reward -8.08, eps 0.02, fps 56.52, trained time 00:56:59\n",
      "frame 201739, epsiode 122, mean reward -7.71, eps 0.02, fps 56.14, trained time 00:57:32\n",
      "frame 203590, epsiode 123, mean reward -7.32, eps 0.02, fps 55.93, trained time 00:58:05\n",
      "frame 205381, epsiode 124, mean reward -6.91, eps 0.02, fps 55.95, trained time 00:58:37\n",
      "frame 207300, epsiode 125, mean reward -6.51, eps 0.02, fps 56.20, trained time 00:59:12\n",
      "frame 209459, epsiode 126, mean reward -6.14, eps 0.02, fps 54.74, trained time 00:59:51\n",
      "frame 211189, epsiode 127, mean reward -5.73, eps 0.02, fps 56.02, trained time 01:00:22\n",
      "frame 213186, epsiode 128, mean reward -5.36, eps 0.02, fps 55.67, trained time 01:00:58\n",
      "frame 215018, epsiode 129, mean reward -4.96, eps 0.02, fps 56.07, trained time 01:01:30\n",
      "frame 217030, epsiode 130, mean reward -4.63, eps 0.02, fps 55.84, trained time 01:02:06\n",
      "frame 218744, epsiode 131, mean reward -4.24, eps 0.02, fps 56.39, trained time 01:02:37\n",
      "frame 220943, epsiode 132, mean reward -3.91, eps 0.02, fps 56.06, trained time 01:03:16\n",
      "frame 223740, epsiode 133, mean reward -3.63, eps 0.02, fps 56.35, trained time 01:04:06\n",
      "frame 226715, epsiode 134, mean reward -3.36, eps 0.02, fps 56.42, trained time 01:04:58\n",
      "frame 229058, epsiode 135, mean reward -3.01, eps 0.02, fps 56.21, trained time 01:05:40\n",
      "frame 231596, epsiode 136, mean reward -2.71, eps 0.02, fps 55.67, trained time 01:06:26\n",
      "frame 234059, epsiode 137, mean reward -2.43, eps 0.02, fps 55.80, trained time 01:07:10\n",
      "frame 236374, epsiode 138, mean reward -2.10, eps 0.02, fps 55.87, trained time 01:07:51\n",
      "frame 238172, epsiode 139, mean reward -1.71, eps 0.02, fps 56.42, trained time 01:08:23\n",
      "frame 240246, epsiode 140, mean reward -1.34, eps 0.02, fps 56.30, trained time 01:09:00\n",
      "frame 242029, epsiode 141, mean reward -0.97, eps 0.02, fps 54.77, trained time 01:09:33\n",
      "frame 244121, epsiode 142, mean reward -0.65, eps 0.02, fps 55.95, trained time 01:10:10\n",
      "frame 245907, epsiode 143, mean reward -0.26, eps 0.02, fps 56.58, trained time 01:10:41\n",
      "frame 247864, epsiode 144, mean reward 0.10, eps 0.02, fps 56.34, trained time 01:11:16\n",
      "frame 249943, epsiode 145, mean reward 0.45, eps 0.02, fps 56.23, trained time 01:11:53\n",
      "frame 251939, epsiode 146, mean reward 0.82, eps 0.02, fps 56.01, trained time 01:12:29\n",
      "frame 253829, epsiode 147, mean reward 1.19, eps 0.02, fps 56.08, trained time 01:13:03\n",
      "frame 255787, epsiode 148, mean reward 1.53, eps 0.02, fps 56.39, trained time 01:13:37\n",
      "frame 257486, epsiode 149, mean reward 1.93, eps 0.02, fps 56.27, trained time 01:14:07\n",
      "frame 259579, epsiode 150, mean reward 2.27, eps 0.02, fps 56.31, trained time 01:14:45\n",
      "frame 261244, epsiode 151, mean reward 2.67, eps 0.02, fps 56.59, trained time 01:15:14\n",
      "frame 262979, epsiode 152, mean reward 3.06, eps 0.02, fps 56.18, trained time 01:15:45\n",
      "frame 265085, epsiode 153, mean reward 3.42, eps 0.02, fps 56.35, trained time 01:16:22\n",
      "frame 267205, epsiode 154, mean reward 3.76, eps 0.02, fps 56.49, trained time 01:17:00\n",
      "frame 269022, epsiode 155, mean reward 4.14, eps 0.02, fps 56.62, trained time 01:17:32\n",
      "frame 270655, epsiode 156, mean reward 4.54, eps 0.02, fps 55.91, trained time 01:18:01\n",
      "frame 272650, epsiode 157, mean reward 4.90, eps 0.02, fps 56.28, trained time 01:18:37\n",
      "frame 274487, epsiode 158, mean reward 5.26, eps 0.02, fps 55.74, trained time 01:19:09\n",
      "frame 276297, epsiode 159, mean reward 5.59, eps 0.02, fps 56.10, trained time 01:19:42\n",
      "frame 278121, epsiode 160, mean reward 5.97, eps 0.02, fps 56.01, trained time 01:20:14\n",
      "frame 280122, epsiode 161, mean reward 6.31, eps 0.02, fps 55.91, trained time 01:20:50\n",
      "frame 282049, epsiode 162, mean reward 6.68, eps 0.02, fps 56.17, trained time 01:21:24\n",
      "frame 283892, epsiode 163, mean reward 7.04, eps 0.02, fps 56.02, trained time 01:21:57\n",
      "frame 285672, epsiode 164, mean reward 7.38, eps 0.02, fps 56.59, trained time 01:22:29\n",
      "frame 287598, epsiode 165, mean reward 7.72, eps 0.02, fps 56.42, trained time 01:23:03\n",
      "frame 289233, epsiode 166, mean reward 8.07, eps 0.02, fps 55.91, trained time 01:23:32\n",
      "frame 291120, epsiode 167, mean reward 8.45, eps 0.02, fps 55.97, trained time 01:24:06\n",
      "frame 293135, epsiode 168, mean reward 8.81, eps 0.02, fps 56.31, trained time 01:24:42\n",
      "frame 294945, epsiode 169, mean reward 9.11, eps 0.02, fps 55.83, trained time 01:25:14\n",
      "frame 296888, epsiode 170, mean reward 9.50, eps 0.02, fps 56.08, trained time 01:25:49\n",
      "frame 298747, epsiode 171, mean reward 9.86, eps 0.02, fps 56.32, trained time 01:26:22\n",
      "frame 300698, epsiode 172, mean reward 10.25, eps 0.02, fps 56.49, trained time 01:26:56\n",
      "frame 302892, epsiode 173, mean reward 10.59, eps 0.02, fps 56.31, trained time 01:27:35\n",
      "frame 305239, epsiode 174, mean reward 10.89, eps 0.02, fps 56.69, trained time 01:28:17\n",
      "frame 307255, epsiode 175, mean reward 11.22, eps 0.02, fps 56.53, trained time 01:28:52\n",
      "frame 309146, epsiode 176, mean reward 11.55, eps 0.02, fps 54.73, trained time 01:29:27\n",
      "frame 310937, epsiode 177, mean reward 11.85, eps 0.02, fps 56.24, trained time 01:29:59\n",
      "frame 312748, epsiode 178, mean reward 12.20, eps 0.02, fps 56.23, trained time 01:30:31\n",
      "frame 314514, epsiode 179, mean reward 12.54, eps 0.02, fps 56.22, trained time 01:31:02\n",
      "frame 316418, epsiode 180, mean reward 12.80, eps 0.02, fps 56.28, trained time 01:31:36\n",
      "frame 318052, epsiode 181, mean reward 13.08, eps 0.02, fps 55.42, trained time 01:32:06\n",
      "frame 319684, epsiode 182, mean reward 13.46, eps 0.02, fps 56.29, trained time 01:32:35\n",
      "frame 321573, epsiode 183, mean reward 13.56, eps 0.02, fps 55.91, trained time 01:33:08\n",
      "frame 323357, epsiode 184, mean reward 13.67, eps 0.02, fps 56.04, trained time 01:33:40\n",
      "frame 325357, epsiode 185, mean reward 13.77, eps 0.02, fps 55.95, trained time 01:34:16\n",
      "frame 327325, epsiode 186, mean reward 14.03, eps 0.02, fps 56.21, trained time 01:34:51\n",
      "frame 329067, epsiode 187, mean reward 14.34, eps 0.02, fps 55.91, trained time 01:35:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 331050, epsiode 188, mean reward 14.59, eps 0.02, fps 56.75, trained time 01:35:57\n",
      "frame 332767, epsiode 189, mean reward 14.85, eps 0.02, fps 56.06, trained time 01:36:28\n",
      "frame 334515, epsiode 190, mean reward 14.99, eps 0.02, fps 56.05, trained time 01:36:59\n",
      "frame 336311, epsiode 191, mean reward 15.35, eps 0.02, fps 56.40, trained time 01:37:31\n",
      "frame 338459, epsiode 192, mean reward 15.46, eps 0.02, fps 55.78, trained time 01:38:09\n",
      "frame 340784, epsiode 193, mean reward 15.54, eps 0.02, fps 56.34, trained time 01:38:51\n",
      "frame 342578, epsiode 194, mean reward 15.56, eps 0.02, fps 56.40, trained time 01:39:22\n",
      "frame 344248, epsiode 195, mean reward 15.70, eps 0.02, fps 55.70, trained time 01:39:52\n",
      "frame 346213, epsiode 196, mean reward 15.70, eps 0.02, fps 55.54, trained time 01:40:28\n",
      "frame 347898, epsiode 197, mean reward 15.71, eps 0.02, fps 56.12, trained time 01:40:58\n",
      "frame 350032, epsiode 198, mean reward 15.72, eps 0.02, fps 55.87, trained time 01:41:36\n",
      "frame 351684, epsiode 199, mean reward 15.98, eps 0.02, fps 55.95, trained time 01:42:05\n",
      "frame 353575, epsiode 200, mean reward 15.96, eps 0.02, fps 56.13, trained time 01:42:39\n",
      "frame 355429, epsiode 201, mean reward 15.93, eps 0.02, fps 56.22, trained time 01:43:12\n",
      "frame 357592, epsiode 202, mean reward 15.92, eps 0.02, fps 56.50, trained time 01:43:50\n",
      "frame 359223, epsiode 203, mean reward 16.07, eps 0.02, fps 56.63, trained time 01:44:19\n",
      "The agent 'DQNAgent with basic DQN' has solved the game 'PongNoFrameskip-v4' in 359223 frames, 203 episodes\n"
     ]
    }
   ],
   "source": [
    "train(env_name, agent, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4fa90",
   "metadata": {},
   "source": [
    "#### 4.2 train the Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38fdb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][1]\n",
    "writer = SummaryWriter(comment=\"-\"+ str(agent)  + \"-\" + env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0b9eefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 899, epsiode 1, mean reward -20.00, eps 0.99, fps 772.16, trained time 00:00:01\n",
      "frame 1739, epsiode 2, mean reward -20.00, eps 0.98, fps 712.50, trained time 00:00:02\n",
      "frame 2912, epsiode 3, mean reward -19.67, eps 0.97, fps 726.20, trained time 00:00:03\n",
      "frame 3814, epsiode 4, mean reward -20.00, eps 0.96, fps 674.64, trained time 00:00:05\n",
      "frame 4576, epsiode 5, mean reward -20.20, eps 0.95, fps 631.55, trained time 00:00:06\n",
      "frame 5504, epsiode 6, mean reward -20.17, eps 0.94, fps 633.28, trained time 00:00:07\n",
      "frame 6560, epsiode 7, mean reward -20.14, eps 0.93, fps 664.22, trained time 00:00:09\n",
      "frame 7507, epsiode 8, mean reward -20.12, eps 0.92, fps 621.71, trained time 00:00:11\n",
      "frame 8467, epsiode 9, mean reward -20.11, eps 0.92, fps 616.82, trained time 00:00:12\n",
      "frame 9368, epsiode 10, mean reward -20.20, eps 0.91, fps 596.14, trained time 00:00:14\n",
      "frame 10250, epsiode 11, mean reward -20.27, eps 0.90, fps 142.79, trained time 00:00:20\n",
      "frame 11275, epsiode 12, mean reward -20.17, eps 0.89, fps 43.43, trained time 00:00:43\n",
      "frame 12213, epsiode 13, mean reward -20.08, eps 0.88, fps 53.58, trained time 00:01:01\n",
      "frame 13183, epsiode 14, mean reward -20.14, eps 0.87, fps 54.01, trained time 00:01:19\n",
      "frame 14033, epsiode 15, mean reward -20.20, eps 0.86, fps 52.97, trained time 00:01:35\n",
      "frame 14883, epsiode 16, mean reward -20.25, eps 0.85, fps 53.37, trained time 00:01:51\n",
      "frame 15975, epsiode 17, mean reward -20.18, eps 0.84, fps 53.51, trained time 00:02:11\n",
      "frame 16900, epsiode 18, mean reward -20.17, eps 0.83, fps 54.35, trained time 00:02:28\n",
      "frame 17783, epsiode 19, mean reward -20.21, eps 0.82, fps 53.96, trained time 00:02:45\n",
      "frame 18686, epsiode 20, mean reward -20.20, eps 0.81, fps 52.42, trained time 00:03:02\n",
      "frame 19833, epsiode 21, mean reward -20.14, eps 0.80, fps 53.08, trained time 00:03:23\n",
      "frame 20702, epsiode 22, mean reward -20.18, eps 0.79, fps 54.65, trained time 00:03:39\n",
      "frame 21681, epsiode 23, mean reward -20.17, eps 0.78, fps 54.43, trained time 00:03:57\n",
      "frame 22701, epsiode 24, mean reward -20.17, eps 0.77, fps 53.56, trained time 00:04:16\n",
      "frame 23702, epsiode 25, mean reward -20.20, eps 0.76, fps 54.06, trained time 00:04:35\n",
      "frame 24650, epsiode 26, mean reward -20.23, eps 0.75, fps 54.17, trained time 00:04:52\n",
      "frame 25753, epsiode 27, mean reward -20.19, eps 0.74, fps 53.73, trained time 00:05:13\n",
      "frame 26838, epsiode 28, mean reward -20.18, eps 0.73, fps 53.60, trained time 00:05:33\n",
      "frame 27772, epsiode 29, mean reward -20.21, eps 0.72, fps 52.51, trained time 00:05:51\n",
      "frame 28674, epsiode 30, mean reward -20.23, eps 0.71, fps 53.22, trained time 00:06:08\n",
      "frame 29514, epsiode 31, mean reward -20.23, eps 0.70, fps 53.44, trained time 00:06:24\n",
      "frame 30413, epsiode 32, mean reward -20.25, eps 0.70, fps 54.22, trained time 00:06:40\n",
      "frame 31325, epsiode 33, mean reward -20.27, eps 0.69, fps 41.43, trained time 00:07:02\n",
      "frame 32087, epsiode 34, mean reward -20.29, eps 0.68, fps 50.79, trained time 00:07:17\n",
      "frame 32925, epsiode 35, mean reward -20.31, eps 0.67, fps 47.68, trained time 00:07:35\n",
      "frame 33807, epsiode 36, mean reward -20.33, eps 0.66, fps 52.15, trained time 00:07:52\n",
      "frame 34984, epsiode 37, mean reward -20.27, eps 0.65, fps 53.06, trained time 00:08:14\n",
      "frame 35917, epsiode 38, mean reward -20.26, eps 0.64, fps 53.18, trained time 00:08:31\n",
      "frame 36737, epsiode 39, mean reward -20.28, eps 0.63, fps 50.60, trained time 00:08:48\n",
      "frame 37743, epsiode 40, mean reward -20.30, eps 0.62, fps 48.70, trained time 00:09:08\n",
      "frame 38585, epsiode 41, mean reward -20.32, eps 0.61, fps 52.47, trained time 00:09:24\n",
      "frame 39608, epsiode 42, mean reward -20.29, eps 0.60, fps 52.20, trained time 00:09:44\n",
      "frame 40612, epsiode 43, mean reward -20.30, eps 0.59, fps 48.39, trained time 00:10:05\n",
      "frame 41616, epsiode 44, mean reward -20.32, eps 0.58, fps 50.00, trained time 00:10:25\n",
      "frame 42436, epsiode 45, mean reward -20.33, eps 0.58, fps 46.35, trained time 00:10:43\n",
      "frame 43531, epsiode 46, mean reward -20.30, eps 0.56, fps 48.76, trained time 00:11:05\n",
      "frame 44460, epsiode 47, mean reward -20.30, eps 0.56, fps 53.44, trained time 00:11:22\n",
      "frame 45458, epsiode 48, mean reward -20.27, eps 0.55, fps 53.48, trained time 00:11:41\n",
      "frame 46608, epsiode 49, mean reward -20.24, eps 0.53, fps 53.47, trained time 00:12:03\n",
      "frame 47580, epsiode 50, mean reward -20.26, eps 0.52, fps 53.11, trained time 00:12:21\n",
      "frame 48682, epsiode 51, mean reward -20.27, eps 0.51, fps 53.54, trained time 00:12:41\n",
      "frame 49973, epsiode 52, mean reward -20.25, eps 0.50, fps 53.42, trained time 00:13:06\n",
      "frame 50984, epsiode 53, mean reward -20.25, eps 0.49, fps 52.55, trained time 00:13:25\n",
      "frame 52320, epsiode 54, mean reward -20.20, eps 0.48, fps 52.60, trained time 00:13:50\n",
      "frame 53447, epsiode 55, mean reward -20.20, eps 0.47, fps 52.83, trained time 00:14:12\n",
      "frame 54691, epsiode 56, mean reward -20.18, eps 0.45, fps 52.77, trained time 00:14:35\n",
      "frame 55910, epsiode 57, mean reward -20.19, eps 0.44, fps 50.64, trained time 00:14:59\n",
      "frame 57379, epsiode 58, mean reward -20.17, eps 0.43, fps 53.15, trained time 00:15:27\n",
      "frame 58418, epsiode 59, mean reward -20.17, eps 0.42, fps 52.33, trained time 00:15:47\n",
      "frame 59925, epsiode 60, mean reward -20.17, eps 0.40, fps 52.94, trained time 00:16:15\n",
      "frame 61230, epsiode 61, mean reward -20.15, eps 0.39, fps 52.90, trained time 00:16:40\n",
      "frame 62571, epsiode 62, mean reward -20.15, eps 0.37, fps 52.92, trained time 00:17:05\n",
      "frame 63845, epsiode 63, mean reward -20.11, eps 0.36, fps 53.03, trained time 00:17:29\n",
      "frame 65209, epsiode 64, mean reward -20.06, eps 0.35, fps 52.90, trained time 00:17:55\n",
      "frame 66832, epsiode 65, mean reward -20.06, eps 0.33, fps 52.80, trained time 00:18:26\n",
      "frame 68457, epsiode 66, mean reward -20.05, eps 0.32, fps 52.26, trained time 00:18:57\n",
      "frame 70497, epsiode 67, mean reward -19.97, eps 0.30, fps 52.38, trained time 00:19:36\n",
      "frame 72621, epsiode 68, mean reward -19.90, eps 0.27, fps 52.80, trained time 00:20:16\n",
      "frame 74030, epsiode 69, mean reward -19.90, eps 0.26, fps 52.33, trained time 00:20:43\n",
      "frame 75924, epsiode 70, mean reward -19.87, eps 0.24, fps 52.43, trained time 00:21:19\n",
      "frame 77587, epsiode 71, mean reward -19.85, eps 0.22, fps 52.70, trained time 00:21:51\n",
      "frame 79762, epsiode 72, mean reward -19.78, eps 0.20, fps 52.41, trained time 00:22:32\n",
      "frame 81497, epsiode 73, mean reward -19.78, eps 0.19, fps 51.92, trained time 00:23:05\n",
      "frame 82899, epsiode 74, mean reward -19.76, eps 0.17, fps 52.51, trained time 00:23:32\n",
      "frame 84707, epsiode 75, mean reward -19.71, eps 0.15, fps 52.52, trained time 00:24:07\n",
      "frame 86747, epsiode 76, mean reward -19.68, eps 0.13, fps 52.58, trained time 00:24:45\n",
      "frame 88439, epsiode 77, mean reward -19.70, eps 0.12, fps 49.85, trained time 00:25:19\n",
      "frame 90669, epsiode 78, mean reward -19.67, eps 0.09, fps 52.43, trained time 00:26:02\n",
      "frame 93172, epsiode 79, mean reward -19.66, eps 0.07, fps 52.16, trained time 00:26:50\n",
      "frame 95036, epsiode 80, mean reward -19.68, eps 0.05, fps 51.67, trained time 00:27:26\n",
      "frame 96960, epsiode 81, mean reward -19.65, eps 0.03, fps 43.89, trained time 00:28:10\n",
      "frame 98455, epsiode 82, mean reward -19.65, eps 0.02, fps 47.72, trained time 00:28:41\n",
      "frame 100711, epsiode 83, mean reward -19.60, eps 0.02, fps 49.89, trained time 00:29:26\n",
      "frame 102671, epsiode 84, mean reward -19.57, eps 0.02, fps 50.15, trained time 00:30:05\n",
      "frame 104797, epsiode 85, mean reward -19.56, eps 0.02, fps 50.42, trained time 00:30:48\n",
      "frame 106822, epsiode 86, mean reward -19.58, eps 0.02, fps 52.16, trained time 00:31:26\n",
      "frame 109409, epsiode 87, mean reward -19.56, eps 0.02, fps 51.99, trained time 00:32:16\n",
      "frame 111860, epsiode 88, mean reward -19.52, eps 0.02, fps 51.61, trained time 00:33:04\n",
      "frame 114304, epsiode 89, mean reward -19.47, eps 0.02, fps 49.67, trained time 00:33:53\n",
      "frame 116341, epsiode 90, mean reward -19.43, eps 0.02, fps 51.50, trained time 00:34:32\n",
      "frame 118855, epsiode 91, mean reward -19.40, eps 0.02, fps 51.85, trained time 00:35:21\n",
      "frame 121168, epsiode 92, mean reward -19.36, eps 0.02, fps 51.57, trained time 00:36:06\n",
      "frame 123510, epsiode 93, mean reward -19.33, eps 0.02, fps 48.30, trained time 00:36:54\n",
      "frame 126098, epsiode 94, mean reward -19.26, eps 0.02, fps 52.16, trained time 00:37:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 129146, epsiode 95, mean reward -19.14, eps 0.02, fps 52.17, trained time 00:38:42\n",
      "frame 131921, epsiode 96, mean reward -19.06, eps 0.02, fps 52.42, trained time 00:39:35\n",
      "frame 133997, epsiode 97, mean reward -19.03, eps 0.02, fps 52.15, trained time 00:40:15\n",
      "frame 135945, epsiode 98, mean reward -18.99, eps 0.02, fps 52.25, trained time 00:40:52\n",
      "frame 138305, epsiode 99, mean reward -18.91, eps 0.02, fps 51.87, trained time 00:41:38\n",
      "frame 141448, epsiode 100, mean reward -18.82, eps 0.02, fps 51.95, trained time 00:42:38\n",
      "frame 144925, epsiode 101, mean reward -18.73, eps 0.02, fps 52.06, trained time 00:43:45\n",
      "frame 146611, epsiode 102, mean reward -18.68, eps 0.02, fps 52.05, trained time 00:44:17\n",
      "frame 149238, epsiode 103, mean reward -18.57, eps 0.02, fps 51.29, trained time 00:45:09\n",
      "frame 152083, epsiode 104, mean reward -18.46, eps 0.02, fps 51.95, trained time 00:46:03\n",
      "frame 154341, epsiode 105, mean reward -18.39, eps 0.02, fps 52.35, trained time 00:46:47\n",
      "frame 157153, epsiode 106, mean reward -18.26, eps 0.02, fps 52.42, trained time 00:47:40\n",
      "frame 159736, epsiode 107, mean reward -18.14, eps 0.02, fps 52.49, trained time 00:48:29\n",
      "frame 162652, epsiode 108, mean reward -18.00, eps 0.02, fps 52.35, trained time 00:49:25\n",
      "frame 166065, epsiode 109, mean reward -17.85, eps 0.02, fps 52.38, trained time 00:50:30\n",
      "frame 168575, epsiode 110, mean reward -17.73, eps 0.02, fps 52.09, trained time 00:51:19\n",
      "frame 171351, epsiode 111, mean reward -17.57, eps 0.02, fps 52.09, trained time 00:52:12\n",
      "frame 174050, epsiode 112, mean reward -17.51, eps 0.02, fps 52.09, trained time 00:53:04\n",
      "frame 177503, epsiode 113, mean reward -17.38, eps 0.02, fps 52.12, trained time 00:54:10\n",
      "frame 180480, epsiode 114, mean reward -17.25, eps 0.02, fps 52.17, trained time 00:55:07\n",
      "frame 183966, epsiode 115, mean reward -17.09, eps 0.02, fps 52.46, trained time 00:56:13\n",
      "frame 187290, epsiode 116, mean reward -16.92, eps 0.02, fps 52.29, trained time 00:57:17\n",
      "frame 190834, epsiode 117, mean reward -16.80, eps 0.02, fps 52.39, trained time 00:58:25\n",
      "frame 195015, epsiode 118, mean reward -16.62, eps 0.02, fps 52.19, trained time 00:59:45\n",
      "frame 197552, epsiode 119, mean reward -16.54, eps 0.02, fps 52.68, trained time 01:00:33\n",
      "frame 200892, epsiode 120, mean reward -16.42, eps 0.02, fps 52.62, trained time 01:01:36\n",
      "frame 203169, epsiode 121, mean reward -16.38, eps 0.02, fps 52.85, trained time 01:02:19\n",
      "frame 207264, epsiode 122, mean reward -16.18, eps 0.02, fps 52.86, trained time 01:03:37\n",
      "frame 210191, epsiode 123, mean reward -16.08, eps 0.02, fps 52.82, trained time 01:04:32\n",
      "frame 213775, epsiode 124, mean reward -15.81, eps 0.02, fps 52.58, trained time 01:05:40\n",
      "frame 217665, epsiode 125, mean reward -15.65, eps 0.02, fps 52.45, trained time 01:06:55\n",
      "frame 221058, epsiode 126, mean reward -15.46, eps 0.02, fps 52.79, trained time 01:07:59\n",
      "frame 224552, epsiode 127, mean reward -15.34, eps 0.02, fps 52.61, trained time 01:09:05\n",
      "frame 228028, epsiode 128, mean reward -15.17, eps 0.02, fps 52.75, trained time 01:10:11\n",
      "frame 231862, epsiode 129, mean reward -15.01, eps 0.02, fps 52.88, trained time 01:11:24\n",
      "frame 235706, epsiode 130, mean reward -14.81, eps 0.02, fps 52.59, trained time 01:12:37\n",
      "frame 239777, epsiode 131, mean reward -14.59, eps 0.02, fps 52.76, trained time 01:13:54\n",
      "frame 243710, epsiode 132, mean reward -14.34, eps 0.02, fps 51.99, trained time 01:15:10\n",
      "frame 248066, epsiode 133, mean reward -14.14, eps 0.02, fps 52.64, trained time 01:16:32\n",
      "frame 251927, epsiode 134, mean reward -13.95, eps 0.02, fps 52.45, trained time 01:17:46\n",
      "frame 255328, epsiode 135, mean reward -13.79, eps 0.02, fps 52.75, trained time 01:18:50\n",
      "frame 258171, epsiode 136, mean reward -13.63, eps 0.02, fps 52.77, trained time 01:19:44\n",
      "frame 261218, epsiode 137, mean reward -13.39, eps 0.02, fps 52.85, trained time 01:20:42\n",
      "frame 264214, epsiode 138, mean reward -13.25, eps 0.02, fps 52.66, trained time 01:21:39\n",
      "frame 267618, epsiode 139, mean reward -13.01, eps 0.02, fps 52.76, trained time 01:22:43\n",
      "frame 271406, epsiode 140, mean reward -12.82, eps 0.02, fps 52.53, trained time 01:23:56\n",
      "frame 274345, epsiode 141, mean reward -12.55, eps 0.02, fps 52.74, trained time 01:24:51\n",
      "frame 278415, epsiode 142, mean reward -12.38, eps 0.02, fps 52.68, trained time 01:26:08\n",
      "frame 282092, epsiode 143, mean reward -12.22, eps 0.02, fps 52.77, trained time 01:27:18\n",
      "frame 285620, epsiode 144, mean reward -11.96, eps 0.02, fps 52.82, trained time 01:28:25\n",
      "frame 288934, epsiode 145, mean reward -11.76, eps 0.02, fps 52.85, trained time 01:29:28\n",
      "frame 291616, epsiode 146, mean reward -11.48, eps 0.02, fps 52.48, trained time 01:30:19\n",
      "frame 294654, epsiode 147, mean reward -11.34, eps 0.02, fps 52.72, trained time 01:31:16\n",
      "frame 298349, epsiode 148, mean reward -11.18, eps 0.02, fps 52.68, trained time 01:32:27\n",
      "frame 301181, epsiode 149, mean reward -11.04, eps 0.02, fps 52.88, trained time 01:33:20\n",
      "frame 303926, epsiode 150, mean reward -10.74, eps 0.02, fps 52.95, trained time 01:34:12\n",
      "frame 307601, epsiode 151, mean reward -10.48, eps 0.02, fps 52.93, trained time 01:35:21\n",
      "frame 310607, epsiode 152, mean reward -10.22, eps 0.02, fps 52.74, trained time 01:36:18\n",
      "frame 313407, epsiode 153, mean reward -9.96, eps 0.02, fps 52.67, trained time 01:37:12\n",
      "frame 316256, epsiode 154, mean reward -9.76, eps 0.02, fps 52.89, trained time 01:38:05\n",
      "frame 318981, epsiode 155, mean reward -9.46, eps 0.02, fps 53.05, trained time 01:38:57\n",
      "frame 322165, epsiode 156, mean reward -9.20, eps 0.02, fps 52.90, trained time 01:39:57\n",
      "frame 325094, epsiode 157, mean reward -8.88, eps 0.02, fps 52.93, trained time 01:40:52\n",
      "frame 327989, epsiode 158, mean reward -8.72, eps 0.02, fps 52.66, trained time 01:41:47\n",
      "frame 330611, epsiode 159, mean reward -8.45, eps 0.02, fps 52.82, trained time 01:42:37\n",
      "frame 333101, epsiode 160, mean reward -8.17, eps 0.02, fps 53.13, trained time 01:43:24\n",
      "frame 335707, epsiode 161, mean reward -7.92, eps 0.02, fps 52.96, trained time 01:44:13\n",
      "frame 338055, epsiode 162, mean reward -7.59, eps 0.02, fps 51.96, trained time 01:44:58\n",
      "frame 340571, epsiode 163, mean reward -7.31, eps 0.02, fps 52.70, trained time 01:45:46\n",
      "frame 343417, epsiode 164, mean reward -7.11, eps 0.02, fps 52.81, trained time 01:46:40\n",
      "frame 345111, epsiode 165, mean reward -6.71, eps 0.02, fps 52.52, trained time 01:47:12\n",
      "frame 346901, epsiode 166, mean reward -6.34, eps 0.02, fps 52.82, trained time 01:47:46\n",
      "frame 349584, epsiode 167, mean reward -6.11, eps 0.02, fps 53.14, trained time 01:48:36\n",
      "frame 351754, epsiode 168, mean reward -5.83, eps 0.02, fps 53.06, trained time 01:49:17\n",
      "frame 354039, epsiode 169, mean reward -5.51, eps 0.02, fps 52.85, trained time 01:50:01\n",
      "frame 355670, epsiode 170, mean reward -5.12, eps 0.02, fps 53.26, trained time 01:50:31\n",
      "frame 357882, epsiode 171, mean reward -4.80, eps 0.02, fps 52.93, trained time 01:51:13\n",
      "frame 360580, epsiode 172, mean reward -4.50, eps 0.02, fps 52.79, trained time 01:52:04\n",
      "frame 363201, epsiode 173, mean reward -4.23, eps 0.02, fps 46.41, trained time 01:53:01\n",
      "frame 365282, epsiode 174, mean reward -3.92, eps 0.02, fps 49.14, trained time 01:53:43\n",
      "frame 367056, epsiode 175, mean reward -3.57, eps 0.02, fps 47.57, trained time 01:54:20\n",
      "frame 369469, epsiode 176, mean reward -3.27, eps 0.02, fps 48.90, trained time 01:55:10\n",
      "frame 371277, epsiode 177, mean reward -2.87, eps 0.02, fps 51.12, trained time 01:55:45\n",
      "frame 373217, epsiode 178, mean reward -2.52, eps 0.02, fps 49.84, trained time 01:56:24\n",
      "frame 374907, epsiode 179, mean reward -2.13, eps 0.02, fps 50.27, trained time 01:56:57\n",
      "frame 377105, epsiode 180, mean reward -1.84, eps 0.02, fps 50.58, trained time 01:57:41\n",
      "frame 378852, epsiode 181, mean reward -1.46, eps 0.02, fps 49.37, trained time 01:58:16\n",
      "frame 380658, epsiode 182, mean reward -1.07, eps 0.02, fps 50.59, trained time 01:58:52\n",
      "frame 382995, epsiode 183, mean reward -0.79, eps 0.02, fps 52.78, trained time 01:59:36\n",
      "frame 385748, epsiode 184, mean reward -0.56, eps 0.02, fps 53.10, trained time 02:00:28\n",
      "frame 388123, epsiode 185, mean reward -0.26, eps 0.02, fps 52.86, trained time 02:01:13\n",
      "frame 390390, epsiode 186, mean reward 0.08, eps 0.02, fps 53.12, trained time 02:01:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 393504, epsiode 187, mean reward 0.21, eps 0.02, fps 52.81, trained time 02:02:55\n",
      "frame 396094, epsiode 188, mean reward 0.49, eps 0.02, fps 52.89, trained time 02:03:44\n",
      "frame 397867, epsiode 189, mean reward 0.83, eps 0.02, fps 53.03, trained time 02:04:17\n",
      "frame 400015, epsiode 190, mean reward 1.16, eps 0.02, fps 52.84, trained time 02:04:58\n",
      "frame 402308, epsiode 191, mean reward 1.45, eps 0.02, fps 51.66, trained time 02:05:42\n",
      "frame 404692, epsiode 192, mean reward 1.72, eps 0.02, fps 52.94, trained time 02:06:27\n",
      "frame 406521, epsiode 193, mean reward 2.06, eps 0.02, fps 53.30, trained time 02:07:01\n",
      "frame 408482, epsiode 194, mean reward 2.36, eps 0.02, fps 52.55, trained time 02:07:39\n",
      "frame 410370, epsiode 195, mean reward 2.62, eps 0.02, fps 53.00, trained time 02:08:14\n",
      "frame 412345, epsiode 196, mean reward 2.91, eps 0.02, fps 53.09, trained time 02:08:52\n",
      "frame 414556, epsiode 197, mean reward 3.22, eps 0.02, fps 52.99, trained time 02:09:33\n",
      "frame 416785, epsiode 198, mean reward 3.52, eps 0.02, fps 52.27, trained time 02:10:16\n",
      "frame 418692, epsiode 199, mean reward 3.82, eps 0.02, fps 52.62, trained time 02:10:52\n",
      "frame 420992, epsiode 200, mean reward 4.02, eps 0.02, fps 52.66, trained time 02:11:36\n",
      "frame 423296, epsiode 201, mean reward 4.23, eps 0.02, fps 52.96, trained time 02:12:19\n",
      "frame 425927, epsiode 202, mean reward 4.43, eps 0.02, fps 52.86, trained time 02:13:09\n",
      "frame 428432, epsiode 203, mean reward 4.63, eps 0.02, fps 52.89, trained time 02:13:57\n",
      "frame 430822, epsiode 204, mean reward 4.86, eps 0.02, fps 52.20, trained time 02:14:42\n",
      "frame 432625, epsiode 205, mean reward 5.20, eps 0.02, fps 52.66, trained time 02:15:17\n",
      "frame 435058, epsiode 206, mean reward 5.38, eps 0.02, fps 52.77, trained time 02:16:03\n",
      "frame 437522, epsiode 207, mean reward 5.55, eps 0.02, fps 51.61, trained time 02:16:50\n",
      "frame 439931, epsiode 208, mean reward 5.75, eps 0.02, fps 52.63, trained time 02:17:36\n",
      "frame 441956, epsiode 209, mean reward 5.95, eps 0.02, fps 53.02, trained time 02:18:14\n",
      "frame 443769, epsiode 210, mean reward 6.22, eps 0.02, fps 53.09, trained time 02:18:49\n",
      "frame 445902, epsiode 211, mean reward 6.40, eps 0.02, fps 52.89, trained time 02:19:29\n",
      "frame 448209, epsiode 212, mean reward 6.67, eps 0.02, fps 53.02, trained time 02:20:12\n",
      "frame 451002, epsiode 213, mean reward 6.82, eps 0.02, fps 52.77, trained time 02:21:05\n",
      "frame 453338, epsiode 214, mean reward 7.05, eps 0.02, fps 52.47, trained time 02:21:50\n",
      "frame 455695, epsiode 215, mean reward 7.23, eps 0.02, fps 51.72, trained time 02:22:35\n",
      "frame 457327, epsiode 216, mean reward 7.48, eps 0.02, fps 51.33, trained time 02:23:07\n",
      "frame 458993, epsiode 217, mean reward 7.75, eps 0.02, fps 53.08, trained time 02:23:39\n",
      "frame 461234, epsiode 218, mean reward 7.92, eps 0.02, fps 52.68, trained time 02:24:21\n",
      "frame 462970, epsiode 219, mean reward 8.25, eps 0.02, fps 53.00, trained time 02:24:54\n",
      "frame 464926, epsiode 220, mean reward 8.49, eps 0.02, fps 52.22, trained time 02:25:31\n",
      "frame 466673, epsiode 221, mean reward 8.83, eps 0.02, fps 53.06, trained time 02:26:04\n",
      "frame 468560, epsiode 222, mean reward 9.02, eps 0.02, fps 53.04, trained time 02:26:40\n",
      "frame 470633, epsiode 223, mean reward 9.29, eps 0.02, fps 52.65, trained time 02:27:19\n",
      "frame 472894, epsiode 224, mean reward 9.36, eps 0.02, fps 52.95, trained time 02:28:02\n",
      "frame 474562, epsiode 225, mean reward 9.61, eps 0.02, fps 52.76, trained time 02:28:34\n",
      "frame 476700, epsiode 226, mean reward 9.79, eps 0.02, fps 52.25, trained time 02:29:14\n",
      "frame 478465, epsiode 227, mean reward 10.05, eps 0.02, fps 53.13, trained time 02:29:48\n",
      "frame 480195, epsiode 228, mean reward 10.27, eps 0.02, fps 52.92, trained time 02:30:20\n",
      "frame 481861, epsiode 229, mean reward 10.52, eps 0.02, fps 52.92, trained time 02:30:52\n",
      "frame 484120, epsiode 230, mean reward 10.69, eps 0.02, fps 52.80, trained time 02:31:35\n",
      "frame 486006, epsiode 231, mean reward 10.86, eps 0.02, fps 52.54, trained time 02:32:11\n",
      "frame 488310, epsiode 232, mean reward 10.96, eps 0.02, fps 52.92, trained time 02:32:54\n",
      "frame 490768, epsiode 233, mean reward 11.10, eps 0.02, fps 51.76, trained time 02:33:42\n",
      "frame 493734, epsiode 234, mean reward 11.24, eps 0.02, fps 53.06, trained time 02:34:37\n",
      "frame 495784, epsiode 235, mean reward 11.48, eps 0.02, fps 52.76, trained time 02:35:16\n",
      "frame 498215, epsiode 236, mean reward 11.63, eps 0.02, fps 52.88, trained time 02:36:02\n",
      "frame 500601, epsiode 237, mean reward 11.65, eps 0.02, fps 52.92, trained time 02:36:47\n",
      "frame 503600, epsiode 238, mean reward 11.76, eps 0.02, fps 52.95, trained time 02:37:44\n",
      "frame 505870, epsiode 239, mean reward 11.88, eps 0.02, fps 52.13, trained time 02:38:28\n",
      "frame 508058, epsiode 240, mean reward 12.05, eps 0.02, fps 52.76, trained time 02:39:09\n",
      "frame 509814, epsiode 241, mean reward 12.19, eps 0.02, fps 53.02, trained time 02:39:42\n",
      "frame 511732, epsiode 242, mean reward 12.38, eps 0.02, fps 51.84, trained time 02:40:19\n",
      "frame 513490, epsiode 243, mean reward 12.62, eps 0.02, fps 53.32, trained time 02:40:52\n",
      "frame 515123, epsiode 244, mean reward 12.78, eps 0.02, fps 52.27, trained time 02:41:23\n",
      "frame 517514, epsiode 245, mean reward 12.89, eps 0.02, fps 51.97, trained time 02:42:09\n",
      "frame 519922, epsiode 246, mean reward 12.94, eps 0.02, fps 53.12, trained time 02:42:55\n",
      "frame 521627, epsiode 247, mean reward 13.20, eps 0.02, fps 52.87, trained time 02:43:27\n",
      "frame 524307, epsiode 248, mean reward 13.29, eps 0.02, fps 53.00, trained time 02:44:18\n",
      "frame 526163, epsiode 249, mean reward 13.52, eps 0.02, fps 51.73, trained time 02:44:53\n",
      "frame 528650, epsiode 250, mean reward 13.53, eps 0.02, fps 53.04, trained time 02:45:40\n",
      "frame 530820, epsiode 251, mean reward 13.58, eps 0.02, fps 52.48, trained time 02:46:22\n",
      "frame 532649, epsiode 252, mean reward 13.69, eps 0.02, fps 52.41, trained time 02:46:57\n",
      "frame 534500, epsiode 253, mean reward 13.80, eps 0.02, fps 52.78, trained time 02:47:32\n",
      "frame 536738, epsiode 254, mean reward 13.90, eps 0.02, fps 52.82, trained time 02:48:14\n",
      "frame 538506, epsiode 255, mean reward 13.99, eps 0.02, fps 52.03, trained time 02:48:48\n",
      "frame 540478, epsiode 256, mean reward 14.09, eps 0.02, fps 52.66, trained time 02:49:25\n",
      "frame 542365, epsiode 257, mean reward 14.16, eps 0.02, fps 52.93, trained time 02:50:01\n",
      "frame 544101, epsiode 258, mean reward 14.39, eps 0.02, fps 53.07, trained time 02:50:34\n",
      "frame 545769, epsiode 259, mean reward 14.52, eps 0.02, fps 52.18, trained time 02:51:06\n",
      "frame 547405, epsiode 260, mean reward 14.65, eps 0.02, fps 52.91, trained time 02:51:37\n",
      "frame 549528, epsiode 261, mean reward 14.75, eps 0.02, fps 52.50, trained time 02:52:17\n",
      "frame 552223, epsiode 262, mean reward 14.67, eps 0.02, fps 52.75, trained time 02:53:08\n",
      "frame 554089, epsiode 263, mean reward 14.74, eps 0.02, fps 52.89, trained time 02:53:43\n",
      "frame 555955, epsiode 264, mean reward 14.90, eps 0.02, fps 52.97, trained time 02:54:19\n",
      "frame 558123, epsiode 265, mean reward 14.82, eps 0.02, fps 52.79, trained time 02:55:00\n",
      "frame 559833, epsiode 266, mean reward 14.84, eps 0.02, fps 53.02, trained time 02:55:32\n",
      "frame 561711, epsiode 267, mean reward 14.94, eps 0.02, fps 53.05, trained time 02:56:07\n",
      "frame 563347, epsiode 268, mean reward 15.02, eps 0.02, fps 52.96, trained time 02:56:38\n",
      "frame 566039, epsiode 269, mean reward 14.99, eps 0.02, fps 52.68, trained time 02:57:29\n",
      "frame 568042, epsiode 270, mean reward 14.96, eps 0.02, fps 53.04, trained time 02:58:07\n",
      "frame 570261, epsiode 271, mean reward 14.93, eps 0.02, fps 52.29, trained time 02:58:50\n",
      "frame 572031, epsiode 272, mean reward 14.96, eps 0.02, fps 52.74, trained time 02:59:23\n",
      "frame 573731, epsiode 273, mean reward 15.09, eps 0.02, fps 53.00, trained time 02:59:55\n",
      "frame 575365, epsiode 274, mean reward 15.17, eps 0.02, fps 52.68, trained time 03:00:26\n",
      "frame 577816, epsiode 275, mean reward 15.06, eps 0.02, fps 52.08, trained time 03:01:13\n",
      "frame 579766, epsiode 276, mean reward 15.12, eps 0.02, fps 51.49, trained time 03:01:51\n",
      "frame 582102, epsiode 277, mean reward 15.05, eps 0.02, fps 53.05, trained time 03:02:35\n",
      "frame 584147, epsiode 278, mean reward 15.01, eps 0.02, fps 52.80, trained time 03:03:14\n",
      "frame 586189, epsiode 279, mean reward 14.95, eps 0.02, fps 52.79, trained time 03:03:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 588050, epsiode 280, mean reward 15.05, eps 0.02, fps 53.01, trained time 03:04:28\n",
      "frame 589686, epsiode 281, mean reward 15.06, eps 0.02, fps 53.38, trained time 03:04:58\n",
      "frame 591484, epsiode 282, mean reward 15.04, eps 0.02, fps 52.96, trained time 03:05:32\n",
      "frame 593272, epsiode 283, mean reward 15.10, eps 0.02, fps 53.17, trained time 03:06:06\n",
      "frame 595220, epsiode 284, mean reward 15.22, eps 0.02, fps 53.12, trained time 03:06:43\n",
      "frame 597664, epsiode 285, mean reward 15.22, eps 0.02, fps 52.66, trained time 03:07:29\n",
      "frame 599375, epsiode 286, mean reward 15.29, eps 0.02, fps 53.30, trained time 03:08:01\n",
      "frame 601213, epsiode 287, mean reward 15.52, eps 0.02, fps 52.78, trained time 03:08:36\n",
      "frame 603483, epsiode 288, mean reward 15.52, eps 0.02, fps 52.92, trained time 03:09:19\n",
      "frame 605612, epsiode 289, mean reward 15.45, eps 0.02, fps 52.79, trained time 03:09:59\n",
      "frame 607618, epsiode 290, mean reward 15.42, eps 0.02, fps 53.32, trained time 03:10:37\n",
      "frame 609651, epsiode 291, mean reward 15.46, eps 0.02, fps 53.20, trained time 03:11:15\n",
      "frame 611507, epsiode 292, mean reward 15.54, eps 0.02, fps 53.29, trained time 03:11:50\n",
      "frame 613609, epsiode 293, mean reward 15.53, eps 0.02, fps 53.14, trained time 03:12:29\n",
      "frame 615729, epsiode 294, mean reward 15.48, eps 0.02, fps 53.01, trained time 03:13:09\n",
      "frame 617649, epsiode 295, mean reward 15.47, eps 0.02, fps 52.91, trained time 03:13:46\n",
      "frame 619714, epsiode 296, mean reward 15.46, eps 0.02, fps 52.68, trained time 03:14:25\n",
      "frame 621465, epsiode 297, mean reward 15.51, eps 0.02, fps 50.89, trained time 03:14:59\n",
      "frame 623930, epsiode 298, mean reward 15.48, eps 0.02, fps 52.68, trained time 03:15:46\n",
      "frame 625846, epsiode 299, mean reward 15.47, eps 0.02, fps 53.03, trained time 03:16:22\n",
      "frame 627816, epsiode 300, mean reward 15.52, eps 0.02, fps 52.87, trained time 03:16:59\n",
      "frame 629797, epsiode 301, mean reward 15.59, eps 0.02, fps 52.85, trained time 03:17:37\n",
      "frame 631783, epsiode 302, mean reward 15.73, eps 0.02, fps 53.41, trained time 03:18:14\n",
      "frame 633675, epsiode 303, mean reward 15.78, eps 0.02, fps 53.11, trained time 03:18:50\n",
      "frame 635376, epsiode 304, mean reward 15.85, eps 0.02, fps 53.23, trained time 03:19:22\n",
      "frame 637014, epsiode 305, mean reward 15.86, eps 0.02, fps 53.09, trained time 03:19:53\n",
      "frame 638875, epsiode 306, mean reward 15.92, eps 0.02, fps 53.05, trained time 03:20:28\n",
      "frame 640664, epsiode 307, mean reward 16.02, eps 0.02, fps 52.43, trained time 03:21:02\n",
      "frame 642634, epsiode 308, mean reward 16.06, eps 0.02, fps 52.75, trained time 03:21:39\n",
      "frame 644408, epsiode 309, mean reward 16.10, eps 0.02, fps 52.73, trained time 03:22:13\n",
      "frame 646385, epsiode 310, mean reward 16.09, eps 0.02, fps 52.97, trained time 03:22:50\n",
      "frame 648825, epsiode 311, mean reward 16.08, eps 0.02, fps 53.01, trained time 03:23:36\n",
      "frame 650620, epsiode 312, mean reward 16.14, eps 0.02, fps 52.42, trained time 03:24:10\n",
      "frame 652425, epsiode 313, mean reward 16.24, eps 0.02, fps 52.62, trained time 03:24:45\n",
      "frame 654147, epsiode 314, mean reward 16.28, eps 0.02, fps 51.62, trained time 03:25:18\n",
      "frame 656726, epsiode 315, mean reward 16.13, eps 0.02, fps 52.79, trained time 03:26:07\n",
      "frame 658663, epsiode 316, mean reward 16.11, eps 0.02, fps 52.86, trained time 03:26:44\n",
      "frame 660302, epsiode 317, mean reward 16.12, eps 0.02, fps 53.29, trained time 03:27:14\n",
      "frame 662223, epsiode 318, mean reward 16.14, eps 0.02, fps 53.03, trained time 03:27:51\n",
      "frame 664301, epsiode 319, mean reward 16.09, eps 0.02, fps 52.95, trained time 03:28:30\n",
      "frame 666250, epsiode 320, mean reward 16.11, eps 0.02, fps 52.74, trained time 03:29:07\n",
      "frame 667979, epsiode 321, mean reward 16.12, eps 0.02, fps 53.40, trained time 03:29:39\n",
      "frame 669820, epsiode 322, mean reward 16.12, eps 0.02, fps 53.17, trained time 03:30:14\n",
      "frame 671723, epsiode 323, mean reward 16.12, eps 0.02, fps 52.89, trained time 03:30:50\n",
      "frame 673621, epsiode 324, mean reward 16.15, eps 0.02, fps 52.99, trained time 03:31:26\n",
      "frame 675351, epsiode 325, mean reward 16.15, eps 0.02, fps 52.75, trained time 03:31:58\n",
      "frame 677642, epsiode 326, mean reward 16.14, eps 0.02, fps 53.19, trained time 03:32:41\n",
      "frame 679375, epsiode 327, mean reward 16.15, eps 0.02, fps 53.10, trained time 03:33:14\n",
      "frame 681230, epsiode 328, mean reward 16.14, eps 0.02, fps 52.86, trained time 03:33:49\n",
      "frame 683147, epsiode 329, mean reward 16.12, eps 0.02, fps 53.28, trained time 03:34:25\n",
      "frame 684786, epsiode 330, mean reward 16.17, eps 0.02, fps 53.25, trained time 03:34:56\n",
      "frame 686534, epsiode 331, mean reward 16.17, eps 0.02, fps 53.25, trained time 03:35:29\n",
      "frame 688230, epsiode 332, mean reward 16.23, eps 0.02, fps 53.24, trained time 03:36:01\n",
      "frame 689990, epsiode 333, mean reward 16.29, eps 0.02, fps 53.32, trained time 03:36:34\n",
      "frame 691685, epsiode 334, mean reward 16.37, eps 0.02, fps 53.04, trained time 03:37:06\n",
      "frame 693520, epsiode 335, mean reward 16.37, eps 0.02, fps 53.32, trained time 03:37:40\n",
      "frame 695247, epsiode 336, mean reward 16.46, eps 0.02, fps 53.23, trained time 03:38:12\n",
      "frame 697298, epsiode 337, mean reward 16.53, eps 0.02, fps 53.48, trained time 03:38:51\n",
      "frame 699116, epsiode 338, mean reward 16.66, eps 0.02, fps 52.89, trained time 03:39:25\n",
      "frame 700849, epsiode 339, mean reward 16.71, eps 0.02, fps 52.80, trained time 03:39:58\n",
      "frame 702836, epsiode 340, mean reward 16.72, eps 0.02, fps 52.89, trained time 03:40:35\n",
      "frame 704868, epsiode 341, mean reward 16.68, eps 0.02, fps 53.04, trained time 03:41:14\n",
      "frame 706679, epsiode 342, mean reward 16.71, eps 0.02, fps 52.61, trained time 03:41:48\n",
      "frame 709082, epsiode 343, mean reward 16.64, eps 0.02, fps 52.71, trained time 03:42:34\n",
      "frame 710722, epsiode 344, mean reward 16.64, eps 0.02, fps 52.62, trained time 03:43:05\n",
      "frame 712632, epsiode 345, mean reward 16.70, eps 0.02, fps 52.79, trained time 03:43:41\n",
      "frame 714452, epsiode 346, mean reward 16.75, eps 0.02, fps 52.83, trained time 03:44:16\n",
      "frame 716238, epsiode 347, mean reward 16.74, eps 0.02, fps 50.65, trained time 03:44:51\n",
      "frame 718135, epsiode 348, mean reward 16.86, eps 0.02, fps 52.20, trained time 03:45:27\n",
      "frame 719908, epsiode 349, mean reward 16.87, eps 0.02, fps 52.44, trained time 03:46:01\n",
      "frame 721917, epsiode 350, mean reward 16.93, eps 0.02, fps 52.46, trained time 03:46:39\n",
      "frame 723557, epsiode 351, mean reward 17.04, eps 0.02, fps 52.85, trained time 03:47:10\n",
      "frame 725423, epsiode 352, mean reward 17.03, eps 0.02, fps 52.68, trained time 03:47:46\n",
      "frame 727682, epsiode 353, mean reward 16.99, eps 0.02, fps 53.25, trained time 03:48:28\n",
      "frame 729667, epsiode 354, mean reward 17.04, eps 0.02, fps 53.11, trained time 03:49:06\n",
      "frame 731470, epsiode 355, mean reward 17.04, eps 0.02, fps 53.08, trained time 03:49:40\n",
      "frame 733645, epsiode 356, mean reward 17.00, eps 0.02, fps 53.26, trained time 03:50:20\n",
      "frame 735337, epsiode 357, mean reward 17.02, eps 0.02, fps 52.93, trained time 03:50:52\n",
      "frame 737120, epsiode 358, mean reward 17.01, eps 0.02, fps 53.00, trained time 03:51:26\n",
      "frame 739121, epsiode 359, mean reward 16.98, eps 0.02, fps 53.30, trained time 03:52:04\n",
      "frame 740845, epsiode 360, mean reward 16.96, eps 0.02, fps 53.34, trained time 03:52:36\n",
      "frame 742766, epsiode 361, mean reward 16.98, eps 0.02, fps 53.18, trained time 03:53:12\n",
      "frame 744720, epsiode 362, mean reward 17.11, eps 0.02, fps 53.42, trained time 03:53:49\n",
      "frame 746472, epsiode 363, mean reward 17.13, eps 0.02, fps 53.03, trained time 03:54:22\n",
      "frame 748207, epsiode 364, mean reward 17.14, eps 0.02, fps 53.13, trained time 03:54:54\n",
      "frame 749847, epsiode 365, mean reward 17.23, eps 0.02, fps 52.87, trained time 03:55:25\n",
      "frame 751981, epsiode 366, mean reward 17.18, eps 0.02, fps 52.85, trained time 03:56:06\n",
      "frame 753820, epsiode 367, mean reward 17.18, eps 0.02, fps 52.67, trained time 03:56:41\n",
      "frame 755549, epsiode 368, mean reward 17.17, eps 0.02, fps 52.89, trained time 03:57:13\n",
      "frame 757270, epsiode 369, mean reward 17.28, eps 0.02, fps 53.12, trained time 03:57:46\n",
      "frame 758926, epsiode 370, mean reward 17.31, eps 0.02, fps 52.77, trained time 03:58:17\n",
      "frame 760690, epsiode 371, mean reward 17.39, eps 0.02, fps 52.47, trained time 03:58:51\n",
      "frame 762741, epsiode 372, mean reward 17.40, eps 0.02, fps 53.22, trained time 03:59:29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 764379, epsiode 373, mean reward 17.41, eps 0.02, fps 52.90, trained time 04:00:00\n",
      "frame 766868, epsiode 374, mean reward 17.32, eps 0.02, fps 52.98, trained time 04:00:47\n",
      "frame 768678, epsiode 375, mean reward 17.44, eps 0.02, fps 52.40, trained time 04:01:22\n",
      "frame 770422, epsiode 376, mean reward 17.45, eps 0.02, fps 52.83, trained time 04:01:55\n",
      "frame 772408, epsiode 377, mean reward 17.49, eps 0.02, fps 52.79, trained time 04:02:32\n",
      "frame 774101, epsiode 378, mean reward 17.55, eps 0.02, fps 52.12, trained time 04:03:05\n",
      "frame 775907, epsiode 379, mean reward 17.60, eps 0.02, fps 52.78, trained time 04:03:39\n",
      "frame 777873, epsiode 380, mean reward 17.56, eps 0.02, fps 52.80, trained time 04:04:16\n",
      "frame 779718, epsiode 381, mean reward 17.54, eps 0.02, fps 52.90, trained time 04:04:51\n",
      "frame 781417, epsiode 382, mean reward 17.57, eps 0.02, fps 53.12, trained time 04:05:23\n",
      "frame 783119, epsiode 383, mean reward 17.59, eps 0.02, fps 52.78, trained time 04:05:55\n",
      "frame 784756, epsiode 384, mean reward 17.62, eps 0.02, fps 52.97, trained time 04:06:26\n",
      "frame 786474, epsiode 385, mean reward 17.71, eps 0.02, fps 52.82, trained time 04:06:59\n",
      "frame 788494, epsiode 386, mean reward 17.68, eps 0.02, fps 52.60, trained time 04:07:37\n",
      "frame 790264, epsiode 387, mean reward 17.69, eps 0.02, fps 52.65, trained time 04:08:11\n",
      "frame 792255, epsiode 388, mean reward 17.73, eps 0.02, fps 53.19, trained time 04:08:48\n",
      "frame 794061, epsiode 389, mean reward 17.79, eps 0.02, fps 52.83, trained time 04:09:22\n",
      "frame 796328, epsiode 390, mean reward 17.76, eps 0.02, fps 53.13, trained time 04:10:05\n",
      "frame 797964, epsiode 391, mean reward 17.80, eps 0.02, fps 53.14, trained time 04:10:36\n",
      "frame 799602, epsiode 392, mean reward 17.82, eps 0.02, fps 52.71, trained time 04:11:07\n",
      "frame 801514, epsiode 393, mean reward 17.84, eps 0.02, fps 53.04, trained time 04:11:43\n",
      "frame 803702, epsiode 394, mean reward 17.85, eps 0.02, fps 52.75, trained time 04:12:24\n",
      "frame 805482, epsiode 395, mean reward 17.87, eps 0.02, fps 53.30, trained time 04:12:58\n",
      "frame 807424, epsiode 396, mean reward 17.89, eps 0.02, fps 52.61, trained time 04:13:35\n",
      "frame 809142, epsiode 397, mean reward 17.89, eps 0.02, fps 52.68, trained time 04:14:07\n",
      "frame 810998, epsiode 398, mean reward 17.95, eps 0.02, fps 51.99, trained time 04:14:43\n",
      "frame 812636, epsiode 399, mean reward 17.98, eps 0.02, fps 52.90, trained time 04:15:14\n",
      "frame 814551, epsiode 400, mean reward 18.02, eps 0.02, fps 53.09, trained time 04:15:50\n",
      "frame 816241, epsiode 401, mean reward 18.05, eps 0.02, fps 52.82, trained time 04:16:22\n",
      "frame 817932, epsiode 402, mean reward 18.06, eps 0.02, fps 52.90, trained time 04:16:54\n",
      "frame 820006, epsiode 403, mean reward 18.04, eps 0.02, fps 52.99, trained time 04:17:33\n",
      "frame 821698, epsiode 404, mean reward 18.04, eps 0.02, fps 52.43, trained time 04:18:05\n",
      "frame 823334, epsiode 405, mean reward 18.04, eps 0.02, fps 53.13, trained time 04:18:36\n",
      "frame 825393, epsiode 406, mean reward 18.02, eps 0.02, fps 52.93, trained time 04:19:15\n",
      "frame 827029, epsiode 407, mean reward 18.04, eps 0.02, fps 53.33, trained time 04:19:46\n",
      "frame 828867, epsiode 408, mean reward 18.06, eps 0.02, fps 52.70, trained time 04:20:21\n",
      "frame 830616, epsiode 409, mean reward 18.07, eps 0.02, fps 52.64, trained time 04:20:54\n",
      "frame 832550, epsiode 410, mean reward 18.07, eps 0.02, fps 52.92, trained time 04:21:30\n",
      "frame 834240, epsiode 411, mean reward 18.15, eps 0.02, fps 53.22, trained time 04:22:02\n",
      "frame 835875, epsiode 412, mean reward 18.16, eps 0.02, fps 52.58, trained time 04:22:33\n",
      "frame 837574, epsiode 413, mean reward 18.17, eps 0.02, fps 52.68, trained time 04:23:06\n",
      "frame 839213, epsiode 414, mean reward 18.19, eps 0.02, fps 52.81, trained time 04:23:37\n",
      "frame 841029, epsiode 415, mean reward 18.39, eps 0.02, fps 52.88, trained time 04:24:11\n",
      "frame 842664, epsiode 416, mean reward 18.41, eps 0.02, fps 52.58, trained time 04:24:42\n",
      "frame 844299, epsiode 417, mean reward 18.41, eps 0.02, fps 52.33, trained time 04:25:13\n",
      "frame 846102, epsiode 418, mean reward 18.42, eps 0.02, fps 53.14, trained time 04:25:47\n",
      "frame 847739, epsiode 419, mean reward 18.48, eps 0.02, fps 52.72, trained time 04:26:18\n",
      "frame 849472, epsiode 420, mean reward 18.50, eps 0.02, fps 53.15, trained time 04:26:51\n",
      "frame 851469, epsiode 421, mean reward 18.45, eps 0.02, fps 52.50, trained time 04:27:29\n",
      "frame 853366, epsiode 422, mean reward 18.44, eps 0.02, fps 53.06, trained time 04:28:05\n",
      "frame 855058, epsiode 423, mean reward 18.47, eps 0.02, fps 53.42, trained time 04:28:36\n",
      "frame 856695, epsiode 424, mean reward 18.51, eps 0.02, fps 52.48, trained time 04:29:08\n",
      "frame 858529, epsiode 425, mean reward 18.49, eps 0.02, fps 52.50, trained time 04:29:42\n",
      "frame 860255, epsiode 426, mean reward 18.54, eps 0.02, fps 53.14, trained time 04:30:15\n",
      "frame 861890, epsiode 427, mean reward 18.55, eps 0.02, fps 53.08, trained time 04:30:46\n",
      "frame 863801, epsiode 428, mean reward 18.56, eps 0.02, fps 53.25, trained time 04:31:22\n",
      "frame 865436, epsiode 429, mean reward 18.59, eps 0.02, fps 52.45, trained time 04:31:53\n",
      "frame 867247, epsiode 430, mean reward 18.57, eps 0.02, fps 52.72, trained time 04:32:27\n",
      "frame 869115, epsiode 431, mean reward 18.56, eps 0.02, fps 53.47, trained time 04:33:02\n",
      "frame 870750, epsiode 432, mean reward 18.57, eps 0.02, fps 53.07, trained time 04:33:33\n",
      "frame 872748, epsiode 433, mean reward 18.55, eps 0.02, fps 53.00, trained time 04:34:11\n",
      "frame 874383, epsiode 434, mean reward 18.56, eps 0.02, fps 53.12, trained time 04:34:41\n",
      "frame 876265, epsiode 435, mean reward 18.55, eps 0.02, fps 52.69, trained time 04:35:17\n",
      "frame 877970, epsiode 436, mean reward 18.56, eps 0.02, fps 52.69, trained time 04:35:49\n",
      "frame 879603, epsiode 437, mean reward 18.62, eps 0.02, fps 53.00, trained time 04:36:20\n",
      "frame 881502, epsiode 438, mean reward 18.61, eps 0.02, fps 53.10, trained time 04:36:56\n",
      "frame 883242, epsiode 439, mean reward 18.61, eps 0.02, fps 53.03, trained time 04:37:29\n",
      "frame 885079, epsiode 440, mean reward 18.65, eps 0.02, fps 53.29, trained time 04:38:03\n",
      "frame 886966, epsiode 441, mean reward 18.68, eps 0.02, fps 53.01, trained time 04:38:39\n",
      "frame 888766, epsiode 442, mean reward 18.67, eps 0.02, fps 53.09, trained time 04:39:13\n",
      "frame 891064, epsiode 443, mean reward 18.68, eps 0.02, fps 53.35, trained time 04:39:56\n",
      "frame 893297, epsiode 444, mean reward 18.54, eps 0.02, fps 53.24, trained time 04:40:38\n",
      "frame 895311, epsiode 445, mean reward 18.54, eps 0.02, fps 53.19, trained time 04:41:16\n",
      "frame 897286, epsiode 446, mean reward 18.53, eps 0.02, fps 52.90, trained time 04:41:53\n",
      "frame 899036, epsiode 447, mean reward 18.54, eps 0.02, fps 52.91, trained time 04:42:26\n",
      "frame 901157, epsiode 448, mean reward 18.45, eps 0.02, fps 52.61, trained time 04:43:06\n",
      "frame 902978, epsiode 449, mean reward 18.45, eps 0.02, fps 53.37, trained time 04:43:41\n",
      "frame 904721, epsiode 450, mean reward 18.49, eps 0.02, fps 53.03, trained time 04:44:13\n",
      "frame 906785, epsiode 451, mean reward 18.42, eps 0.02, fps 52.09, trained time 04:44:53\n",
      "frame 908589, epsiode 452, mean reward 18.43, eps 0.02, fps 52.98, trained time 04:45:27\n",
      "frame 910563, epsiode 453, mean reward 18.46, eps 0.02, fps 53.14, trained time 04:46:04\n",
      "frame 912202, epsiode 454, mean reward 18.50, eps 0.02, fps 53.31, trained time 04:46:35\n",
      "frame 913896, epsiode 455, mean reward 18.51, eps 0.02, fps 52.85, trained time 04:47:07\n",
      "frame 915535, epsiode 456, mean reward 18.59, eps 0.02, fps 53.70, trained time 04:47:38\n",
      "frame 917257, epsiode 457, mean reward 18.59, eps 0.02, fps 52.99, trained time 04:48:10\n",
      "frame 919314, epsiode 458, mean reward 18.57, eps 0.02, fps 52.99, trained time 04:48:49\n",
      "frame 921356, epsiode 459, mean reward 18.58, eps 0.02, fps 53.21, trained time 04:49:27\n",
      "frame 923196, epsiode 460, mean reward 18.56, eps 0.02, fps 53.24, trained time 04:50:02\n",
      "frame 925148, epsiode 461, mean reward 18.56, eps 0.02, fps 53.34, trained time 04:50:38\n",
      "frame 926786, epsiode 462, mean reward 18.59, eps 0.02, fps 52.98, trained time 04:51:09\n",
      "frame 928791, epsiode 463, mean reward 18.56, eps 0.02, fps 53.05, trained time 04:51:47\n",
      "frame 930426, epsiode 464, mean reward 18.57, eps 0.02, fps 53.11, trained time 04:52:18\n",
      "frame 932395, epsiode 465, mean reward 18.53, eps 0.02, fps 53.52, trained time 04:52:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 934034, epsiode 466, mean reward 18.59, eps 0.02, fps 53.23, trained time 04:53:25\n",
      "frame 935670, epsiode 467, mean reward 18.62, eps 0.02, fps 53.16, trained time 04:53:56\n",
      "frame 937406, epsiode 468, mean reward 18.61, eps 0.02, fps 52.91, trained time 04:54:29\n",
      "frame 939515, epsiode 469, mean reward 18.59, eps 0.02, fps 52.97, trained time 04:55:09\n",
      "frame 941157, epsiode 470, mean reward 18.59, eps 0.02, fps 53.10, trained time 04:55:40\n",
      "frame 943032, epsiode 471, mean reward 18.57, eps 0.02, fps 53.35, trained time 04:56:15\n",
      "frame 944667, epsiode 472, mean reward 18.59, eps 0.02, fps 53.04, trained time 04:56:46\n",
      "frame 946536, epsiode 473, mean reward 18.56, eps 0.02, fps 53.23, trained time 04:57:21\n",
      "frame 948326, epsiode 474, mean reward 18.64, eps 0.02, fps 53.54, trained time 04:57:54\n",
      "frame 950049, epsiode 475, mean reward 18.64, eps 0.02, fps 53.30, trained time 04:58:27\n",
      "frame 951684, epsiode 476, mean reward 18.66, eps 0.02, fps 53.01, trained time 04:58:57\n",
      "frame 953564, epsiode 477, mean reward 18.68, eps 0.02, fps 53.10, trained time 04:59:33\n",
      "frame 955341, epsiode 478, mean reward 18.68, eps 0.02, fps 52.61, trained time 05:00:07\n",
      "frame 957090, epsiode 479, mean reward 18.69, eps 0.02, fps 53.25, trained time 05:00:40\n",
      "frame 959036, epsiode 480, mean reward 18.73, eps 0.02, fps 53.47, trained time 05:01:16\n",
      "frame 960672, epsiode 481, mean reward 18.75, eps 0.02, fps 53.17, trained time 05:01:47\n",
      "frame 962789, epsiode 482, mean reward 18.70, eps 0.02, fps 53.23, trained time 05:02:26\n",
      "frame 964489, epsiode 483, mean reward 18.70, eps 0.02, fps 53.23, trained time 05:02:58\n",
      "frame 966311, epsiode 484, mean reward 18.69, eps 0.02, fps 53.03, trained time 05:03:33\n",
      "frame 968201, epsiode 485, mean reward 18.68, eps 0.02, fps 53.20, trained time 05:04:08\n",
      "frame 969840, epsiode 486, mean reward 18.72, eps 0.02, fps 52.44, trained time 05:04:40\n",
      "frame 971673, epsiode 487, mean reward 18.72, eps 0.02, fps 52.61, trained time 05:05:14\n",
      "frame 973473, epsiode 488, mean reward 18.75, eps 0.02, fps 53.03, trained time 05:05:48\n",
      "frame 975111, epsiode 489, mean reward 18.78, eps 0.02, fps 53.54, trained time 05:06:19\n",
      "frame 976749, epsiode 490, mean reward 18.88, eps 0.02, fps 53.39, trained time 05:06:50\n",
      "frame 978678, epsiode 491, mean reward 18.86, eps 0.02, fps 53.05, trained time 05:07:26\n",
      "frame 980494, epsiode 492, mean reward 18.85, eps 0.02, fps 53.49, trained time 05:08:00\n",
      "frame 982133, epsiode 493, mean reward 18.88, eps 0.02, fps 53.35, trained time 05:08:31\n",
      "frame 983938, epsiode 494, mean reward 18.92, eps 0.02, fps 53.34, trained time 05:09:04\n",
      "frame 985701, epsiode 495, mean reward 18.92, eps 0.02, fps 53.50, trained time 05:09:37\n",
      "frame 987411, epsiode 496, mean reward 18.94, eps 0.02, fps 53.18, trained time 05:10:10\n",
      "frame 989231, epsiode 497, mean reward 18.93, eps 0.02, fps 52.82, trained time 05:10:44\n",
      "frame 991014, epsiode 498, mean reward 18.93, eps 0.02, fps 53.19, trained time 05:11:18\n",
      "frame 992869, epsiode 499, mean reward 18.91, eps 0.02, fps 53.46, trained time 05:11:52\n",
      "frame 994747, epsiode 500, mean reward 18.90, eps 0.02, fps 53.23, trained time 05:12:28\n",
      "frame 996671, epsiode 501, mean reward 18.88, eps 0.02, fps 53.05, trained time 05:13:04\n",
      "frame 998545, epsiode 502, mean reward 18.86, eps 0.02, fps 53.30, trained time 05:13:39\n",
      "frame 1000274, epsiode 503, mean reward 18.91, eps 0.02, fps 52.98, trained time 05:14:12\n",
      "frame 1002257, epsiode 504, mean reward 18.89, eps 0.02, fps 52.13, trained time 05:14:50\n",
      "frame 1004034, epsiode 505, mean reward 18.88, eps 0.02, fps 53.36, trained time 05:15:23\n",
      "frame 1005670, epsiode 506, mean reward 18.94, eps 0.02, fps 52.98, trained time 05:15:54\n",
      "frame 1007493, epsiode 507, mean reward 18.92, eps 0.02, fps 53.33, trained time 05:16:28\n",
      "frame 1009129, epsiode 508, mean reward 18.93, eps 0.02, fps 52.77, trained time 05:16:59\n",
      "frame 1010764, epsiode 509, mean reward 18.94, eps 0.02, fps 53.17, trained time 05:17:30\n",
      "frame 1012477, epsiode 510, mean reward 18.97, eps 0.02, fps 53.27, trained time 05:18:02\n",
      "frame 1014112, epsiode 511, mean reward 18.98, eps 0.02, fps 53.50, trained time 05:18:32\n",
      "frame 1015747, epsiode 512, mean reward 18.98, eps 0.02, fps 52.74, trained time 05:19:03\n",
      "frame 1017452, epsiode 513, mean reward 18.98, eps 0.02, fps 53.54, trained time 05:19:35\n",
      "frame 1019395, epsiode 514, mean reward 18.93, eps 0.02, fps 53.31, trained time 05:20:12\n",
      "frame 1021204, epsiode 515, mean reward 18.94, eps 0.02, fps 53.08, trained time 05:20:46\n",
      "frame 1022840, epsiode 516, mean reward 18.94, eps 0.02, fps 52.49, trained time 05:21:17\n",
      "frame 1024571, epsiode 517, mean reward 18.92, eps 0.02, fps 53.05, trained time 05:21:50\n",
      "frame 1026616, epsiode 518, mean reward 18.85, eps 0.02, fps 52.60, trained time 05:22:29\n",
      "frame 1028252, epsiode 519, mean reward 18.85, eps 0.02, fps 52.45, trained time 05:23:00\n",
      "frame 1030216, epsiode 520, mean reward 18.83, eps 0.02, fps 53.25, trained time 05:23:37\n",
      "frame 1032026, epsiode 521, mean reward 18.87, eps 0.02, fps 53.09, trained time 05:24:11\n",
      "frame 1033928, epsiode 522, mean reward 18.89, eps 0.02, fps 52.77, trained time 05:24:47\n",
      "frame 1035560, epsiode 523, mean reward 18.90, eps 0.02, fps 53.02, trained time 05:25:18\n",
      "frame 1037341, epsiode 524, mean reward 18.88, eps 0.02, fps 53.30, trained time 05:25:51\n",
      "frame 1039500, epsiode 525, mean reward 18.84, eps 0.02, fps 52.90, trained time 05:26:32\n",
      "frame 1041201, epsiode 526, mean reward 18.84, eps 0.02, fps 53.06, trained time 05:27:04\n",
      "frame 1042888, epsiode 527, mean reward 18.83, eps 0.02, fps 53.25, trained time 05:27:35\n",
      "frame 1044525, epsiode 528, mean reward 18.85, eps 0.02, fps 53.10, trained time 05:28:06\n",
      "frame 1046414, epsiode 529, mean reward 18.81, eps 0.02, fps 52.96, trained time 05:28:42\n",
      "frame 1048189, epsiode 530, mean reward 18.82, eps 0.02, fps 53.23, trained time 05:29:15\n",
      "frame 1050525, epsiode 531, mean reward 18.62, eps 0.02, fps 53.07, trained time 05:29:59\n",
      "frame 1052471, epsiode 532, mean reward 18.60, eps 0.02, fps 53.64, trained time 05:30:36\n",
      "frame 1054187, epsiode 533, mean reward 18.63, eps 0.02, fps 53.05, trained time 05:31:08\n",
      "frame 1055822, epsiode 534, mean reward 18.63, eps 0.02, fps 53.83, trained time 05:31:38\n",
      "frame 1057633, epsiode 535, mean reward 18.64, eps 0.02, fps 45.54, trained time 05:32:18\n",
      "frame 1059268, epsiode 536, mean reward 18.65, eps 0.02, fps 48.45, trained time 05:32:52\n",
      "frame 1061153, epsiode 537, mean reward 18.63, eps 0.02, fps 49.74, trained time 05:33:30\n",
      "frame 1062820, epsiode 538, mean reward 18.66, eps 0.02, fps 50.03, trained time 05:34:03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\563920051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m19.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\1048435760.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env_name, agent, writer, reward_bound, replay_start_size, sync_frames)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mloss_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\1704343668.py\u001b[0m in \u001b[0;36mcal_loss\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# next q action values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# double DQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_actions_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# choose next action from the net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             next_q_action_values_v = self.target_net(next_states_v).gather( # but use the value from the target net\n\u001b[0;32m     83\u001b[0m                 1, next_actions_v.unsqueeze(-1)).squeeze(-1)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\1665755084.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0madv\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0madv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# - adv.mean() to ensure advantage function has 0 mean value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mfc_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfc_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfc_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(env_name, agent, writer, reward_bound=19.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8a2a3",
   "metadata": {},
   "source": [
    "#### 4.3 train the Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccca44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][2]\n",
    "writer = SummaryWriter(comment=\"-\"+ str(agent)  + \"-\" + env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "407ed6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 896, epsiode 1, mean reward -20.00, eps 0.99, fps 740.26, trained time 00:00:01\n",
      "frame 1737, epsiode 2, mean reward -20.50, eps 0.98, fps 675.00, trained time 00:00:02\n",
      "frame 2656, epsiode 3, mean reward -20.33, eps 0.97, fps 667.39, trained time 00:00:03\n",
      "frame 3602, epsiode 4, mean reward -20.50, eps 0.96, fps 635.52, trained time 00:00:05\n",
      "frame 4411, epsiode 5, mean reward -20.60, eps 0.96, fps 630.16, trained time 00:00:06\n",
      "frame 5261, epsiode 6, mean reward -20.67, eps 0.95, fps 627.95, trained time 00:00:07\n",
      "frame 6023, epsiode 7, mean reward -20.71, eps 0.94, fps 598.14, trained time 00:00:09\n",
      "frame 6785, epsiode 8, mean reward -20.75, eps 0.93, fps 604.43, trained time 00:00:10\n",
      "frame 7547, epsiode 9, mean reward -20.78, eps 0.92, fps 605.07, trained time 00:00:11\n",
      "frame 8537, epsiode 10, mean reward -20.60, eps 0.91, fps 585.62, trained time 00:00:13\n",
      "frame 9299, epsiode 11, mean reward -20.64, eps 0.91, fps 627.79, trained time 00:00:14\n",
      "frame 10121, epsiode 12, mean reward -20.67, eps 0.90, fps 223.76, trained time 00:00:18\n",
      "frame 10939, epsiode 13, mean reward -20.69, eps 0.89, fps 50.43, trained time 00:00:34\n",
      "frame 11849, epsiode 14, mean reward -20.71, eps 0.88, fps 49.94, trained time 00:00:52\n",
      "frame 12689, epsiode 15, mean reward -20.67, eps 0.87, fps 46.68, trained time 00:01:10\n",
      "frame 13710, epsiode 16, mean reward -20.62, eps 0.86, fps 47.79, trained time 00:01:32\n",
      "frame 14551, epsiode 17, mean reward -20.65, eps 0.85, fps 46.47, trained time 00:01:50\n",
      "frame 15402, epsiode 18, mean reward -20.67, eps 0.85, fps 50.80, trained time 00:02:06\n",
      "frame 16316, epsiode 19, mean reward -20.68, eps 0.84, fps 47.31, trained time 00:02:26\n",
      "frame 17138, epsiode 20, mean reward -20.70, eps 0.83, fps 51.09, trained time 00:02:42\n",
      "frame 18113, epsiode 21, mean reward -20.67, eps 0.82, fps 49.28, trained time 00:03:02\n",
      "frame 19094, epsiode 22, mean reward -20.64, eps 0.81, fps 50.89, trained time 00:03:21\n",
      "frame 20066, epsiode 23, mean reward -20.65, eps 0.80, fps 50.86, trained time 00:03:40\n",
      "frame 20961, epsiode 24, mean reward -20.62, eps 0.79, fps 51.89, trained time 00:03:57\n",
      "frame 21783, epsiode 25, mean reward -20.64, eps 0.78, fps 52.19, trained time 00:04:13\n",
      "frame 22647, epsiode 26, mean reward -20.62, eps 0.77, fps 50.50, trained time 00:04:30\n",
      "frame 23428, epsiode 27, mean reward -20.63, eps 0.77, fps 51.90, trained time 00:04:45\n",
      "frame 24306, epsiode 28, mean reward -20.64, eps 0.76, fps 52.17, trained time 00:05:02\n",
      "frame 25223, epsiode 29, mean reward -20.62, eps 0.75, fps 51.64, trained time 00:05:20\n",
      "frame 26045, epsiode 30, mean reward -20.63, eps 0.74, fps 52.01, trained time 00:05:36\n",
      "frame 27011, epsiode 31, mean reward -20.61, eps 0.73, fps 51.66, trained time 00:05:54\n",
      "frame 27887, epsiode 32, mean reward -20.59, eps 0.72, fps 51.20, trained time 00:06:11\n",
      "frame 28998, epsiode 33, mean reward -20.55, eps 0.71, fps 51.88, trained time 00:06:33\n",
      "frame 29835, epsiode 34, mean reward -20.56, eps 0.70, fps 51.40, trained time 00:06:49\n",
      "frame 31081, epsiode 35, mean reward -20.51, eps 0.69, fps 51.85, trained time 00:07:13\n",
      "frame 32089, epsiode 36, mean reward -20.47, eps 0.68, fps 51.24, trained time 00:07:33\n",
      "frame 33129, epsiode 37, mean reward -20.43, eps 0.67, fps 51.61, trained time 00:07:53\n",
      "frame 33981, epsiode 38, mean reward -20.45, eps 0.66, fps 51.91, trained time 00:08:09\n",
      "frame 35203, epsiode 39, mean reward -20.41, eps 0.65, fps 51.20, trained time 00:08:33\n",
      "frame 36203, epsiode 40, mean reward -20.43, eps 0.64, fps 51.57, trained time 00:08:53\n",
      "frame 37110, epsiode 41, mean reward -20.44, eps 0.63, fps 51.62, trained time 00:09:10\n",
      "frame 38417, epsiode 42, mean reward -20.38, eps 0.62, fps 51.69, trained time 00:09:36\n",
      "frame 39749, epsiode 43, mean reward -20.33, eps 0.60, fps 51.04, trained time 00:10:02\n",
      "frame 41059, epsiode 44, mean reward -20.30, eps 0.59, fps 51.50, trained time 00:10:27\n",
      "frame 42174, epsiode 45, mean reward -20.27, eps 0.58, fps 51.54, trained time 00:10:49\n",
      "frame 43593, epsiode 46, mean reward -20.22, eps 0.56, fps 51.65, trained time 00:11:16\n",
      "frame 45100, epsiode 47, mean reward -20.19, eps 0.55, fps 51.41, trained time 00:11:45\n",
      "frame 46384, epsiode 48, mean reward -20.12, eps 0.54, fps 51.48, trained time 00:12:10\n",
      "frame 47598, epsiode 49, mean reward -20.10, eps 0.52, fps 51.13, trained time 00:12:34\n",
      "frame 48921, epsiode 50, mean reward -20.08, eps 0.51, fps 50.70, trained time 00:13:00\n",
      "frame 50603, epsiode 51, mean reward -20.06, eps 0.49, fps 51.23, trained time 00:13:33\n",
      "frame 51908, epsiode 52, mean reward -20.06, eps 0.48, fps 51.21, trained time 00:13:59\n",
      "frame 53769, epsiode 53, mean reward -20.00, eps 0.46, fps 51.07, trained time 00:14:35\n",
      "frame 55350, epsiode 54, mean reward -19.98, eps 0.45, fps 50.17, trained time 00:15:07\n",
      "frame 57043, epsiode 55, mean reward -19.93, eps 0.43, fps 50.79, trained time 00:15:40\n",
      "frame 59032, epsiode 56, mean reward -19.86, eps 0.41, fps 50.62, trained time 00:16:19\n",
      "frame 60968, epsiode 57, mean reward -19.82, eps 0.39, fps 50.84, trained time 00:16:57\n",
      "frame 62724, epsiode 58, mean reward -19.76, eps 0.37, fps 50.84, trained time 00:17:32\n",
      "frame 64867, epsiode 59, mean reward -19.71, eps 0.35, fps 50.51, trained time 00:18:14\n",
      "frame 66755, epsiode 60, mean reward -19.67, eps 0.33, fps 50.29, trained time 00:18:52\n",
      "frame 67972, epsiode 61, mean reward -19.64, eps 0.32, fps 50.23, trained time 00:19:16\n",
      "frame 69577, epsiode 62, mean reward -19.58, eps 0.30, fps 50.43, trained time 00:19:48\n",
      "frame 71459, epsiode 63, mean reward -19.56, eps 0.29, fps 50.33, trained time 00:20:25\n",
      "frame 73390, epsiode 64, mean reward -19.55, eps 0.27, fps 50.15, trained time 00:21:04\n",
      "frame 75613, epsiode 65, mean reward -19.46, eps 0.24, fps 49.99, trained time 00:21:48\n",
      "frame 77340, epsiode 66, mean reward -19.42, eps 0.23, fps 50.15, trained time 00:22:23\n",
      "frame 79061, epsiode 67, mean reward -19.39, eps 0.21, fps 50.04, trained time 00:22:57\n",
      "frame 81231, epsiode 68, mean reward -19.31, eps 0.19, fps 50.10, trained time 00:23:40\n",
      "frame 83000, epsiode 69, mean reward -19.26, eps 0.17, fps 49.84, trained time 00:24:16\n",
      "frame 84536, epsiode 70, mean reward -19.26, eps 0.15, fps 50.22, trained time 00:24:46\n",
      "frame 86526, epsiode 71, mean reward -19.20, eps 0.13, fps 50.08, trained time 00:25:26\n",
      "frame 88543, epsiode 72, mean reward -19.14, eps 0.11, fps 49.89, trained time 00:26:07\n",
      "frame 90783, epsiode 73, mean reward -19.07, eps 0.09, fps 49.88, trained time 00:26:51\n",
      "frame 93027, epsiode 74, mean reward -19.01, eps 0.07, fps 49.82, trained time 00:27:36\n",
      "frame 95867, epsiode 75, mean reward -18.92, eps 0.04, fps 49.55, trained time 00:28:34\n",
      "frame 97419, epsiode 76, mean reward -18.93, eps 0.03, fps 49.46, trained time 00:29:05\n",
      "frame 100430, epsiode 77, mean reward -18.83, eps 0.02, fps 49.81, trained time 00:30:06\n",
      "frame 102847, epsiode 78, mean reward -18.76, eps 0.02, fps 49.82, trained time 00:30:54\n",
      "frame 105526, epsiode 79, mean reward -18.63, eps 0.02, fps 49.83, trained time 00:31:48\n",
      "frame 108372, epsiode 80, mean reward -18.54, eps 0.02, fps 49.91, trained time 00:32:45\n",
      "frame 111823, epsiode 81, mean reward -18.36, eps 0.02, fps 49.78, trained time 00:33:54\n",
      "frame 114484, epsiode 82, mean reward -18.28, eps 0.02, fps 49.13, trained time 00:34:48\n",
      "frame 117475, epsiode 83, mean reward -18.23, eps 0.02, fps 49.77, trained time 00:35:49\n",
      "frame 120301, epsiode 84, mean reward -18.12, eps 0.02, fps 49.65, trained time 00:36:45\n",
      "frame 123798, epsiode 85, mean reward -18.00, eps 0.02, fps 49.72, trained time 00:37:56\n",
      "frame 125792, epsiode 86, mean reward -17.98, eps 0.02, fps 49.64, trained time 00:38:36\n",
      "frame 129585, epsiode 87, mean reward -17.91, eps 0.02, fps 49.62, trained time 00:39:52\n",
      "frame 132228, epsiode 88, mean reward -17.84, eps 0.02, fps 49.56, trained time 00:40:46\n",
      "frame 135772, epsiode 89, mean reward -17.70, eps 0.02, fps 49.57, trained time 00:41:57\n",
      "frame 140180, epsiode 90, mean reward -17.51, eps 0.02, fps 49.90, trained time 00:43:26\n",
      "frame 144503, epsiode 91, mean reward -17.30, eps 0.02, fps 49.74, trained time 00:44:52\n",
      "frame 148610, epsiode 92, mean reward -17.18, eps 0.02, fps 49.10, trained time 00:46:16\n",
      "frame 152435, epsiode 93, mean reward -17.04, eps 0.02, fps 48.94, trained time 00:47:34\n",
      "frame 156174, epsiode 94, mean reward -16.89, eps 0.02, fps 49.51, trained time 00:48:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 159397, epsiode 95, mean reward -16.75, eps 0.02, fps 49.21, trained time 00:49:55\n",
      "frame 162293, epsiode 96, mean reward -16.52, eps 0.02, fps 50.06, trained time 00:50:53\n",
      "frame 165769, epsiode 97, mean reward -16.42, eps 0.02, fps 49.09, trained time 00:52:04\n",
      "frame 168471, epsiode 98, mean reward -16.13, eps 0.02, fps 49.79, trained time 00:52:58\n",
      "frame 171239, epsiode 99, mean reward -15.88, eps 0.02, fps 49.85, trained time 00:53:54\n",
      "frame 173868, epsiode 100, mean reward -15.62, eps 0.02, fps 49.79, trained time 00:54:46\n",
      "frame 176141, epsiode 101, mean reward -15.32, eps 0.02, fps 49.52, trained time 00:55:32\n",
      "frame 178138, epsiode 102, mean reward -14.92, eps 0.02, fps 49.71, trained time 00:56:13\n",
      "frame 180033, epsiode 103, mean reward -14.54, eps 0.02, fps 49.55, trained time 00:56:51\n",
      "frame 181673, epsiode 104, mean reward -14.12, eps 0.02, fps 49.24, trained time 00:57:24\n",
      "frame 183434, epsiode 105, mean reward -13.72, eps 0.02, fps 48.95, trained time 00:58:00\n",
      "frame 185073, epsiode 106, mean reward -13.30, eps 0.02, fps 50.00, trained time 00:58:33\n",
      "frame 186851, epsiode 107, mean reward -12.90, eps 0.02, fps 49.98, trained time 00:59:08\n",
      "frame 188801, epsiode 108, mean reward -12.53, eps 0.02, fps 49.89, trained time 00:59:48\n",
      "frame 190601, epsiode 109, mean reward -12.13, eps 0.02, fps 50.05, trained time 01:00:23\n",
      "frame 192568, epsiode 110, mean reward -11.78, eps 0.02, fps 49.60, trained time 01:01:03\n",
      "frame 194404, epsiode 111, mean reward -11.37, eps 0.02, fps 49.70, trained time 01:01:40\n",
      "frame 196550, epsiode 112, mean reward -11.04, eps 0.02, fps 49.45, trained time 01:02:23\n",
      "frame 198608, epsiode 113, mean reward -10.68, eps 0.02, fps 49.63, trained time 01:03:05\n",
      "frame 200803, epsiode 114, mean reward -10.33, eps 0.02, fps 49.72, trained time 01:03:49\n",
      "frame 202770, epsiode 115, mean reward -9.98, eps 0.02, fps 49.06, trained time 01:04:29\n",
      "frame 204692, epsiode 116, mean reward -9.61, eps 0.02, fps 49.64, trained time 01:05:08\n",
      "frame 206636, epsiode 117, mean reward -9.22, eps 0.02, fps 49.64, trained time 01:05:47\n",
      "frame 208461, epsiode 118, mean reward -8.82, eps 0.02, fps 44.48, trained time 01:06:28\n",
      "frame 210214, epsiode 119, mean reward -8.41, eps 0.02, fps 44.31, trained time 01:07:08\n",
      "frame 212005, epsiode 120, mean reward -8.00, eps 0.02, fps 49.82, trained time 01:07:44\n",
      "frame 213703, epsiode 121, mean reward -7.60, eps 0.02, fps 49.97, trained time 01:08:18\n",
      "frame 215575, epsiode 122, mean reward -7.22, eps 0.02, fps 50.00, trained time 01:08:55\n",
      "frame 217214, epsiode 123, mean reward -6.80, eps 0.02, fps 50.06, trained time 01:09:28\n",
      "frame 218972, epsiode 124, mean reward -6.41, eps 0.02, fps 49.88, trained time 01:10:03\n",
      "frame 220793, epsiode 125, mean reward -6.02, eps 0.02, fps 49.79, trained time 01:10:40\n",
      "frame 222697, epsiode 126, mean reward -5.64, eps 0.02, fps 48.50, trained time 01:11:19\n",
      "frame 224336, epsiode 127, mean reward -5.22, eps 0.02, fps 49.67, trained time 01:11:52\n",
      "frame 225976, epsiode 128, mean reward -4.80, eps 0.02, fps 49.66, trained time 01:12:25\n",
      "frame 227898, epsiode 129, mean reward -4.42, eps 0.02, fps 49.80, trained time 01:13:03\n",
      "frame 229656, epsiode 130, mean reward -4.01, eps 0.02, fps 49.95, trained time 01:13:39\n",
      "frame 231436, epsiode 131, mean reward -3.63, eps 0.02, fps 48.77, trained time 01:14:15\n",
      "frame 233456, epsiode 132, mean reward -3.29, eps 0.02, fps 49.19, trained time 01:14:56\n",
      "frame 235598, epsiode 133, mean reward -2.95, eps 0.02, fps 49.71, trained time 01:15:39\n",
      "frame 237384, epsiode 134, mean reward -2.55, eps 0.02, fps 49.46, trained time 01:16:15\n",
      "frame 239423, epsiode 135, mean reward -2.19, eps 0.02, fps 49.59, trained time 01:16:57\n",
      "frame 241153, epsiode 136, mean reward -1.80, eps 0.02, fps 49.67, trained time 01:17:31\n",
      "frame 242852, epsiode 137, mean reward -1.41, eps 0.02, fps 49.85, trained time 01:18:05\n",
      "frame 244600, epsiode 138, mean reward -1.00, eps 0.02, fps 49.78, trained time 01:18:41\n",
      "frame 246236, epsiode 139, mean reward -0.60, eps 0.02, fps 49.25, trained time 01:19:14\n",
      "frame 247955, epsiode 140, mean reward -0.20, eps 0.02, fps 49.77, trained time 01:19:48\n",
      "frame 249865, epsiode 141, mean reward 0.18, eps 0.02, fps 49.90, trained time 01:20:27\n",
      "frame 251583, epsiode 142, mean reward 0.56, eps 0.02, fps 49.65, trained time 01:21:01\n",
      "frame 253218, epsiode 143, mean reward 0.95, eps 0.02, fps 49.32, trained time 01:21:34\n",
      "frame 255058, epsiode 144, mean reward 1.34, eps 0.02, fps 49.71, trained time 01:22:11\n",
      "frame 256861, epsiode 145, mean reward 1.72, eps 0.02, fps 49.87, trained time 01:22:48\n",
      "frame 258716, epsiode 146, mean reward 2.08, eps 0.02, fps 49.86, trained time 01:23:25\n",
      "frame 260759, epsiode 147, mean reward 2.45, eps 0.02, fps 49.54, trained time 01:24:06\n",
      "frame 262637, epsiode 148, mean reward 2.79, eps 0.02, fps 49.81, trained time 01:24:44\n",
      "frame 264327, epsiode 149, mean reward 3.18, eps 0.02, fps 49.45, trained time 01:25:18\n",
      "frame 266035, epsiode 150, mean reward 3.57, eps 0.02, fps 49.76, trained time 01:25:52\n",
      "frame 267936, epsiode 151, mean reward 3.95, eps 0.02, fps 49.55, trained time 01:26:31\n",
      "frame 269697, epsiode 152, mean reward 4.34, eps 0.02, fps 46.49, trained time 01:27:08\n",
      "frame 271778, epsiode 153, mean reward 4.65, eps 0.02, fps 49.69, trained time 01:27:50\n",
      "frame 273416, epsiode 154, mean reward 5.05, eps 0.02, fps 46.59, trained time 01:28:25\n",
      "frame 275164, epsiode 155, mean reward 5.42, eps 0.02, fps 47.93, trained time 01:29:02\n",
      "frame 276854, epsiode 156, mean reward 5.78, eps 0.02, fps 49.62, trained time 01:29:36\n",
      "frame 278494, epsiode 157, mean reward 6.17, eps 0.02, fps 49.44, trained time 01:30:09\n",
      "frame 280130, epsiode 158, mean reward 6.54, eps 0.02, fps 49.79, trained time 01:30:42\n",
      "frame 281983, epsiode 159, mean reward 6.88, eps 0.02, fps 49.59, trained time 01:31:19\n",
      "frame 283909, epsiode 160, mean reward 7.21, eps 0.02, fps 49.64, trained time 01:31:58\n",
      "frame 285717, epsiode 161, mean reward 7.58, eps 0.02, fps 49.36, trained time 01:32:35\n",
      "frame 287503, epsiode 162, mean reward 7.93, eps 0.02, fps 49.34, trained time 01:33:11\n",
      "frame 289329, epsiode 163, mean reward 8.29, eps 0.02, fps 49.25, trained time 01:33:48\n",
      "frame 291105, epsiode 164, mean reward 8.67, eps 0.02, fps 48.79, trained time 01:34:24\n",
      "frame 292802, epsiode 165, mean reward 9.01, eps 0.02, fps 49.66, trained time 01:34:59\n",
      "frame 294641, epsiode 166, mean reward 9.36, eps 0.02, fps 49.48, trained time 01:35:36\n",
      "frame 296396, epsiode 167, mean reward 9.72, eps 0.02, fps 49.69, trained time 01:36:11\n",
      "frame 298121, epsiode 168, mean reward 10.05, eps 0.02, fps 49.56, trained time 01:36:46\n",
      "frame 299758, epsiode 169, mean reward 10.42, eps 0.02, fps 49.57, trained time 01:37:19\n",
      "frame 301477, epsiode 170, mean reward 10.80, eps 0.02, fps 49.51, trained time 01:37:54\n",
      "frame 303360, epsiode 171, mean reward 11.12, eps 0.02, fps 49.53, trained time 01:38:32\n",
      "frame 305178, epsiode 172, mean reward 11.45, eps 0.02, fps 49.60, trained time 01:39:08\n",
      "frame 306933, epsiode 173, mean reward 11.79, eps 0.02, fps 49.60, trained time 01:39:44\n",
      "frame 308759, epsiode 174, mean reward 12.13, eps 0.02, fps 46.88, trained time 01:40:23\n",
      "frame 310590, epsiode 175, mean reward 12.44, eps 0.02, fps 48.35, trained time 01:41:01\n",
      "frame 312284, epsiode 176, mean reward 12.84, eps 0.02, fps 49.42, trained time 01:41:35\n",
      "frame 314184, epsiode 177, mean reward 13.13, eps 0.02, fps 49.81, trained time 01:42:13\n",
      "frame 315999, epsiode 178, mean reward 13.45, eps 0.02, fps 49.87, trained time 01:42:49\n",
      "frame 318073, epsiode 179, mean reward 13.69, eps 0.02, fps 49.71, trained time 01:43:31\n",
      "frame 319762, epsiode 180, mean reward 14.00, eps 0.02, fps 49.63, trained time 01:44:05\n",
      "frame 321483, epsiode 181, mean reward 14.23, eps 0.02, fps 49.49, trained time 01:44:40\n",
      "frame 323119, epsiode 182, mean reward 14.56, eps 0.02, fps 49.47, trained time 01:45:13\n",
      "frame 324873, epsiode 183, mean reward 14.91, eps 0.02, fps 49.67, trained time 01:45:48\n",
      "frame 326512, epsiode 184, mean reward 15.21, eps 0.02, fps 49.38, trained time 01:46:21\n",
      "frame 328393, epsiode 185, mean reward 15.47, eps 0.02, fps 49.68, trained time 01:46:59\n",
      "frame 330395, epsiode 186, mean reward 15.79, eps 0.02, fps 49.71, trained time 01:47:40\n",
      "frame 332087, epsiode 187, mean reward 16.11, eps 0.02, fps 49.75, trained time 01:48:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 333752, epsiode 188, mean reward 16.43, eps 0.02, fps 49.62, trained time 01:48:47\n",
      "frame 335578, epsiode 189, mean reward 16.66, eps 0.02, fps 49.80, trained time 01:49:24\n",
      "frame 337984, epsiode 190, mean reward 16.81, eps 0.02, fps 46.87, trained time 01:50:15\n",
      "frame 339963, epsiode 191, mean reward 16.97, eps 0.02, fps 49.63, trained time 01:50:55\n",
      "frame 341779, epsiode 192, mean reward 17.23, eps 0.02, fps 49.65, trained time 01:51:32\n",
      "frame 343575, epsiode 193, mean reward 17.48, eps 0.02, fps 49.52, trained time 01:52:08\n",
      "frame 345287, epsiode 194, mean reward 17.71, eps 0.02, fps 49.63, trained time 01:52:42\n",
      "frame 346979, epsiode 195, mean reward 17.94, eps 0.02, fps 49.94, trained time 01:53:16\n",
      "frame 348780, epsiode 196, mean reward 18.08, eps 0.02, fps 49.93, trained time 01:53:52\n",
      "frame 350435, epsiode 197, mean reward 18.36, eps 0.02, fps 50.07, trained time 01:54:25\n",
      "frame 352300, epsiode 198, mean reward 18.42, eps 0.02, fps 49.76, trained time 01:55:03\n",
      "frame 354207, epsiode 199, mean reward 18.52, eps 0.02, fps 49.85, trained time 01:55:41\n",
      "The agent 'DQNAgent with Dueling DQN' has solved the game 'PongNoFrameskip-v4' in 354207 frames, 199 episodes\n"
     ]
    }
   ],
   "source": [
    "train(env_name, agent, writer, reward_bound=18.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae4cac",
   "metadata": {},
   "source": [
    "#### 4.4 train the D3QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bfd5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][3]\n",
    "writer = SummaryWriter(comment=\"-\"+ str(agent)  + \"-\" + env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1d63967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 790, epsiode 1, mean reward -21.00, eps 0.99, fps 742.41, trained time 00:00:01\n",
      "frame 1736, epsiode 2, mean reward -20.50, eps 0.98, fps 643.99, trained time 00:00:02\n",
      "frame 2636, epsiode 3, mean reward -20.33, eps 0.97, fps 616.64, trained time 00:00:03\n",
      "frame 3478, epsiode 4, mean reward -20.25, eps 0.97, fps 641.85, trained time 00:00:05\n",
      "frame 4240, epsiode 5, mean reward -20.40, eps 0.96, fps 651.10, trained time 00:00:06\n",
      "frame 5278, epsiode 6, mean reward -20.33, eps 0.95, fps 649.02, trained time 00:00:08\n",
      "frame 6202, epsiode 7, mean reward -20.29, eps 0.94, fps 663.31, trained time 00:00:09\n",
      "frame 7024, epsiode 8, mean reward -20.38, eps 0.93, fps 581.33, trained time 00:00:10\n",
      "frame 7920, epsiode 9, mean reward -20.33, eps 0.92, fps 547.96, trained time 00:00:12\n",
      "frame 8744, epsiode 10, mean reward -20.40, eps 0.91, fps 551.79, trained time 00:00:14\n",
      "frame 9594, epsiode 11, mean reward -20.45, eps 0.90, fps 549.82, trained time 00:00:15\n",
      "frame 10356, epsiode 12, mean reward -20.50, eps 0.90, fps 92.60, trained time 00:00:23\n",
      "frame 11500, epsiode 13, mean reward -20.38, eps 0.89, fps 45.31, trained time 00:00:49\n",
      "frame 12477, epsiode 14, mean reward -20.36, eps 0.88, fps 48.88, trained time 00:01:09\n",
      "frame 13570, epsiode 15, mean reward -20.27, eps 0.86, fps 49.65, trained time 00:01:31\n",
      "frame 14592, epsiode 16, mean reward -20.25, eps 0.85, fps 48.77, trained time 00:01:51\n",
      "frame 15565, epsiode 17, mean reward -20.24, eps 0.84, fps 48.91, trained time 00:02:11\n",
      "frame 16447, epsiode 18, mean reward -20.28, eps 0.84, fps 48.51, trained time 00:02:30\n",
      "frame 17271, epsiode 19, mean reward -20.32, eps 0.83, fps 48.70, trained time 00:02:46\n",
      "frame 18093, epsiode 20, mean reward -20.35, eps 0.82, fps 48.47, trained time 00:03:03\n",
      "frame 18943, epsiode 21, mean reward -20.38, eps 0.81, fps 48.57, trained time 00:03:21\n",
      "frame 19733, epsiode 22, mean reward -20.41, eps 0.80, fps 48.06, trained time 00:03:37\n",
      "frame 20569, epsiode 23, mean reward -20.39, eps 0.79, fps 48.35, trained time 00:03:55\n",
      "frame 21490, epsiode 24, mean reward -20.38, eps 0.79, fps 48.69, trained time 00:04:14\n",
      "frame 22354, epsiode 25, mean reward -20.36, eps 0.78, fps 48.83, trained time 00:04:31\n",
      "frame 23176, epsiode 26, mean reward -20.38, eps 0.77, fps 47.85, trained time 00:04:48\n",
      "frame 24135, epsiode 27, mean reward -20.41, eps 0.76, fps 48.66, trained time 00:05:08\n",
      "frame 24925, epsiode 28, mean reward -20.43, eps 0.75, fps 49.95, trained time 00:05:24\n",
      "frame 25706, epsiode 29, mean reward -20.45, eps 0.74, fps 48.67, trained time 00:05:40\n",
      "frame 26574, epsiode 30, mean reward -20.43, eps 0.73, fps 48.54, trained time 00:05:58\n",
      "frame 27563, epsiode 31, mean reward -20.39, eps 0.72, fps 48.29, trained time 00:06:18\n",
      "frame 28413, epsiode 32, mean reward -20.41, eps 0.72, fps 48.80, trained time 00:06:36\n",
      "frame 29223, epsiode 33, mean reward -20.42, eps 0.71, fps 48.63, trained time 00:06:52\n",
      "frame 30063, epsiode 34, mean reward -20.41, eps 0.70, fps 48.47, trained time 00:07:10\n",
      "frame 30885, epsiode 35, mean reward -20.43, eps 0.69, fps 45.33, trained time 00:07:28\n",
      "frame 31721, epsiode 36, mean reward -20.42, eps 0.68, fps 48.65, trained time 00:07:45\n",
      "frame 32557, epsiode 37, mean reward -20.41, eps 0.67, fps 47.60, trained time 00:08:03\n",
      "frame 33434, epsiode 38, mean reward -20.39, eps 0.67, fps 48.27, trained time 00:08:21\n",
      "frame 34196, epsiode 39, mean reward -20.41, eps 0.66, fps 48.22, trained time 00:08:37\n",
      "frame 34977, epsiode 40, mean reward -20.43, eps 0.65, fps 47.93, trained time 00:08:53\n",
      "frame 35940, epsiode 41, mean reward -20.39, eps 0.64, fps 48.12, trained time 00:09:13\n",
      "frame 36822, epsiode 42, mean reward -20.40, eps 0.63, fps 47.94, trained time 00:09:31\n",
      "frame 37603, epsiode 43, mean reward -20.42, eps 0.62, fps 48.25, trained time 00:09:48\n",
      "frame 38473, epsiode 44, mean reward -20.43, eps 0.62, fps 48.24, trained time 00:10:06\n",
      "frame 39235, epsiode 45, mean reward -20.44, eps 0.61, fps 48.15, trained time 00:10:21\n",
      "frame 40085, epsiode 46, mean reward -20.46, eps 0.60, fps 48.26, trained time 00:10:39\n",
      "frame 40985, epsiode 47, mean reward -20.45, eps 0.59, fps 48.07, trained time 00:10:58\n",
      "frame 41747, epsiode 48, mean reward -20.46, eps 0.58, fps 48.20, trained time 00:11:14\n",
      "frame 42509, epsiode 49, mean reward -20.47, eps 0.57, fps 48.04, trained time 00:11:29\n",
      "frame 43486, epsiode 50, mean reward -20.46, eps 0.57, fps 47.75, trained time 00:11:50\n",
      "frame 44338, epsiode 51, mean reward -20.47, eps 0.56, fps 48.09, trained time 00:12:08\n",
      "frame 45128, epsiode 52, mean reward -20.48, eps 0.55, fps 48.03, trained time 00:12:24\n",
      "frame 45964, epsiode 53, mean reward -20.47, eps 0.54, fps 48.12, trained time 00:12:41\n",
      "frame 46726, epsiode 54, mean reward -20.48, eps 0.53, fps 47.85, trained time 00:12:57\n",
      "frame 47733, epsiode 55, mean reward -20.49, eps 0.52, fps 47.83, trained time 00:13:18\n",
      "frame 48495, epsiode 56, mean reward -20.50, eps 0.52, fps 48.00, trained time 00:13:34\n",
      "frame 49421, epsiode 57, mean reward -20.49, eps 0.51, fps 47.97, trained time 00:13:54\n",
      "frame 50487, epsiode 58, mean reward -20.48, eps 0.50, fps 47.63, trained time 00:14:16\n",
      "frame 51526, epsiode 59, mean reward -20.47, eps 0.48, fps 48.03, trained time 00:14:38\n",
      "frame 52436, epsiode 60, mean reward -20.48, eps 0.48, fps 47.73, trained time 00:14:57\n",
      "frame 53337, epsiode 61, mean reward -20.49, eps 0.47, fps 47.48, trained time 00:15:16\n",
      "frame 54118, epsiode 62, mean reward -20.50, eps 0.46, fps 47.15, trained time 00:15:32\n",
      "frame 55063, epsiode 63, mean reward -20.49, eps 0.45, fps 47.48, trained time 00:15:52\n",
      "frame 56115, epsiode 64, mean reward -20.48, eps 0.44, fps 47.66, trained time 00:16:14\n",
      "frame 57147, epsiode 65, mean reward -20.49, eps 0.43, fps 47.40, trained time 00:16:36\n",
      "frame 57969, epsiode 66, mean reward -20.50, eps 0.42, fps 46.95, trained time 00:16:53\n",
      "frame 58869, epsiode 67, mean reward -20.49, eps 0.41, fps 47.45, trained time 00:17:12\n",
      "frame 59631, epsiode 68, mean reward -20.50, eps 0.40, fps 47.41, trained time 00:17:28\n",
      "frame 60393, epsiode 69, mean reward -20.51, eps 0.40, fps 44.84, trained time 00:17:45\n",
      "frame 61155, epsiode 70, mean reward -20.51, eps 0.39, fps 43.81, trained time 00:18:03\n",
      "frame 62084, epsiode 71, mean reward -20.52, eps 0.38, fps 47.64, trained time 00:18:22\n",
      "frame 62967, epsiode 72, mean reward -20.53, eps 0.37, fps 47.28, trained time 00:18:41\n",
      "frame 63729, epsiode 73, mean reward -20.53, eps 0.36, fps 47.71, trained time 00:18:57\n",
      "frame 64491, epsiode 74, mean reward -20.54, eps 0.36, fps 47.42, trained time 00:19:13\n",
      "frame 65373, epsiode 75, mean reward -20.55, eps 0.35, fps 47.41, trained time 00:19:32\n",
      "frame 66135, epsiode 76, mean reward -20.55, eps 0.34, fps 47.41, trained time 00:19:48\n",
      "frame 66897, epsiode 77, mean reward -20.56, eps 0.33, fps 47.62, trained time 00:20:04\n",
      "frame 67843, epsiode 78, mean reward -20.54, eps 0.32, fps 40.54, trained time 00:20:27\n",
      "frame 68665, epsiode 79, mean reward -20.54, eps 0.31, fps 44.64, trained time 00:20:46\n",
      "frame 69487, epsiode 80, mean reward -20.55, eps 0.31, fps 45.45, trained time 00:21:04\n",
      "frame 70371, epsiode 81, mean reward -20.56, eps 0.30, fps 45.34, trained time 00:21:23\n",
      "frame 71195, epsiode 82, mean reward -20.56, eps 0.29, fps 36.60, trained time 00:21:46\n",
      "frame 72077, epsiode 83, mean reward -20.57, eps 0.28, fps 39.52, trained time 00:22:08\n",
      "frame 72901, epsiode 84, mean reward -20.57, eps 0.27, fps 40.77, trained time 00:22:28\n",
      "frame 73725, epsiode 85, mean reward -20.58, eps 0.26, fps 43.91, trained time 00:22:47\n",
      "frame 74609, epsiode 86, mean reward -20.58, eps 0.25, fps 44.17, trained time 00:23:07\n",
      "frame 75433, epsiode 87, mean reward -20.59, eps 0.25, fps 42.98, trained time 00:23:26\n",
      "frame 76195, epsiode 88, mean reward -20.59, eps 0.24, fps 34.73, trained time 00:23:48\n",
      "frame 77036, epsiode 89, mean reward -20.60, eps 0.23, fps 46.92, trained time 00:24:06\n",
      "frame 77860, epsiode 90, mean reward -20.60, eps 0.22, fps 47.41, trained time 00:24:23\n",
      "frame 78696, epsiode 91, mean reward -20.59, eps 0.21, fps 46.38, trained time 00:24:41\n",
      "frame 79594, epsiode 92, mean reward -20.59, eps 0.20, fps 46.65, trained time 00:25:01\n",
      "frame 80356, epsiode 93, mean reward -20.59, eps 0.20, fps 46.44, trained time 00:25:17\n",
      "frame 81178, epsiode 94, mean reward -20.60, eps 0.19, fps 46.17, trained time 00:25:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 82002, epsiode 95, mean reward -20.60, eps 0.18, fps 46.70, trained time 00:25:52\n",
      "frame 82838, epsiode 96, mean reward -20.59, eps 0.17, fps 46.88, trained time 00:26:10\n",
      "frame 83600, epsiode 97, mean reward -20.60, eps 0.16, fps 46.55, trained time 00:26:27\n",
      "frame 84362, epsiode 98, mean reward -20.60, eps 0.16, fps 46.77, trained time 00:26:43\n",
      "frame 85186, epsiode 99, mean reward -20.61, eps 0.15, fps 46.47, trained time 00:27:01\n",
      "frame 85948, epsiode 100, mean reward -20.61, eps 0.14, fps 45.94, trained time 00:27:17\n",
      "frame 86784, epsiode 101, mean reward -20.60, eps 0.13, fps 44.95, trained time 00:27:36\n",
      "frame 87606, epsiode 102, mean reward -20.61, eps 0.12, fps 46.42, trained time 00:27:54\n",
      "frame 88368, epsiode 103, mean reward -20.62, eps 0.12, fps 46.29, trained time 00:28:10\n",
      "frame 89192, epsiode 104, mean reward -20.63, eps 0.11, fps 46.68, trained time 00:28:28\n",
      "frame 89954, epsiode 105, mean reward -20.63, eps 0.10, fps 46.82, trained time 00:28:44\n",
      "frame 90716, epsiode 106, mean reward -20.64, eps 0.09, fps 46.20, trained time 00:29:00\n",
      "frame 91478, epsiode 107, mean reward -20.65, eps 0.09, fps 45.87, trained time 00:29:17\n",
      "frame 92314, epsiode 108, mean reward -20.64, eps 0.08, fps 46.14, trained time 00:29:35\n",
      "frame 93138, epsiode 109, mean reward -20.65, eps 0.07, fps 46.19, trained time 00:29:53\n",
      "frame 93928, epsiode 110, mean reward -20.65, eps 0.06, fps 45.89, trained time 00:30:10\n",
      "frame 94770, epsiode 111, mean reward -20.65, eps 0.05, fps 46.43, trained time 00:30:28\n",
      "frame 95592, epsiode 112, mean reward -20.65, eps 0.04, fps 44.66, trained time 00:30:47\n",
      "frame 96354, epsiode 113, mean reward -20.67, eps 0.04, fps 45.94, trained time 00:31:03\n",
      "frame 97116, epsiode 114, mean reward -20.68, eps 0.03, fps 46.07, trained time 00:31:20\n",
      "frame 97878, epsiode 115, mean reward -20.70, eps 0.02, fps 45.67, trained time 00:31:37\n",
      "frame 98640, epsiode 116, mean reward -20.71, eps 0.02, fps 46.12, trained time 00:31:53\n",
      "frame 99402, epsiode 117, mean reward -20.72, eps 0.02, fps 45.98, trained time 00:32:10\n",
      "frame 100345, epsiode 118, mean reward -20.72, eps 0.02, fps 46.01, trained time 00:32:30\n",
      "frame 101167, epsiode 119, mean reward -20.72, eps 0.02, fps 45.88, trained time 00:32:48\n",
      "frame 101947, epsiode 120, mean reward -20.72, eps 0.02, fps 45.67, trained time 00:33:05\n",
      "frame 102709, epsiode 121, mean reward -20.72, eps 0.02, fps 46.24, trained time 00:33:22\n",
      "frame 103591, epsiode 122, mean reward -20.72, eps 0.02, fps 45.93, trained time 00:33:41\n",
      "frame 104353, epsiode 123, mean reward -20.73, eps 0.02, fps 46.04, trained time 00:33:57\n",
      "frame 105115, epsiode 124, mean reward -20.74, eps 0.02, fps 46.16, trained time 00:34:14\n",
      "frame 105997, epsiode 125, mean reward -20.75, eps 0.02, fps 45.94, trained time 00:34:33\n",
      "frame 106759, epsiode 126, mean reward -20.75, eps 0.02, fps 46.24, trained time 00:34:50\n",
      "frame 107521, epsiode 127, mean reward -20.75, eps 0.02, fps 46.23, trained time 00:35:06\n",
      "frame 108343, epsiode 128, mean reward -20.75, eps 0.02, fps 46.26, trained time 00:35:24\n",
      "frame 109105, epsiode 129, mean reward -20.75, eps 0.02, fps 46.32, trained time 00:35:40\n",
      "frame 109867, epsiode 130, mean reward -20.76, eps 0.02, fps 45.95, trained time 00:35:57\n",
      "frame 110689, epsiode 131, mean reward -20.78, eps 0.02, fps 46.33, trained time 00:36:15\n",
      "frame 111451, epsiode 132, mean reward -20.78, eps 0.02, fps 46.46, trained time 00:36:31\n",
      "frame 112213, epsiode 133, mean reward -20.78, eps 0.02, fps 46.36, trained time 00:36:47\n",
      "frame 113035, epsiode 134, mean reward -20.79, eps 0.02, fps 46.29, trained time 00:37:05\n",
      "frame 113797, epsiode 135, mean reward -20.79, eps 0.02, fps 46.42, trained time 00:37:22\n",
      "frame 114559, epsiode 136, mean reward -20.80, eps 0.02, fps 46.81, trained time 00:37:38\n",
      "frame 115321, epsiode 137, mean reward -20.81, eps 0.02, fps 44.36, trained time 00:37:55\n",
      "frame 116083, epsiode 138, mean reward -20.82, eps 0.02, fps 43.39, trained time 00:38:13\n",
      "frame 116933, epsiode 139, mean reward -20.82, eps 0.02, fps 43.87, trained time 00:38:32\n",
      "frame 117757, epsiode 140, mean reward -20.82, eps 0.02, fps 45.80, trained time 00:38:50\n",
      "frame 118579, epsiode 141, mean reward -20.84, eps 0.02, fps 44.58, trained time 00:39:08\n",
      "frame 119641, epsiode 142, mean reward -20.84, eps 0.02, fps 43.14, trained time 00:39:33\n",
      "frame 120463, epsiode 143, mean reward -20.84, eps 0.02, fps 45.06, trained time 00:39:51\n",
      "frame 121225, epsiode 144, mean reward -20.84, eps 0.02, fps 44.27, trained time 00:40:09\n",
      "frame 121987, epsiode 145, mean reward -20.84, eps 0.02, fps 41.80, trained time 00:40:27\n",
      "frame 122869, epsiode 146, mean reward -20.84, eps 0.02, fps 43.40, trained time 00:40:47\n",
      "frame 123631, epsiode 147, mean reward -20.85, eps 0.02, fps 44.12, trained time 00:41:04\n",
      "frame 124393, epsiode 148, mean reward -20.85, eps 0.02, fps 46.30, trained time 00:41:21\n",
      "frame 125155, epsiode 149, mean reward -20.85, eps 0.02, fps 46.89, trained time 00:41:37\n",
      "frame 125917, epsiode 150, mean reward -20.86, eps 0.02, fps 43.91, trained time 00:41:54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\563920051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m19.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\1048435760.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env_name, agent, writer, reward_bound, replay_start_size, sync_frames)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mloss_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\1704343668.py\u001b[0m in \u001b[0;36mcal_loss\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# double DQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_actions_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# choose next action from the net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             next_q_action_values_v = self.target_net(next_states_v).gather( # but use the value from the target net\n\u001b[0m\u001b[0;32m     83\u001b[0m                 1, next_actions_v.unsqueeze(-1)).squeeze(-1)\n\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# basic DQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15104\\1665755084.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;34m\"\"\"output is the Q value of each action for this observation obs\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mconv_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mfc_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# flatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdueling\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(env_name, agent, writer, reward_bound=19.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda265f",
   "metadata": {},
   "source": [
    "### step5. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1786997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env_name, policy_func=None, step_limit=10000, render_mode=\"human\"):\n",
    "    env = make_wrapped_env(env_name, render_mode)\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for step in range(step_limit):\n",
    "        if policy_func:\n",
    "            action = policy_func(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            print(f\"Game over in {step+1} steps and the episode reward is {total_reward}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Steps reached limit as {step_limit} steps, where the total reward is {total_reward}\")\n",
    "        \n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43d945",
   "metadata": {},
   "source": [
    "#### 5.1 test the basic DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eda85224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNAgent with basic DQN\n"
     ]
    }
   ],
   "source": [
    "test_agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][0]\n",
    "print(test_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20a2d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over in 1742 steps and the episode reward is 20.0\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"./ckpt/bDQN-16.pth\"\n",
    "test_agent.load_ckpt(ckpt_path)\n",
    "test_agent.eps = 0.0 # do not use epsilon greedy search\n",
    "test(env_name, test_agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b12646",
   "metadata": {},
   "source": [
    "#### 5.2 test the Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9121cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNAgent with Double DQN\n"
     ]
    }
   ],
   "source": [
    "test_agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][1]\n",
    "print(test_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91709aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over in 1635 steps and the episode reward is 21.0\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"./ckpt/dubDQN-19.pth\"\n",
    "test_agent.load_ckpt(ckpt_path)\n",
    "test_agent.eps = 0.0 # do not use epsilon greedy search\n",
    "test(env_name, test_agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d806570",
   "metadata": {},
   "source": [
    "#### 5.3 test the Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edb4a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNAgent with Dueling DQN\n"
     ]
    }
   ],
   "source": [
    "test_agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][2]\n",
    "print(test_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "037a03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over in 1636 steps and the episode reward is 21.0\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"./ckpt/dueDQN-18.5.pth\"\n",
    "test_agent.load_ckpt(ckpt_path)\n",
    "test_agent.eps = 0.0 # do not use epsilon greedy search\n",
    "test(env_name, test_agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805b337",
   "metadata": {},
   "source": [
    "#### 5.4 test the D3QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent = [\n",
    "    DQNAgent(env_name, double=False, dueling=False), # Basic DQN\n",
    "    DQNAgent(env_name, double=True, dueling=False), # Double DQN\n",
    "    DQNAgent(env_name, double=False, dueling=True), # Dueling DQN\n",
    "    DQNAgent(env_name, double=True, dueling=True) # Dueling Double DQN\n",
    "][3]\n",
    "print(test_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb164d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"\"\n",
    "test_agent.load_ckpt(ckpt_path)\n",
    "test_agent.eps = 0.0 # do not use epsilon greedy search\n",
    "test(env_name, test_agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841e190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
