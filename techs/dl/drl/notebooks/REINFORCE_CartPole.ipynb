{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94fb440",
   "metadata": {},
   "source": [
    "## PG-based REINFORCE method to solve CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1f766",
   "metadata": {},
   "source": [
    "$\\text{Policy Gradient}: \\min\\limits_{\\pi} \\mathcal{L} := -\\mathbb{E}_{\\tau}[Q(s,a)\\log \\pi(a|s)], \\tau=\\{(s_i,a_i,r_i)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477ccc2",
   "metadata": {},
   "source": [
    "$\\text{REINFORCE: the vanilla kind of PG methods when}\\;\\; Q(s_t,a_t) = \\sum\\limits_{i=t}^T \\gamma^{i} r_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "447e9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0647361",
   "metadata": {},
   "source": [
    "### step0. load the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7484fa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-4.3404794e+00,  2.0558559e+38, -2.7077520e-01,  1.4372895e+38],\n",
       "       dtype=float32),\n",
       " (4,),\n",
       " 0,\n",
       " 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.observation_space.sample(), env.observation_space.shape, \\\n",
    "env.action_space.sample(), env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db509795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env_name, policy_func=None, max_steps=1000):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "    obs, _ = env.reset()\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if policy_func:\n",
    "            action = policy_func(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "        if terminated or truncated:\n",
    "            print(f\"The game is over with {step+1} steps and {total_rewards} epsiode reward\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"The step limit has been reached at {max_steps} steps\")\n",
    "    \n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8059884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is over with 16 steps and 16.0 epsiode reward\n"
     ]
    }
   ],
   "source": [
    "play(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d02f0b",
   "metadata": {},
   "source": [
    "### step1. build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b29b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy network: observation vector => action logits\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits # no softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff846a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE way to calculate Q values for each step in a epsiode\n",
    "def cal_qvals(rewards, gamma=1.0):\n",
    "    \"\"\"\n",
    "    rewards: [ri]\n",
    "    res: [r_1, r_1+gamma*r_2, r_1+gamma*r_2+gamma^2*r_3,...]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= gamma\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "532ee0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"Agent class using REINFORCE algorithm\"\"\"\n",
    "    def __init__(self, policy_net, device=\"cuda:0\"):\n",
    "        self.policy_net = policy_net.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def policy_func(self, obs):\n",
    "        batch_mode = len(obs.shape) > 1 # there's a batch dim\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get tensorized observation\n",
    "            if batch_mode:\n",
    "                obs_t = torch.tensor(obs).to(self.device) # shape=(batch_size, obs_dim)\n",
    "            else: # single observation\n",
    "                obs_t = torch.tensor([obs]).to(self.device)\n",
    "            # get logits\n",
    "            action_logits_t = self.policy_net(obs_t) \n",
    "            # get probs\n",
    "            action_probs_t = F.softmax(action_logits_t, dim=1) # shape=(batch_size, n_actions)\n",
    "            # sample actions\n",
    "            actions_t = Categorical(action_probs_t).sample() # shape=(batch_size,)\n",
    "            if batch_mode:\n",
    "                actions = actions_t.long().cpu().numpy()\n",
    "            else:\n",
    "                actions = actions_t.long().cpu().item()\n",
    "            \n",
    "            return actions\n",
    "        \n",
    "    def loss_func(self, batch_obs, batch_acts, batch_qvals):\n",
    "        \"\"\"loss = - Q(s,a) * logπ(a | s) = -∑γ^iri * logπ(a | s)\"\"\"\n",
    "        # tensorize\n",
    "        batch_obs_t = torch.tensor(batch_obs).to(self.device) # shape=(batch_size, obs_dim)\n",
    "        batch_acts_t = torch.tensor(batch_acts, dtype=torch.int64).to(self.device).unsqueeze(1) # shape=(batch_size, 1)\n",
    "        batch_qvals_t = torch.tensor(batch_qvals).to(self.device) # shape=(batch, )\n",
    "        # forward to get action logits\n",
    "        action_logits_t = self.policy_net(batch_obs_t)\n",
    "        # calculate log pdf for all actions\n",
    "        log_pdf_t = F.log_softmax(action_logits_t, dim=1) # shape = (batch_size, n_actions)\n",
    "        #  gather the log probs for the executed actions\n",
    "        log_probs_t = log_pdf_t.gather(1, batch_acts_t).squeeze(1) # shape = (batch_size, )\n",
    "        # calculate the loss\n",
    "        loss_t = -batch_qvals_t * log_probs_t # shape=(batch_size,)\n",
    "        \n",
    "        return loss_t.mean()\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        torch.save(self.policy_net.state_dict(), save_path)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72729faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNet(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = PolicyNet(\n",
    "    input_size=env.observation_space.shape[0],\n",
    "    n_actions=env.action_space.n\n",
    ")\n",
    "agent = REINFORCEAgent(net)\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720c8b6",
   "metadata": {},
   "source": [
    "### step2. build the dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b12ee012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_exp(env, policy_func=None):\n",
    "    \"\"\"return a 4-elem tuple (obs, action, reward, next_obs) as an experience for each step in one episode\"\"\"\n",
    "    done = True\n",
    "    \n",
    "    while True:\n",
    "        # initial a new episode\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "        # get the action based on current observation\n",
    "        if policy_func:\n",
    "            action = policy_func(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        # step the env to get the next observation, reward, and game-over flag\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        yield obs, action, reward, next_obs, terminated\n",
    "        \n",
    "        if terminated:\n",
    "            done = True\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6707b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_exp_batch(env, policy_func=None, batch_episodes=16, gamma=1.0):\n",
    "    \"\"\"collect an experience batch of episodes consisting of obs, acts, qvals and episode rewards\"\"\"\n",
    "    batch_obs, batch_acts, batch_qvals, batch_rewards = [], [], [], []\n",
    "    episode_rewards = []\n",
    "    episode = 0\n",
    "    \n",
    "    while episode < batch_episodes:\n",
    "        for exp in iter_exp(env, policy_func):\n",
    "            obs, act, reward, next_obs, done = exp\n",
    "            batch_obs.append(obs)\n",
    "            batch_acts.append(act)\n",
    "            batch_rewards.append(reward)\n",
    "            if done:\n",
    "                # calculate qvals for this episode\n",
    "                batch_qvals.extend(cal_qvals(batch_rewards, gamma=gamma))\n",
    "                # calculate episode reward\n",
    "                episode_rewards.append(float(np.sum(batch_rewards)))\n",
    "                batch_rewards.clear()\n",
    "                # get to next episode if it exists\n",
    "                episode += 1\n",
    "                break\n",
    "    return batch_obs, batch_acts, batch_qvals, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541c4db",
   "metadata": {},
   "source": [
    "### step3. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6afd3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, agent, writer, \n",
    "          max_epochs=500, batch_episodes=4, lr=0.01, gamma=0.99, reward_bound=195, recent=50):\n",
    "    # init\n",
    "    env = gym.make(env_name)\n",
    "    optimizer = torch.optim.Adam(agent.policy_net.parameters(), lr=lr)\n",
    "    episode_rewards, mean_reward = [], 0.0\n",
    "    # loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # get the batch data (obs, act, qvals), with epi_rewards for logging\n",
    "        batch_data = collect_exp_batch(env, agent.policy_func, batch_episodes, gamma)\n",
    "        ers = batch_data[-1]\n",
    "        # optimize a step with this batch data\n",
    "        optimizer.zero_grad()\n",
    "        loss = agent.loss_func(*batch_data[:-1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # log for each episode\n",
    "        for er in ers:\n",
    "            episode_rewards.append(er)\n",
    "            episode_idx = len(episode_rewards)\n",
    "            mean_reward = float(np.mean(episode_rewards[-recent:]))\n",
    "            # log each episode reward\n",
    "            writer.add_scalar(\"episode reward\", er, episode_idx)\n",
    "            # log the mean reward of last recent episodes\n",
    "            writer.add_scalar(\"mean reward\", mean_reward, episode_idx)\n",
    "        # print for each epoch\n",
    "        print(f\"epoch {epoch} => loss: {loss.item()} | mean reward: {mean_reward}\")\n",
    "        if mean_reward > reward_bound: # good enough\n",
    "            break\n",
    "    # close\n",
    "    env.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e08fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"-PG_REINFORCE_CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cbf8eba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 => loss: 8.235739707946777 | mean reward: 41.0\n",
      "epoch 1 => loss: 7.017628192901611 | mean reward: 36.625\n",
      "epoch 2 => loss: 6.653279781341553 | mean reward: 34.666666666666664\n",
      "epoch 3 => loss: 5.73574161529541 | mean reward: 32.75\n",
      "epoch 4 => loss: 5.891140937805176 | mean reward: 31.75\n",
      "epoch 5 => loss: 6.923009872436523 | mean reward: 31.208333333333332\n",
      "epoch 6 => loss: 6.854440212249756 | mean reward: 30.607142857142858\n",
      "epoch 7 => loss: 5.16597843170166 | mean reward: 29.90625\n",
      "epoch 8 => loss: 4.680619716644287 | mean reward: 29.083333333333332\n",
      "epoch 9 => loss: 5.615002632141113 | mean reward: 28.925\n",
      "epoch 10 => loss: 5.906534671783447 | mean reward: 28.886363636363637\n",
      "epoch 11 => loss: 5.32065486907959 | mean reward: 28.5625\n",
      "epoch 12 => loss: 5.779853820800781 | mean reward: 28.24\n",
      "epoch 13 => loss: 6.911384105682373 | mean reward: 27.94\n",
      "epoch 14 => loss: 5.245478630065918 | mean reward: 27.32\n",
      "epoch 15 => loss: 8.976016998291016 | mean reward: 28.24\n",
      "epoch 16 => loss: 8.338434219360352 | mean reward: 28.86\n",
      "epoch 17 => loss: 7.93828010559082 | mean reward: 29.86\n",
      "epoch 18 => loss: 8.117749214172363 | mean reward: 31.36\n",
      "epoch 19 => loss: 10.389372825622559 | mean reward: 33.12\n",
      "epoch 20 => loss: 9.132607460021973 | mean reward: 35.48\n",
      "epoch 21 => loss: 7.677192687988281 | mean reward: 37.32\n",
      "epoch 22 => loss: 12.43699836730957 | mean reward: 40.08\n",
      "epoch 23 => loss: 10.744694709777832 | mean reward: 43.04\n",
      "epoch 24 => loss: 11.760687828063965 | mean reward: 46.8\n",
      "epoch 25 => loss: 9.19941234588623 | mean reward: 49.04\n",
      "epoch 26 => loss: 13.682491302490234 | mean reward: 54.1\n",
      "epoch 27 => loss: 10.85717487335205 | mean reward: 57.24\n",
      "epoch 28 => loss: 12.26951789855957 | mean reward: 60.66\n",
      "epoch 29 => loss: 14.468647003173828 | mean reward: 65.84\n",
      "epoch 30 => loss: 11.797638893127441 | mean reward: 69.36\n",
      "epoch 31 => loss: 15.385518074035645 | mean reward: 76.4\n",
      "epoch 32 => loss: 14.357579231262207 | mean reward: 81.06\n",
      "epoch 33 => loss: 17.538225173950195 | mean reward: 87.66\n",
      "epoch 34 => loss: 13.089897155761719 | mean reward: 92.18\n",
      "epoch 35 => loss: 16.113252639770508 | mean reward: 97.18\n",
      "epoch 36 => loss: 14.338674545288086 | mean reward: 100.96\n",
      "epoch 37 => loss: 16.466081619262695 | mean reward: 107.42\n",
      "epoch 38 => loss: 13.653297424316406 | mean reward: 109.54\n",
      "epoch 39 => loss: 12.319244384765625 | mean reward: 110.44\n",
      "epoch 40 => loss: 12.277260780334473 | mean reward: 111.96\n",
      "epoch 41 => loss: 11.801053047180176 | mean reward: 110.7\n",
      "epoch 42 => loss: 10.26479721069336 | mean reward: 110.08\n",
      "epoch 43 => loss: 11.222311019897461 | mean reward: 107.98\n",
      "epoch 44 => loss: 12.104466438293457 | mean reward: 105.9\n",
      "epoch 45 => loss: 15.451777458190918 | mean reward: 107.5\n",
      "epoch 46 => loss: 13.333112716674805 | mean reward: 104.82\n",
      "epoch 47 => loss: 16.68028450012207 | mean reward: 104.74\n",
      "epoch 48 => loss: 19.489274978637695 | mean reward: 112.26\n",
      "epoch 49 => loss: 17.810787200927734 | mean reward: 115.9\n",
      "epoch 50 => loss: 25.038047790527344 | mean reward: 134.06\n",
      "epoch 51 => loss: 25.58514404296875 | mean reward: 154.2\n",
      "epoch 52 => loss: 27.87924575805664 | mean reward: 180.96\n",
      "epoch 53 => loss: 28.402143478393555 | mean reward: 213.36\n"
     ]
    }
   ],
   "source": [
    "train(env_name, agent, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a370cf0",
   "metadata": {},
   "source": [
    "### step4. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0e247ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is over with 454 steps and 454.0 epsiode reward\n"
     ]
    }
   ],
   "source": [
    "play(env_name, agent.policy_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4df52401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the net\n",
    "save_path =  \"./ckpt/REINFORCE-CartPolev1-r213.pth\"\n",
    "agent.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d6477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
