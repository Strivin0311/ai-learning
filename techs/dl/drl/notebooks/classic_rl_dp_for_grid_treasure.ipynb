{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sunrise-ultimate",
   "metadata": {},
   "source": [
    "## Step1: Test gym library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlling-arrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/strivin/gym/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/strivin/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42, return_info=True)\n",
    "\n",
    "for _ in range(1000):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if done:\n",
    "        observation, info = env.reset(return_info=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-freedom",
   "metadata": {},
   "source": [
    "## Step2: Construct GridTreasure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "painful-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from gym.utils.renderer import Renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-wedding",
   "metadata": {},
   "source": [
    "$\\Rightarrow 游戏描述:$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-disability",
   "metadata": {},
   "source": [
    "$在一个size\\times size的网格世界中，有一个蓝色的智能小球，它能在网格世界中上下左右移动，同时也有一个红色小方格，上面放着宝藏$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-essay",
   "metadata": {},
   "source": [
    "$蓝色小球和红色方格的初始位置都是在游戏开始前随机确定的，求解蓝色小球寻找宝藏的最优策略$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "light-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTreasureEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"single_rgb_array\",\"hide\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, size: int = 5):\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode  # Define the attribute render_mode in your environment\n",
    "\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        self.numS = size * size\n",
    "        self.numA = 4\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(self.numA)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]), # right\n",
    "            1: np.array([0, 1]), # up\n",
    "            2: np.array([-1, 0]),# left\n",
    "            3: np.array([0, -1]),# down\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode.\n",
    "        \"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            import pygame  # import here to avoid pygame dependency with no render\n",
    "\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "                \n",
    "        # The following line uses the util class Renderer to gather a collection of frames \n",
    "        # using a method that computes a single frame. We will define _render_frame below.\n",
    "        self.renderer = Renderer(self.render_mode, self._render_frame)\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "    \n",
    "    def loc_to_state(self,loc): # mapping location to state\n",
    "        return loc[0] + loc[1]*self.size\n",
    "    \n",
    "    def state_to_loc(self,state): # mapping state to location\n",
    "        return np.array(\n",
    "            [state%self.size,state//self.size],dtype=np.int\n",
    "        )\n",
    "    \n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(0, self.size, size=2)\n",
    "            \n",
    "        # for testing\n",
    "        # self._target_location = np.array([3,1])\n",
    "\n",
    "        # clean the render collection and add the initial frame\n",
    "        if self.render_mode != \"hide\":\n",
    "            self.renderer.reset()\n",
    "            self.renderer.render_step()\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return (observation, info) if return_info else observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done if the agent has reached the target\n",
    "        done = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if done else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # add a frame to the render collection\n",
    "        if self.render_mode != \"hide\":\n",
    "            self.renderer.render_step()\n",
    "\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        # Just return the list of render frames collected by the Renderer.\n",
    "        return self.renderer.get_renders()\n",
    "\n",
    "    def _render_frame(self, mode: str):\n",
    "        # This will be the function called by the Renderer to collect a single frame.\n",
    "        assert mode is not None  # The renderer will not call this function with no-rendering.\n",
    "    \n",
    "        import pygame # avoid global pygame dependency. This method is not called with no-render.\n",
    "    \n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target => a red rect\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent => a blue circle\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if mode == \"human\":\n",
    "            assert self.window is not None\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array or single_rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            import pygame \n",
    "            \n",
    "            pygame.display.quit()\n",
    "            pygame.quit()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patent-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== \t timestep is 0\n",
      "Observation: {'agent': array([1, 3]), 'target': array([3, 2])}\n",
      "Info: {'distance': 3.0}\n",
      "========== \t timestep is 100\n",
      "Observation: {'agent': array([1, 1]), 'target': array([3, 2])}\n",
      "Info: {'distance': 3.0}\n",
      "========== \t timestep is 118\n",
      "Observation: {'agent': array([2, 4]), 'target': array([0, 3])}\n",
      "Info: {'distance': 3.0}\n",
      "~~~~done!~~~~\n"
     ]
    }
   ],
   "source": [
    "## test custom enviroment using random policy\n",
    "\n",
    "# init env to test\n",
    "env = GridTreasureEnv(render_mode=\"human\",size=5)\n",
    "# init action\n",
    "env.action_space.seed(42)\n",
    "# init reset\n",
    "observation, info = env.reset(seed=42, return_info=True)\n",
    "# random walk\n",
    "for t in range(1000):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    \n",
    "    if t%100 == 0:\n",
    "        print(\"=\"*10,\"\\t timestep is {}\".format(t))\n",
    "        print(\"Observation: {}\".format(observation))\n",
    "        print(\"Info: {}\".format(info))\n",
    "\n",
    "    if done:\n",
    "        observation, info = env.reset(return_info=True)\n",
    "        if t%100 != 0:\n",
    "            print(\"=\"*10,\"\\t timestep is {}\".format(t))\n",
    "            print(\"Observation: {}\".format(observation))\n",
    "            print(\"Info: {}\".format(info))\n",
    "        print(\"~~~~done!~~~~\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-brass",
   "metadata": {},
   "source": [
    "## Step3: Define Data Structure for Markov Decision Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-baltimore",
   "metadata": {},
   "source": [
    "$一个\\mathrm{MDP}由五个部分组成：<S,A,P,R,\\gamma>,外加智能策略\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-hudson",
   "metadata": {},
   "source": [
    "$其中：S = \\{s_1,s_2,..,s_n\\}为状态集合，A = \\{a_1,a_2,..,a_m\\}为动作集合，\\pi是S,A上的条件分布：\\pi(a_k|s_i) = P(A_{t}=a_k|S_t=a_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-green",
   "metadata": {},
   "source": [
    "$\\Rightarrow 对于确定性的最佳策略\\pi^*, \\pi^*(s_i) = \\mathrm{arg}\\max\\limits_{a_k\\in A} \\pi(a_k|s_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-signature",
   "metadata": {},
   "source": [
    "$P为状态转移概率，具体定义有两种：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-baseline",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "&P_{s_i,s_j}^{a_k} = P(S_{t+1}=s_j | S_t=s_i,A_t=a_k) \\Rightarrow 状态s_i下执行动作a_k后，转移到状态s_j的概率\\\\\n",
    "&P_{s_i,s_j}^{\\pi} = \\sum\\limits_{a_k\\in A} \\pi(a_k|s_i) \\cdot P_{s_i,s_j}^{a_k} \\Rightarrow 状态s_i下执行策略\\pi后，转移到状态s_j的概率\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-basics",
   "metadata": {},
   "source": [
    "$R为立即回报，具体定义有两种：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-prayer",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "&R_{s_i}^{a_k} = E[R_{t+1}|S_t=s_i,A_t=a_k] \\Rightarrow 状态s_i下执行动作a_k后，得到的立即回报\\\\\n",
    "&R_{s_i}^{\\pi} = \\sum\\limits_{a_k\\in A} \\pi(a_k|s_i)  \\cdot R_{s_i}^{a_k} \\Rightarrow 状态s_i下执行策略\\pi后，得到的立即回报\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-crack",
   "metadata": {},
   "source": [
    "$\\gamma为衰减因子(\\mathrm{Discount}\\; \\mathrm{Factor})\\in [0,1]，在计算未来累计回报G时，用来表示未来时刻的立即回报的权重衰减率$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-river",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "& G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...  = \\sum\\limits_{k=0}^{\\infty} \\gamma^kR_{t+k+1} \\Rightarrow 描述t时刻之后的未来累计的\\gamma指数衰减加权回报\\\\\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-service",
   "metadata": {},
   "source": [
    "$\\Rightarrow 当\\gamma = 0时，表示无需考虑未来的回报，只需考虑当下的立即回报；当\\gamma = 1时，表示任意遥远的未来回报同当下的立即汇报同等重要$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "widespread-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class Policy():\n",
    "    def __init__(self,numS,numA,uniform=False):\n",
    "        self.numS = numS\n",
    "        self.numA = numA\n",
    "        self.pi = np.zeros(shape=(numS,numA),dtype=np.float32)\n",
    "        if uniform:\n",
    "            self.to_uniform()\n",
    "    def update(self,si,ak,val):\n",
    "        self.pi[si][ak] = val\n",
    "    def get(self,si,ak):\n",
    "        return self.pi[si][ak]\n",
    "    def getBestAction(self,si,all_actions=False):\n",
    "        if all_actions:\n",
    "            max_val = np.max(self.pi[si,:])\n",
    "            best_action_list = []\n",
    "            for a,v in enumerate(self.pi[si,:]):\n",
    "                if v == max_val:\n",
    "                    best_action_list.append(a)\n",
    "            return np.array(best_action_list)\n",
    "        return np.argmax(self.pi[si,:])\n",
    "    def to_uniform(self):\n",
    "        self.pi = np.ones(shape=(self.numS,self.numA),dtype=np.float32) / self.numA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "million-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbTransformer():\n",
    "    def __init__(self,numS,numA):\n",
    "        self.numS = numS\n",
    "        self.numA = numA\n",
    "        self.p = np.zeros(shape=(numS,numS,numA),dtype=np.float32)\n",
    "    def update(self,si,sj,ak,val):\n",
    "        self.p[si][sj][ak] = val\n",
    "    def getByAction(self,si,sj,ak):\n",
    "        return self.p[si][sj][ak]\n",
    "    def getByPolicy(self,si,sj,pi:Policy):\n",
    "        return np.sum(\n",
    "            [ pi.get(si,ak) * self.p[si][sj][ak] for ak in range(self.numA) ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "engaging-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward():\n",
    "    def __init__(self,numS,numA):\n",
    "        self.numS = numS\n",
    "        self.numA = numA\n",
    "        self.r = np.zeros(shape=(numS,numA),dtype=np.float32)\n",
    "    def update(self,si,ak,val):\n",
    "        self.r[si][ak] = val\n",
    "    def getByAction(self,si,ak):\n",
    "        return self.r[si][ak]\n",
    "    def getByPolicy(self,si,pi:Policy):\n",
    "        return np.sum(\n",
    "            [ pi.get(si,ak) * self.r[si][ak] for ak in range(self.numA) ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accomplished-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP: # <S,A,P,R,γ> + π\n",
    "    def __init__(self,numS,numA,pi:Policy,gamma=1):\n",
    "        self.numS = numS\n",
    "        self.numA = numA\n",
    "        self.P = ProbTransformer(numS,numA)\n",
    "        self.R = Reward(numS,numA)\n",
    "        self.gamma = gamma\n",
    "        self.pi = pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-conditions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "based-celebrity",
   "metadata": {},
   "source": [
    "## Step4: Define Data Structure for Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-garlic",
   "metadata": {},
   "source": [
    "$贝尔曼方程是求解\\mathrm{MDP}最优策略的基本理论，因此也是强化学习的理论基石$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-science",
   "metadata": {},
   "source": [
    "$贝尔曼方程包含期望方程和最优方程两种形式：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-exhibition",
   "metadata": {},
   "source": [
    "$\\Rightarrow 1. 贝尔曼期望方程：描述了当前值函数（包括状态值函数V_{\\pi}(s_i)和行为值函数Q_{\\pi}(s_i,a_k)）的递推关系，具体定义如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-armor",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "&Q_{\\pi}(s_i,a_k) = E_{\\pi}[G_t|S_t = s_i,A_t=a_k]\\Rightarrow 状态s_i下执行动作a_k后，能得到的期望未来累计回报\\\\\n",
    "&V_{\\pi}(s_i) = E_{\\pi}[G_t|S_t = s_i] = \\sum\\limits_{a_k\\in A} \\pi(a_k|s_i)\\cdot Q_{\\pi}(s_i,a_k)\\Rightarrow 状态s_i下执行策略\\pi后，能得到的期望未来累计回报\\\\\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-sister",
   "metadata": {},
   "source": [
    "$\\quad\\quad *\\; 行为值函数Q_{\\pi}(s_i,a_k)的递推方程推导如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-fleet",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "Q_{\\pi}(s_i,a_k) \n",
    "&= E_{\\pi}[G_t|S_t = s_i,A_t=a_k]\\\\\n",
    "&= E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t = s_i,A_t=a_k]\\\\\n",
    "&= E_{\\pi}[R_{t+1}|S_t = s_i,A_t=a_k] + \\gamma E_{\\pi}[ G_{t+1}|S_t = s_i,A_t=a_k]\\\\\n",
    "&= R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P(S_{t+1}=s_j | S_t=s_i,A_t=a_k) \\cdot E_{\\pi}[ G_{t+1}|S_{t+1} = s_j;S_t = s_i,A_t=a_k]\\\\\n",
    "&= R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P_{s_i,s_j}^{a_k} \\cdot V_{\\pi}(s_j)\\\\\n",
    "&= R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P_{s_i,s_j}^{a_k} \\cdot [\\sum\\limits_{a_l\\in A} \\pi(a_l|s_j)\\cdot Q_{\\pi}(s_j,a_l)]\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-start",
   "metadata": {},
   "source": [
    "$\\quad\\quad *\\; 状态值函数V_{\\pi}(s_i)的递推方程推导如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-decimal",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "V_{\\pi}(s_i) \n",
    "&= \\sum\\limits_{a_k\\in A} \\pi(a_k|s_i)\\cdot Q_{\\pi}(s_i,a_k)\\\\\n",
    "&= \\sum\\limits_{a_k\\in A} \\pi(a_k|s_i)\\cdot [R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P_{s_i,s_j}^{a_k} \\cdot V_{\\pi}(s_j)]\\\\\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-geometry",
   "metadata": {},
   "source": [
    "$\\Rightarrow 2. 贝尔曼最优方程：描述了当前最优值函数（包括状态最优值函数V^*(s_i)和行为最优值函数Q^*(s_i,a_k)）的递推关系，具体定义如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-organic",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "&Q^*(s_i,a_k) = \\max\\limits_{\\pi} Q_{\\pi}(s_i,a_k) = Q_{\\pi^*}(s_i,a_k)  \\Rightarrow 所有策略下值最优的行为值函数，亦对应最优策略\\\\\n",
    "&V^*(s_i) = \\max\\limits_{\\pi} V_{\\pi}(s_i) = \\max\\limits_{a_k\\in A} Q^*(s_i,a_k) = V_{\\pi^*}(s_i) \\Rightarrow 所有策略下值最优的状态值函数，亦对应最优策略和行为最优值函数\\\\\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-construction",
   "metadata": {},
   "source": [
    "$\\quad\\quad *\\; 行为最优值函数Q^*(s_i,a_k)的递推方程推导如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-lithuania",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "Q^*(s_i,a_k) \n",
    "&= R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P_{s_i,s_j}^{a_k} \\cdot V^*(s_j)\\\\\n",
    "&= R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P_{s_i,s_j}^{a_k} \\cdot \\max\\limits_{a_l\\in A} Q^*(s_j,a_l)\\\\\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-notification",
   "metadata": {},
   "source": [
    "$\\quad\\quad *\\; 状态最优值函数V^*(s_i)的递推方程推导如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-margin",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\begin{aligned}\n",
    "V^*(s_i)\n",
    "&= \\max\\limits_{a_k\\in A} Q^*(s_i,a_k) \\\\\n",
    "&= \\max\\limits_{a_k\\in A} [R_{s_i}^{a_k} + \\gamma \\sum\\limits_{s_j\\in S} P_{s_i,s_j}^{a_k} \\cdot V^*(s_j)]\\\\\n",
    "\\end{aligned}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-cartoon",
   "metadata": {},
   "source": [
    "$\\quad\\quad *\\; 最优策略\\pi^*的求解方程如下：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-exhibition",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\pi^*(a_k|s_i) = \n",
    "\\begin{cases}\n",
    "1, & a_k= \\mathrm{arg}\\max\\limits_{a_l\\in A} Q^*(s_i,a_l) \\\\\n",
    "0, & otherwise\n",
    "\\end{cases}\\\\\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-topic",
   "metadata": {},
   "source": [
    "$\\quad\\quad \\Rightarrow 若状态s_i下存在多个使得行为值函数最优的动作集合B = \\{a_{k_1},a_{k_2},..,a_{k_{\\beta}}\\}, 则：\\pi^*(a_{k}|s_i) =  \\begin{cases}\n",
    "\\cfrac{1}{\\beta}, & a_{k} \\in B \\\\\n",
    "0, & otherwise\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-assets",
   "metadata": {},
   "source": [
    "$\\quad\\quad \\Rightarrow 对于任何\\mathrm{MDP}问题，总存在一个\\textbf{确定性}的最优策略\\pi^*，其亦对应状态最优值函数V^*(s_i)和行为最优值函数Q^*(s_i,a_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unsigned-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BE: # Bellman Equation for MDP model\n",
    "    def __init__(self,mdp:MDP):\n",
    "        self.mdp = mdp\n",
    "        self.Q = np.zeros(shape=(mdp.numS,mdp.numA),dtype=np.float32)\n",
    "        self.V = np.zeros(shape=(mdp.numS,),dtype=np.float32)\n",
    "        self.bQ = np.zeros(shape=(mdp.numS,mdp.numA),dtype=np.float32)\n",
    "        self.bV = np.zeros(shape=(mdp.numS,),dtype=np.float32)\n",
    "    def updateQ(self,si,ak):\n",
    "        old_val = self.Q[si][ak]\n",
    "        new_val = self.mdp.R.getByAction(si,ak) + self.mdp.gamma * np.sum([\n",
    "            self.mdp.P.getByAction(si,sj,ak) * np.sum([\n",
    "                self.mdp.pi.get(sj,al) * self.Q[sj][al] for al in range(self.mdp.numA)\n",
    "            ]) for sj in range(self.mdp.numS)\n",
    "        ])\n",
    "        \n",
    "        self.Q[si][ak] = new_val\n",
    "        return old_val\n",
    "    def updateV(self,si):\n",
    "        old_val = self.V[si]\n",
    "        new_val = np.sum([\n",
    "            self.mdp.pi.get(si,ak) * (\n",
    "                self.mdp.R.getByAction(si,ak) + self.mdp.gamma * np.sum([\n",
    "                    self.mdp.P.getByAction(si,sj,ak) * self.V[sj] for sj in range(self.mdp.numS)\n",
    "                ])\n",
    "            ) for ak in range(self.mdp.numA)\n",
    "        ])\n",
    "        \n",
    "        self.V[si] = new_val\n",
    "        return old_val\n",
    "    def updateBestQ(self,si,ak):\n",
    "        old_val = self.bQ[si][ak]\n",
    "        new_val = self.mdp.R.getByAction(si,ak) + self.mdp.gamma * np.sum([\n",
    "            self.mdp.P.getByAction(si,sj,ak) * np.max(self.bQ[sj,:]) for sj in range(self.mdp.numS)\n",
    "        ])\n",
    "        \n",
    "        self.bQ[si][ak] = new_val\n",
    "        return old_val\n",
    "    def updateBestV(self,si):\n",
    "        old_val = self.bV[si]\n",
    "        new_val = np.max([\n",
    "            self.mdp.R.getByAction(si,ak) + self.mdp.gamma * np.sum([\n",
    "                    self.mdp.P.getByAction(si,sj,ak) * self.bV[sj] for sj in range(self.mdp.numS)\n",
    "                ])\n",
    "            for ak in range(self.mdp.numA)\n",
    "        ])\n",
    "        \n",
    "        self.bV[si] = new_val\n",
    "        return old_val\n",
    "    def updatePolicyWithV(self,si):\n",
    "        val_list = np.array([\n",
    "            self.mdp.R.getByAction(si,al) + self.mdp.gamma * np.sum([\n",
    "                self.mdp.P.getByAction(si,sj,al) * self.V[sj] for sj in range(self.mdp.numS)\n",
    "            ])\n",
    "            for al in range(self.mdp.numA)\n",
    "        ])\n",
    "        B = []\n",
    "        max_val = np.max(val_list)\n",
    "        \n",
    "        for a,v in enumerate(val_list):\n",
    "            if v == max_val:\n",
    "                B.append(a)\n",
    "        \n",
    "        for ak in range(self.mdp.numA):\n",
    "            if ak in B:\n",
    "                self.mdp.pi.update(si,ak,1./len(B))\n",
    "            else:\n",
    "                self.mdp.pi.update(si,ak,0.0)\n",
    "    def updatePolicyWithbV(self,si):\n",
    "        val_list = np.array([\n",
    "            self.mdp.R.getByAction(si,al) + self.mdp.gamma * np.sum([\n",
    "                self.mdp.P.getByAction(si,sj,al) * self.bV[sj] for sj in range(self.mdp.numS)\n",
    "            ])\n",
    "            for al in range(self.mdp.numA)\n",
    "        ])\n",
    "        B = []\n",
    "        max_val = np.max(val_list)\n",
    "        \n",
    "        for a,v in enumerate(val_list):\n",
    "            if v == max_val:\n",
    "                B.append(a)\n",
    "        \n",
    "        for ak in range(self.mdp.numA):\n",
    "            if ak in B:\n",
    "                self.mdp.pi.update(si,ak,1./len(B))\n",
    "            else:\n",
    "                self.mdp.pi.update(si,ak,0.0)\n",
    "    def getQ(self,si,ak):\n",
    "        return self.Q[si][ak]\n",
    "    def getV(self,si):\n",
    "        return self.V[si]\n",
    "    def getbQ(self,si,ak):\n",
    "        return self.bQ[si][ak]\n",
    "    def getbV(self,si):\n",
    "        return self.bV[si]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-doubt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "embedded-dakota",
   "metadata": {},
   "source": [
    "## Step5: Build Modeling Policy using Policy-Iterative Dynamic Programming Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exempt-specialist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## init data structure\n",
    "seed = 42\n",
    "env = GridTreasureEnv(render_mode=\"hide\",size=5)\n",
    "numS = env.numS\n",
    "numA = env.numA\n",
    "\n",
    "policy = Policy(numS,numA,uniform=True)\n",
    "mdp = MDP(numS,numA,policy,gamma=1)\n",
    "be = BE(mdp)\n",
    "\n",
    "# random policy at first\n",
    "policy.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "magnetic-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build policy-iterative dynamic programming algorithm\n",
    "def policyDP(env,mdp,be,seed=42,threshold=1e-5,log=False):\n",
    "    observation = env.reset(seed=seed, return_info=False)\n",
    "    target_loc = observation[\"target\"]\n",
    "    agent_loc = observation[\"agent\"]\n",
    "    \n",
    "    # init prob transformer P[si->sj][ak] = 1 if si+ak => sj else 0\n",
    "    # except when si == target_state, P[si->sj][ak] = 1 if sj == si else 0\n",
    "    for si in range(mdp.numS):\n",
    "        for ak in range(mdp.numA):\n",
    "            loci = env.state_to_loc(si)\n",
    "            if (target_loc == loci).all():\n",
    "                mdp.P.update(si,si,ak,1.)\n",
    "            else:\n",
    "                dirtk = env._action_to_direction[ak]\n",
    "                loci = env.state_to_loc(si)\n",
    "                locj = np.clip(loci+dirtk,0,env.size-1)\n",
    "                mdp.P.update(si,env.loc_to_state(locj),ak,1.)\n",
    "                    \n",
    "    # init reward R[si][ak] = 0 if si == target_loc else -1\n",
    "    for si in range(mdp.numS):\n",
    "        for ak in range(mdp.numA):\n",
    "            loci = env.state_to_loc(si)\n",
    "            if (target_loc == loci).all():\n",
    "                mdp.R.update(si,ak,0)\n",
    "            else:\n",
    "                mdp.R.update(si,ak,-1)\n",
    "    \n",
    "    \n",
    "    # policy evalute-improvement\n",
    "    eval_epoch_list = []\n",
    "    while True: # update policy until it's converged\n",
    "        # policy evaluate\n",
    "        eval_epoch = 0\n",
    "        while True: # update V until it's converged\n",
    "            delta = 0\n",
    "            for si in range(mdp.numS): \n",
    "                old_val = be.updateV(si)\n",
    "                new_val = be.getV(si)\n",
    "                delta = max(delta,np.abs(new_val-old_val))   \n",
    "            eval_epoch += 1\n",
    "            if delta < threshold:\n",
    "                break\n",
    "            \n",
    "        if log:\n",
    "            eval_epoch_list.append(eval_epoch)\n",
    "            print(\"=\"*10)\n",
    "            print(\"epoch {} used {} iterations to converge V\".format(len(eval_epoch_list),eval_epoch_list[-1]))\n",
    "            print(\"The current converged V is:\\n\",be.V)\n",
    "        \n",
    "                \n",
    "        # policy improvement\n",
    "        changed = False\n",
    "        for si in range(mdp.numS): \n",
    "            old_best_action = mdp.pi.getBestAction(si)\n",
    "            be.updatePolicyWithV(si)\n",
    "            new_best_actions = mdp.pi.getBestAction(si,all_actions=True)\n",
    "            if old_best_action not in new_best_actions:\n",
    "                changed = True\n",
    "        \n",
    "        if log:\n",
    "            print(\"The curent best policy is:\\n\",mdp.pi.pi)\n",
    "                \n",
    "        if not changed: \n",
    "            if log:\n",
    "                print(\"~\"*10)\n",
    "                print(\"The best policy has converged!\")\n",
    "                print(\"~\"*10)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "built-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "epoch 1 used 293 iterations to converge V\n",
      "The current converged V is:\n",
      " [-47.408867 -44.136166 -38.454384 -32.7726   -31.499882 -46.681606\n",
      " -42.545273 -34.454407 -24.363544 -26.22718  -46.090706 -40.90892\n",
      " -28.454433   0.       -18.81812  -46.681614 -42.54528  -34.454414\n",
      " -24.36355  -26.227184 -47.408882 -44.13618  -38.4544   -32.77261\n",
      " -31.499893]\n",
      "The curent best policy is:\n",
      " [[1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]]\n",
      "==========\n",
      "epoch 2 used 6 iterations to converge V\n",
      "The current converged V is:\n",
      " [-5. -4. -3. -2. -3. -4. -3. -2. -1. -2. -3. -2. -1.  0. -1. -4. -3. -2.\n",
      " -1. -2. -5. -4. -3. -2. -3.]\n",
      "The curent best policy is:\n",
      " [[0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.5  0.5 ]]\n",
      "~~~~~~~~~~\n",
      "The best policy has converged!\n",
      "~~~~~~~~~~\n",
      "DP done!\n"
     ]
    }
   ],
   "source": [
    "# using policy-iterative dynamic programming to get best policy\n",
    "policyDP(env,mdp,be,seed=seed,log=True)\n",
    "print(\"DP done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nasty-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>v</td>\n",
       "      <td>v&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>v</td>\n",
       "      <td>v&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;v&lt;^</td>\n",
       "      <td>&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>^</td>\n",
       "      <td>&lt;^</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>^</td>\n",
       "      <td>&lt;^</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2     3   4\n",
       "0  >v  >v  >v     v  v<\n",
       "1  >v  >v  >v     v  v<\n",
       "2   >   >   >  >v<^   <\n",
       "3  >^  >^  >^     ^  <^\n",
       "4  >^  >^  >^     ^  <^"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the best policy on grid world\n",
    "import pandas as pd\n",
    "policy_grid = pd.DataFrame(np.zeros(shape=(env.size,env.size)))\n",
    "action_map = {\n",
    "    0:\">\", # right\n",
    "    1:\"v\", # up\n",
    "    2:\"<\", # left\n",
    "    3:\"^\"  # down\n",
    "}\n",
    "for si in range(mdp.numS):\n",
    "    loc = env.state_to_loc(si)\n",
    "    s = \"\"\n",
    "    for a in policy.getBestAction(si,all_actions=True):\n",
    "        s += action_map[a]\n",
    "    policy_grid.loc[loc[1],loc[0]] = s\n",
    "policy_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advisory-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== \t timestep is 0\n",
      "Observation: {'agent': array([1, 3]), 'target': array([3, 2])}\n",
      "Info: {'distance': 3.0}\n",
      "========== \t timestep is 1\n",
      "Observation: {'agent': array([2, 3]), 'target': array([3, 2])}\n",
      "Info: {'distance': 2.0}\n",
      "========== \t timestep is 2\n",
      "Observation: {'agent': array([2, 2]), 'target': array([3, 2])}\n",
      "Info: {'distance': 1.0}\n",
      "========== \t timestep is 3\n",
      "Observation: {'agent': array([3, 2]), 'target': array([3, 2])}\n",
      "Info: {'distance': 0.0}\n",
      "~~~~done!~~~~\n"
     ]
    }
   ],
   "source": [
    "# test best policy on real environment\n",
    "env = GridTreasureEnv(render_mode=\"human\",size=5)\n",
    "# init reset\n",
    "observation, info = env.reset(seed=seed, return_info=True)\n",
    "# walk with best policy\n",
    "for t in range(100):\n",
    "    agent_loc = observation[\"agent\"]\n",
    "    best_actions = policy.getBestAction(env.loc_to_state(agent_loc),all_actions=True)\n",
    "    best_action = np.random.choice(best_actions,size=1).item()\n",
    "    observation, reward, done, info = env.step(best_action)\n",
    "    \n",
    "    # log time step\n",
    "    print(\"=\"*10,\"\\t timestep is {}\".format(t))\n",
    "    print(\"Observation: {}\".format(observation))\n",
    "    print(\"Info: {}\".format(info))\n",
    "        \n",
    "\n",
    "    if done:\n",
    "        observation, info = env.reset(return_info=True)\n",
    "        print(\"~~~~done!~~~~\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-breathing",
   "metadata": {},
   "source": [
    "## Step6: Build Modeling Policy using Value-Iterative Dynamic Programming Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "humanitarian-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## init data structure\n",
    "seed = 42\n",
    "env = GridTreasureEnv(render_mode=\"hide\",size=5)\n",
    "numS = env.numS\n",
    "numA = env.numA\n",
    "\n",
    "policy = Policy(numS,numA,uniform=True)\n",
    "mdp = MDP(numS,numA,policy,gamma=1)\n",
    "be = BE(mdp)\n",
    "\n",
    "# random policy at first\n",
    "policy.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "several-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build value-iterative dynamic programming algorithm\n",
    "def valueDP(env,mdp,be,seed=42,threshold=1e-5,log=False):\n",
    "    observation = env.reset(seed=seed, return_info=False)\n",
    "    target_loc = observation[\"target\"]\n",
    "    agent_loc = observation[\"agent\"]\n",
    "    \n",
    "    # init prob transformer P[si->sj][ak] = 1 if si+ak => sj else 0\n",
    "    # except when si == target_state, P[si->sj][ak] = 1 if sj == si else 0\n",
    "    for si in range(mdp.numS):\n",
    "        for ak in range(mdp.numA):\n",
    "            loci = env.state_to_loc(si)\n",
    "            if (target_loc == loci).all():\n",
    "                mdp.P.update(si,si,ak,1.)\n",
    "            else:\n",
    "                dirtk = env._action_to_direction[ak]\n",
    "                loci = env.state_to_loc(si)\n",
    "                locj = np.clip(loci+dirtk,0,env.size-1)\n",
    "                mdp.P.update(si,env.loc_to_state(locj),ak,1.)\n",
    "                    \n",
    "    # init reward R[si][ak] = 0 if si == target_loc else -1\n",
    "    for si in range(mdp.numS):\n",
    "        for ak in range(mdp.numA):\n",
    "            loci = env.state_to_loc(si)\n",
    "            if (target_loc == loci).all():\n",
    "                mdp.R.update(si,ak,0)\n",
    "            else:\n",
    "                mdp.R.update(si,ak,-1)\n",
    "    \n",
    "    \n",
    "    # policy evalute-improvement\n",
    "    # update policy when best V is converged\n",
    "    \n",
    "    # policy evaluate\n",
    "    eval_epoch = 0\n",
    "    while True: # update best V until it's converged\n",
    "        delta = 0\n",
    "        for si in range(mdp.numS): \n",
    "            old_val = be.updateBestV(si)\n",
    "            new_val = be.getbV(si)\n",
    "            delta = max(delta,np.abs(new_val-old_val))   \n",
    "            \n",
    "        if log:\n",
    "            eval_epoch += 1\n",
    "            print(\"=\"*10)\n",
    "            print(\"epoch: {} | The current bV is:\\n\".format(eval_epoch),be.bV)\n",
    "        \n",
    "        \n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "\n",
    "    # policy improvement with best V\n",
    "    for si in range(mdp.numS): \n",
    "        be.updatePolicyWithbV(si)\n",
    "\n",
    "    if log:\n",
    "        print(\"~\"*10)\n",
    "        print(\"The best policy is:\\n\",mdp.pi.pi)\n",
    "        print(\"~\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "returning-vatican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "epoch: 1 | The current bV is:\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.]\n",
      "==========\n",
      "epoch: 2 | The current bV is:\n",
      " [-2. -2. -2. -2. -2. -2. -2. -2. -1. -2. -2. -2. -1.  0. -1. -2. -2. -2.\n",
      " -1. -2. -2. -2. -2. -2. -2.]\n",
      "==========\n",
      "epoch: 3 | The current bV is:\n",
      " [-3. -3. -3. -2. -3. -3. -3. -2. -1. -2. -3. -2. -1.  0. -1. -3. -3. -2.\n",
      " -1. -2. -3. -3. -3. -2. -3.]\n",
      "==========\n",
      "epoch: 4 | The current bV is:\n",
      " [-4. -4. -3. -2. -3. -4. -3. -2. -1. -2. -3. -2. -1.  0. -1. -4. -3. -2.\n",
      " -1. -2. -4. -4. -3. -2. -3.]\n",
      "==========\n",
      "epoch: 5 | The current bV is:\n",
      " [-5. -4. -3. -2. -3. -4. -3. -2. -1. -2. -3. -2. -1.  0. -1. -4. -3. -2.\n",
      " -1. -2. -5. -4. -3. -2. -3.]\n",
      "==========\n",
      "epoch: 6 | The current bV is:\n",
      " [-5. -4. -3. -2. -3. -4. -3. -2. -1. -2. -3. -2. -1.  0. -1. -4. -3. -2.\n",
      " -1. -2. -5. -4. -3. -2. -3.]\n",
      "~~~~~~~~~~\n",
      "The best policy is:\n",
      " [[0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.5  0.5 ]]\n",
      "~~~~~~~~~~\n",
      "DP done!\n"
     ]
    }
   ],
   "source": [
    "# using value-iterative dynamic programming to get best policy\n",
    "valueDP(env,mdp,be,seed=seed,log=True)\n",
    "print(\"DP done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "operational-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>v</td>\n",
       "      <td>v&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>&gt;v</td>\n",
       "      <td>v</td>\n",
       "      <td>v&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;v&lt;^</td>\n",
       "      <td>&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>^</td>\n",
       "      <td>&lt;^</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>&gt;^</td>\n",
       "      <td>^</td>\n",
       "      <td>&lt;^</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2     3   4\n",
       "0  >v  >v  >v     v  v<\n",
       "1  >v  >v  >v     v  v<\n",
       "2   >   >   >  >v<^   <\n",
       "3  >^  >^  >^     ^  <^\n",
       "4  >^  >^  >^     ^  <^"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the best policy on grid world\n",
    "import pandas as pd\n",
    "policy_grid = pd.DataFrame(np.zeros(shape=(env.size,env.size)))\n",
    "action_map = {\n",
    "    0:\">\", # right\n",
    "    1:\"v\", # up\n",
    "    2:\"<\", # left\n",
    "    3:\"^\"  # down\n",
    "}\n",
    "for si in range(mdp.numS):\n",
    "    loc = env.state_to_loc(si)\n",
    "    s = \"\"\n",
    "    for a in policy.getBestAction(si,all_actions=True):\n",
    "        s += action_map[a]\n",
    "    policy_grid.loc[loc[1],loc[0]] = s\n",
    "policy_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "piano-convert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== \t timestep is 0\n",
      "Observation: {'agent': array([0, 2]), 'target': array([3, 2])}\n",
      "Info: {'distance': 3.0}\n",
      "========== \t timestep is 1\n",
      "Observation: {'agent': array([1, 2]), 'target': array([3, 2])}\n",
      "Info: {'distance': 2.0}\n",
      "========== \t timestep is 2\n",
      "Observation: {'agent': array([2, 2]), 'target': array([3, 2])}\n",
      "Info: {'distance': 1.0}\n",
      "========== \t timestep is 3\n",
      "Observation: {'agent': array([3, 2]), 'target': array([3, 2])}\n",
      "Info: {'distance': 0.0}\n",
      "~~~~done!~~~~\n"
     ]
    }
   ],
   "source": [
    "# test best policy on real environment\n",
    "env = GridTreasureEnv(render_mode=\"human\",size=5)\n",
    "# init reset\n",
    "observation, info = env.reset(seed=seed, return_info=True)\n",
    "# walk with best policy\n",
    "for t in range(100):\n",
    "    agent_loc = observation[\"agent\"]\n",
    "    best_actions = policy.getBestAction(env.loc_to_state(agent_loc),all_actions=True)\n",
    "    best_action = np.random.choice(best_actions,size=1).item()\n",
    "    observation, reward, done, info = env.step(best_action)\n",
    "    \n",
    "    # log time step\n",
    "    print(\"=\"*10,\"\\t timestep is {}\".format(t))\n",
    "    print(\"Observation: {}\".format(observation))\n",
    "    print(\"Info: {}\".format(info))\n",
    "        \n",
    "\n",
    "    if done:\n",
    "        observation, info = env.reset(return_info=True)\n",
    "        print(\"~~~~done!~~~~\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-premiere",
   "metadata": {},
   "source": [
    "$\\Rightarrow 综上所述：$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-suspect",
   "metadata": {},
   "source": [
    "$\\mathrm{MDP}过程的求解方法都是基于贝尔曼方程，而对于模型已知的\\mathrm{MDP}，可以直接利用动态规划算法解贝尔曼方程$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-kenya",
   "metadata": {},
   "source": [
    "$根据使用的贝尔曼方程的形式的不同，动态规划算法又分为策略迭代和值迭代两种$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-blackberry",
   "metadata": {},
   "source": [
    "$①策略迭代的动态规划算法使用的是贝尔曼期望方程，算法整体分为两步：\\\\step1. 策略评估，即在当前策略下，通过状态值函数的迭代方程，迭代计算，直到状态值函数收敛\\\\step2. 策略改进，即根据上一步评估的状态值函数，更新最佳策略，重复执行step1,2，直到最佳策略收敛\\\\\\Rightarrow 由于两步均需要迭代，因此计算开销比较大，效率较低$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-thesis",
   "metadata": {},
   "source": [
    "$②值迭代的动态规划算法使用的是贝尔曼最优方程，算法也分为两步：\\\\step1.策略评估+改进，即在当前策略下，通过状态最优值函数的迭代方程，迭代计算，直到状态最优值函数收敛 \\\\\\;\\Rightarrow 看似是在评估，实际上最优值函数的更新就是在改进策略(选择当前使得值函数最大的行为所对应的值)，因此当其收敛时，最优策略亦收敛\\\\step2. 根据上一步评估的状态最优值函数，更新最优策略即可\\\\\\Rightarrow 仅第一步需要迭代，而且由于评估的同时亦在更新，因此迭代次数明显减小，效率较高$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-attitude",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
