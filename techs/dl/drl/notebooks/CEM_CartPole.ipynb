{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a33b068",
   "metadata": {},
   "source": [
    "## PG-based CEM method to solve CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1f766",
   "metadata": {},
   "source": [
    "$\\text{Policy Gradient}: \\min\\limits_{\\pi} \\mathcal{L} := -\\mathbb{E}_{\\tau}[Q(s,a)\\log \\pi(a|s)], \\tau=\\{(s_i,a_i,r_i)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670ac20",
   "metadata": {},
   "source": [
    "$\\text{CEM: a kind of PG methods when Q(s,a) = 1 (good episodes) or 0 (filtered epsiodes)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b644900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "$\\text{Policy Gradient}: \\min\\limits_{\\pi} \\mathcal{L} := -\\mathbb{E}_{\\tau}[Q(s,a)\\log \\pi(a|s)], \\tau=\\{(s_i,a_i,r_i)\\}$\n",
    "\n",
    "$\\text{CEM: a kind of PG methods when Q(s,a) = 1 (good episodes) or 0 (filtered epsiodes)}$import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40c6d9",
   "metadata": {},
   "source": [
    "### step1. define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66cacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfcb538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5679501e+00, -2.8996554e+38, -1.3380405e-01,  1.9239598e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the observation of CarPole-v0 is a four float array, \n",
    "# referring to the x-dim position, velocity, angle and the angle velocity\n",
    "env.observation_space.sample() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e049114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the action of CarPole-v0 is a binary discrete number 0(left) and 1(right)\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1ad5b",
   "metadata": {},
   "source": [
    "### step2. create the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab060d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions, hidden_size=128, device=\"cuda:0\"):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        self.sm = nn.Softmax(dim=1) # softmax to get the final policy prob\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        return self.hidden_layer(x) # return the action logits without softmax\n",
    "    def policy(self, obs):\n",
    "        obs_v = torch.FloatTensor(obs).unsqueeze(dim=0).to(self.device) # obs_v.shape = (1,4)\n",
    "        act_prob_v = self.sm(self(obs_v)) # action prob_v.shape = (1,4)\n",
    "        act_probs = act_prob_v.data.cpu().numpy()[0] # action prob array\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) # sample the action from the pdf\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "931a3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d9f9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden_layer): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       "  (sm): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39f759",
   "metadata": {},
   "source": [
    "### step3. collect the epsiode batch to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8593d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(env, net, batch_size=16):\n",
    "    # init\n",
    "    batch = [] # traing batch\n",
    "    epi_reward = 0.0 # the total reward for certain episode\n",
    "    epi_steps = [] # each step in certain episode is a tuple for (obs, act)\n",
    "    obs, _ = env.reset() # start the env\n",
    "    # loop one episode\n",
    "    while True:\n",
    "        # get action from the policy\n",
    "        action = net.policy(obs)\n",
    "        # step the env using the action\n",
    "        # and get the next observation, immediate reward, and is_done flag\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        # accumulate the episode reward\n",
    "        epi_reward += reward\n",
    "        # append the episode steps\n",
    "        epi_steps.append((obs, action))\n",
    "        # check if the episode is done\n",
    "        if terminated or truncated:\n",
    "            # load the batch with this episode\n",
    "            batch.append({\n",
    "                \"epi_reward\": epi_reward,\n",
    "                \"epi_steps\": epi_steps\n",
    "            })\n",
    "            # if the batch is full, yield it\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "            # reinit for the next episode\n",
    "            epi_reward = 0.0 \n",
    "            epi_steps = []\n",
    "            next_obs,_ = env.reset()\n",
    "        # refresh the observation for next step\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5fabbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percent=70):\n",
    "    rewards = list(map(lambda e: e['epi_reward'], batch))\n",
    "    reward_bound = np.percentile(rewards, percent) # give the boundary reward at the percent\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    # filter out the episode that the reward is lower than the boundary\n",
    "    train_obs,train_act = [], []\n",
    "    for episode in batch:\n",
    "        if episode['epi_reward'] < reward_bound:\n",
    "            continue\n",
    "        # using the fine episode as training dataset\n",
    "        train_obs.extend(list(map(lambda step: step[0], episode['epi_steps'])))\n",
    "        train_act.extend(list(map(lambda step: step[1], episode['epi_steps'])))\n",
    "        train_obs_v, train_act_v = torch.FloatTensor(train_obs), torch.FloatTensor(train_act)\n",
    "    \n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf1a6d",
   "metadata": {},
   "source": [
    "### step4: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5013a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94581015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, net, loss_func, optimizer, writer, max_reward=199):\n",
    "    for iter_idx, batch in enumerate(iterate_batch(env, net)):\n",
    "        # get the training dataset\n",
    "        train_obs_v, train_act_v, reward_bound, reward_mean = filter_batch(batch)\n",
    "        train_obs_v = train_obs_v.to(device)\n",
    "        train_act_v = train_act_v.to(device)\n",
    "        # forward the policy net\n",
    "        optimizer.zero_grad()\n",
    "        action_logits_v = net(train_obs_v)\n",
    "        # compute the loss and backprop\n",
    "        loss_v = loss_func(action_logits_v, train_act_v.long())\n",
    "        loss_v.backward()\n",
    "        # optimize a step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log the training process\n",
    "        print(\"{}: loss={:.3f}, reward_mean={:.1f}, reward_bound={:.1f}\".format(\n",
    "            iter_idx, loss_v.item(), reward_mean, reward_bound\n",
    "        ))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_idx)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_idx)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_idx)\n",
    "        \n",
    "        if reward_mean > max_reward: # the policy is good enough\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50b2dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_22808\\2417605100.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  train_obs_v, train_act_v = torch.FloatTensor(train_obs), torch.FloatTensor(train_act)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.694, reward_mean=21.1, reward_bound=22.5\n",
      "1: loss=0.684, reward_mean=29.2, reward_bound=31.0\n",
      "2: loss=0.668, reward_mean=37.9, reward_bound=41.0\n",
      "3: loss=0.665, reward_mean=34.1, reward_bound=37.0\n",
      "4: loss=0.650, reward_mean=34.1, reward_bound=34.5\n",
      "5: loss=0.644, reward_mean=30.4, reward_bound=31.5\n",
      "6: loss=0.651, reward_mean=31.9, reward_bound=34.0\n",
      "7: loss=0.630, reward_mean=45.1, reward_bound=56.5\n",
      "8: loss=0.637, reward_mean=48.5, reward_bound=43.5\n",
      "9: loss=0.633, reward_mean=47.6, reward_bound=60.5\n",
      "10: loss=0.616, reward_mean=66.1, reward_bound=78.0\n",
      "11: loss=0.614, reward_mean=69.9, reward_bound=83.5\n",
      "12: loss=0.607, reward_mean=50.6, reward_bound=62.0\n",
      "13: loss=0.609, reward_mean=63.2, reward_bound=76.5\n",
      "14: loss=0.594, reward_mean=77.2, reward_bound=92.0\n",
      "15: loss=0.598, reward_mean=62.4, reward_bound=78.0\n",
      "16: loss=0.609, reward_mean=70.2, reward_bound=82.0\n",
      "17: loss=0.608, reward_mean=80.5, reward_bound=92.5\n",
      "18: loss=0.598, reward_mean=80.7, reward_bound=91.5\n",
      "19: loss=0.590, reward_mean=82.6, reward_bound=92.5\n",
      "20: loss=0.582, reward_mean=97.0, reward_bound=117.5\n",
      "21: loss=0.593, reward_mean=94.8, reward_bound=106.0\n",
      "22: loss=0.580, reward_mean=114.3, reward_bound=139.0\n",
      "23: loss=0.569, reward_mean=109.1, reward_bound=106.5\n",
      "24: loss=0.570, reward_mean=114.2, reward_bound=120.0\n",
      "25: loss=0.562, reward_mean=119.5, reward_bound=126.5\n",
      "26: loss=0.560, reward_mean=145.8, reward_bound=172.0\n",
      "27: loss=0.550, reward_mean=134.2, reward_bound=160.0\n",
      "28: loss=0.554, reward_mean=142.7, reward_bound=157.5\n",
      "29: loss=0.568, reward_mean=136.2, reward_bound=140.5\n",
      "30: loss=0.535, reward_mean=173.6, reward_bound=225.0\n",
      "31: loss=0.547, reward_mean=203.5, reward_bound=240.5\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "train(env, net, loss_func, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933e566",
   "metadata": {},
   "source": [
    "### step5: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d2e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, time_limit=1000, random_policy=False):\n",
    "    time = 0\n",
    "    env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "    obs, info = env.reset()\n",
    "    while time < time_limit:\n",
    "        if random_policy:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = net.policy(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(\"Game over with {} steps\".format(time))\n",
    "            env.close()\n",
    "            return\n",
    "        time += 1\n",
    "    env.close()\n",
    "    print(\"Time reaches limits as {} steps\".format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b25daaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over with 227 steps\n"
     ]
    }
   ],
   "source": [
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3541b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over with 22 steps\n"
     ]
    }
   ],
   "source": [
    "test(net, random_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5260c8d",
   "metadata": {},
   "source": [
    "### step6. save the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dced29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./ckpt/cem_policy_net.pt\"\n",
    "torch.save(net.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fcf09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
