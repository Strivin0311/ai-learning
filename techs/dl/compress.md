# Model Compression Strategies for Deep Neural Networks
*Here are some resources about Model Compression strategies for deep neural networks*


## Quantization


#### Searching for Low-Bit Weights in Quantized Neural Networks

paper link: [here](https://arxiv.org/pdf/2009.08695)

citation:

```bibtex
@misc{yang2020searching,
      title={Searching for Low-Bit Weights in Quantized Neural Networks}, 
      author={Zhaohui Yang and Yunhe Wang and Kai Han and Chunjing Xu and Chao Xu and Dacheng Tao and Chang Xu},
      year={2020},
      eprint={2009.08695},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
```


#### Quantizing deep convolutional networks for efficient inference: A whitepaper

paper link: [here](https://arxiv.org/pdf/1806.08342)

citation:

```bibtex
@misc{krishnamoorthi2018quantizing,
      title={Quantizing deep convolutional networks for efficient inference: A whitepaper}, 
      author={Raghuraman Krishnamoorthi},
      year={2018},
      eprint={1806.08342},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
```


## Knowledge Distillation


#### Dreaming to Distill- Data-free Knowledge Transfer via DeepInversion
paper link: [here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf)

citation: 
```bibtex
@inproceedings{yin2020dreaming,
  title={Dreaming to distill: Data-free knowledge transfer via deepinversion},
  author={Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8715--8724},
  year={2020}
}
```

#### The knowledge within- Methods for data-free model compression
paper link: [here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Haroush_The_Knowledge_Within_Methods_for_Data-Free_Model_Compression_CVPR_2020_paper.pdf)

citation: 
```bibtex
@inproceedings{haroush2020knowledge,
  title={The knowledge within: Methods for data-free model compression},
  author={Haroush, Matan and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8494--8502},
  year={2020}
}
```


#### Zero-Shot Knowledge Distillation in Deep Networks
paper link: [here](http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf)

citation: 
```bibtex
@inproceedings{nayak2019zero,
  title={Zero-shot knowledge distillation in deep networks},
  author={Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Shaj, Vaisakh and Radhakrishnan, Venkatesh Babu and Chakraborty, Anirban},
  booktitle={International Conference on Machine Learning},
  pages={4743--4751},
  year={2019},
  organization={PMLR}
}
```



## Pruning


#### SCOP: Scientific Control for Reliable Neural Network Pruning

paper link: [here](https://arxiv.org/pdf/2010.10732)

citation:

```bibtex
@misc{tang2021scop,
      title={SCOP: Scientific Control for Reliable Neural Network Pruning}, 
      author={Yehui Tang and Yunhe Wang and Yixing Xu and Dacheng Tao and Chunjing Xu and Chao Xu and Chang Xu},
      year={2021},
      eprint={2010.10732},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
```


#### AdderNet: Do We Really Need Multiplications in Deep Learning?


paper link: [here](https://arxiv.org/pdf/1912.13200)

citation:

```bibtex
@misc{chen2021addernet,
      title={AdderNet: Do We Really Need Multiplications in Deep Learning?}, 
      author={Hanting Chen and Yunhe Wang and Chunjing Xu and Boxin Shi and Chao Xu and Qi Tian and Chang Xu},
      year={2021},
      eprint={1912.13200},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
```



#### GhostNet: More Features from Cheap Operations

paper link: [here](https://arxiv.org/pdf/1911.11907)

citation:

```bibtex
@misc{han2020ghostnet,
      title={GhostNet: More Features from Cheap Operations}, 
      author={Kai Han and Yunhe Wang and Qi Tian and Jianyuan Guo and Chunjing Xu and Chang Xu},
      year={2020},
      eprint={1911.11907},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
```