# Tricks for Deep Learning
*Here're some resources about Tricks for Deep Learning*


#### Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer

paper link: [here](https://arxiv.org/pdf/2203.03466)

github link: [here](https://github.com/microsoft/mup)

citation:

```bibtex
@misc{yang2022tensorprogramsvtuning,
      title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}, 
      author={Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
      year={2022},
      eprint={2203.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.03466}, 
}
```


#### Decoupled Weight Decay Regularization

paper link: [here](https://arxiv.org/pdf/1711.05101)

citation:
```bibtex
@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### On calibration of modern neural networks

paper link: [here](http://proceedings.mlr.press/v70/guo17a/guo17a.pdf)

citation: 
```bibtex
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}
```


#### Categorical reparameterization with gumbel-softmax

paper link: [here](https://arxiv.org/pdf/1611.01144.pdf)

citation: 
```bibtex
@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}
```

#### Training deep nets with sublinear memory cost

paper link: [here](https://arxiv.org/pdf/1604.06174)

citation: 
```bibtex
@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}
```


#### Batch normalization: Accelerating deep network training by reducing internal covariate shift

paper link: [here](http://proceedings.mlr.press/v37/ioffe15.pdf)

citation: 
```bibtex
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}
```

    
#### Dropout: a simple way to prevent neural networks from overfitting

paper link: [here](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,)

citation: 
```bibtex
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
```




