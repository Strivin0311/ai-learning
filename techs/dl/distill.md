# Knowledge Distillation
*Here're some resources about Knowledge Distillation*


#### Dreaming to Distill- Data-free Knowledge Transfer via DeepInversion [`UNREAD`]
paper link: [here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf)

citation: 
```bibtex
@inproceedings{yin2020dreaming,
  title={Dreaming to distill: Data-free knowledge transfer via deepinversion},
  author={Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8715--8724},
  year={2020}
}
```

#### The knowledge within- Methods for data-free model compression [`UNREAD`]
paper link: [here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Haroush_The_Knowledge_Within_Methods_for_Data-Free_Model_Compression_CVPR_2020_paper.pdf)

citation: 
```bibtex
@inproceedings{haroush2020knowledge,
  title={The knowledge within: Methods for data-free model compression},
  author={Haroush, Matan and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8494--8502},
  year={2020}
}
```


#### Zero-Shot Knowledge Distillation in Deep Networks [`UNREAD`]
paper link: [here](http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf)

citation: 
```bibtex
@inproceedings{nayak2019zero,
  title={Zero-shot knowledge distillation in deep networks},
  author={Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Shaj, Vaisakh and Radhakrishnan, Venkatesh Babu and Chakraborty, Anirban},
  booktitle={International Conference on Machine Learning},
  pages={4743--4751},
  year={2019},
  organization={PMLR}
}
```
