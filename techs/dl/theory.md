# Theory for Deep Learning
*Here're some resources about Theory for Deep Learning*


## Papers


#### Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.19360)

citation: 
```bibtex
@article{wang2023balance,
  title={Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective},
  author={Wang, Yifei and Li, Liangchen and Yang, Jiansheng and Lin, Zhouchen and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.19360},
  year={2023}
}
```
    


#### The clock and the pizza: Two stories in mechanistic explanation of neural networks [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.17844)

citation: 
```bibtex
@article{zhong2023clock,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={arXiv preprint arXiv:2306.17844},
  year={2023}
}
```
    

#### Tracr: Compiled transformers as a laboratory for interpretability [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2301.05062.pdf)

citation: 
```bibtex
@article{lindner2023tracr,
  title={Tracr: Compiled transformers as a laboratory for interpretability},
  author={Lindner, David and Kram{\'a}r, J{\'a}nos and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
  journal={arXiv preprint arXiv:2301.05062},
  year={2023}
}
```
    


#### Fast finite width neural tangent kernel (NTK) [`READ`]

paper link: [here](https://proceedings.mlr.press/v162/novak22a/novak22a.pdf)

citation: 
```bibtex
@inproceedings{novak2022fast,
  title={Fast finite width neural tangent kernel},
  author={Novak, Roman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  booktitle={International Conference on Machine Learning},
  pages={17018--17044},
  year={2022},
  organization={PMLR}
}
```
    

#### Understanding deep learning (still) requires rethinking generalization [`UNREAD`]

paper link: [here](https://dl.acm.org/doi/pdf/10.1145/3446776)

citation: 
```bibtex
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}
```

#### Wide neural networks of any depth evolve as linear models under gradient descent [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf)

citation: 
```bibtex
@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
```

#### Sparse Networks from Scratch: Faster Training without Losing Performance [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1907.04840.pdf)

blog link: [here](https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/)

slides link: [here](https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf)

citation: 
```bibtex
@misc{dettmers2019sparse,
      title={Sparse Networks from Scratch: Faster Training without Losing Performance}, 
      author={Tim Dettmers and Luke Zettlemoyer},
      year={2019},
      eprint={1907.04840},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```
    

#### What can neural networks reason about? [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1905.13211)

citation: 
```bibtex
@article{xu2019can,
  title={What can neural networks reason about?},
  author={Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1905.13211},
  year={2019}
}
```

#### Low-memory neural network training: A technical report [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1904.10631)

citation: 
```bibtex
@article{sohoni2019low,
  title={Low-memory neural network training: A technical report},
  author={Sohoni, Nimit S and Aberger, Christopher R and Leszczynski, Megan and Zhang, Jian and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1904.10631},
  year={2019}
}
```
    

#### Neural tangent kernel: Convergence and generalization in neural networks [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf)

citation: 
```bibtex
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
```
    
#### How does batch normalization help optimization? [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf)

citation: 
```bibtex
@article{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
```
    

#### The matrix calculus you need for deep learning [`READ`]

paper link: [here](https://arxiv.org/pdf/1802.01528.pdf)

citation: 
```bibtex
@article{parr2018matrix,
  title={The matrix calculus you need for deep learning},
  author={Parr, Terence and Howard, Jeremy},
  journal={arXiv preprint arXiv:1802.01528},
  year={2018}
}
```
    
#### Automatic differentiation in machine learning: a survey [`UNREAD`]

paper link: [here](https://www.jmlr.org/papers/volume18/17-468/17-468.pdf)

citation: 
```bibtex
@article{baydin2018automatic,
  title={Automatic differentiation in machine learning: a survey},
  author={Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  journal={Journal of Marchine Learning Research},
  volume={18},
  pages={1--43},
  year={2018},
  publisher={Microtome Publishing}
}
```

#### Reviving and improving recurrent back-propagation [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v80/liao18c/liao18c.pdf)

citation: 
```bibtex
@inproceedings{liao2018reviving,
  title={Reviving and improving recurrent back-propagation},
  author={Liao, Renjie and Xiong, Yuwen and Fetaya, Ethan and Zhang, Lisa and Yoon, KiJung and Pitkow, Xaq and Urtasun, Raquel and Zemel, Richard},
  booktitle={International Conference on Machine Learning},
  pages={3082--3091},
  year={2018},
  organization={PMLR}
}
```
     


#### To understand deep learning we need to understand kernel learning [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf)

citation: 
```bibtex
@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}
```
    

#### Deep neural networks as gaussian processes [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1711.00165)

citation: 
```bibtex
@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}
```
  

#### Grad-cam: Visual explanations from deep networks via gradient-based localization [`READ`]

paper link: [here](http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)

citation: 
```bibtex
@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}
``` 
    

#### Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (PReLU / MSRA) [`UNREAD`]

paper link: [here](https://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)

citation: 
```bibtex
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
```
    

#### Stochastic backpropagation and approximate inference in deep generative models [`READ`]

paper link: [here](http://proceedings.mlr.press/v32/rezende14.pdf)

citation: 
```bibtex
@inproceedings{rezende2014stochastic,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1278--1286},
  year={2014},
  organization={PMLR}
}
```


#### Deep Networks With Large Output Spaces (Infrequent Normalisation) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1412.7479)

citation: 
```bibtex
@article{vijayanarasimhan2014deep,
  title={Deep networks with large output spaces},
  author={Vijayanarasimhan, Sudheendra and Shlens, Jonathon and Monga, Rajat and Yagnik, Jay},
  journal={arXiv preprint arXiv:1412.7479},
  year={2014}
}
```
    
    

#### Understanding the exploding gradient problem [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1211.5063v1.pdf)

citation: 
```bibtex
@article{pascanu2012understanding,
  title={Understanding the exploding gradient problem},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal={CoRR, abs/1211.5063},
  volume={2},
  number={417},
  pages={1},
  year={2012}
}
```
    


#### Understanding the difficulty of training deep feedforward neural networks (Xavier) [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

citation: 
```bibtex
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
```
    
    

## TextBooks

#### Deep Learning (Flower Book) [`READ`]
book link: [here](https://pan.baidu.com/s/1PDcCj4JiWsRlEStaHPchGA), with extraction code: `ya9y`