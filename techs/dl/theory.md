# Theory for Deep Learning
*Here's some resources about Theory for Deep Learning*


## TextBooks

#### Deep Learning (Flower Book) [`READ`]
book link: [here](https://pan.baidu.com/s/1PDcCj4JiWsRlEStaHPchGA), with extraction code: `ya9y`



## Papers

#### Fast finite width neural tangent kernel [`READ`]

paper link: [here](https://proceedings.mlr.press/v162/novak22a/novak22a.pdf)

citation: 
```bibtex
@inproceedings{novak2022fast,
  title={Fast finite width neural tangent kernel},
  author={Novak, Roman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  booktitle={International Conference on Machine Learning},
  pages={17018--17044},
  year={2022},
  organization={PMLR}
}
```
    

#### Understanding deep learning (still) requires rethinking generalization [`UNREAD`]

paper link: [here](https://dl.acm.org/doi/pdf/10.1145/3446776)

citation: 
```bibtex
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}
```

#### Wide neural networks of any depth evolve as linear models under gradient descent [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf)

citation: 
```bibtex
@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
```

#### Neural tangent kernel: Convergence and generalization in neural networks [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf)

citation: 
```bibtex
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
```
    
#### How does batch normalization help optimization? [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf)

citation: 
```bibtex
@article{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
```
    

#### The matrix calculus you need for deep learning [`READ`]

paper link: [here](https://arxiv.org/pdf/1802.01528.pdf)

citation: 
```bibtex
@article{parr2018matrix,
  title={The matrix calculus you need for deep learning},
  author={Parr, Terence and Howard, Jeremy},
  journal={arXiv preprint arXiv:1802.01528},
  year={2018}
}
```
    
#### Automatic differentiation in machine learning: a survey [`UNREAD`]

paper link: [here](https://www.jmlr.org/papers/volume18/17-468/17-468.pdf)

citation: 
```bibtex
@article{baydin2018automatic,
  title={Automatic differentiation in machine learning: a survey},
  author={Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  journal={Journal of Marchine Learning Research},
  volume={18},
  pages={1--43},
  year={2018},
  publisher={Microtome Publishing}
}
```

#### Reviving and improving recurrent back-propagation [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v80/liao18c/liao18c.pdf)

citation: 
```bibtex
@inproceedings{liao2018reviving,
  title={Reviving and improving recurrent back-propagation},
  author={Liao, Renjie and Xiong, Yuwen and Fetaya, Ethan and Zhang, Lisa and Yoon, KiJung and Pitkow, Xaq and Urtasun, Raquel and Zemel, Richard},
  booktitle={International Conference on Machine Learning},
  pages={3082--3091},
  year={2018},
  organization={PMLR}
}
```
     


#### To understand deep learning we need to understand kernel learning [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf)

citation: 
```bibtex
@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}
```
    

#### Deep neural networks as gaussian processes [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1711.00165)

citation: 
```bibtex
@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}
```
    

#### On calibration of modern neural networks [`READ`]

paper link: [here](http://proceedings.mlr.press/v70/guo17a/guo17a.pdf)

citation: 
```bibtex
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}
```

#### Grad-cam: Visual explanations from deep networks via gradient-based localization [`READ`]

paper link: [here](http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)

citation: 
```bibtex
@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}
```

#### Categorical reparameterization with gumbel-softmax [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1611.01144.pdf%20http://arxiv.org/)

citation: 
```bibtex
@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}
```
    
    

#### Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (PReLU / MSRA) [`UNREAD`]

paper link: [here](https://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)

citation: 
```bibtex
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
```
    
#### Batch normalization: Accelerating deep network training by reducing internal covariate shift [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v37/ioffe15.pdf)

citation: 
```bibtex
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}
```

    
#### Dropout: a simple way to prevent neural networks from overfitting [`UNREAD`]

paper link: [here](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,)

citation: 
```bibtex
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
```

#### Stochastic backpropagation and approximate inference in deep generative models [`READ`]

paper link: [here](http://proceedings.mlr.press/v32/rezende14.pdf)

citation: 
```bibtex
@inproceedings{rezende2014stochastic,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1278--1286},
  year={2014},
  organization={PMLR}
}
```
    
    

#### Understanding the exploding gradient problem [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1211.5063v1.pdf)

citation: 
```bibtex
@article{pascanu2012understanding,
  title={Understanding the exploding gradient problem},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal={CoRR, abs/1211.5063},
  volume={2},
  number={417},
  pages={1},
  year={2012}
}
```
    


#### Understanding the difficulty of training deep feedforward neural networks (Xavier) [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

citation: 
```bibtex
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
```
    
    
