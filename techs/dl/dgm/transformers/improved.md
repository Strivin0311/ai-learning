# Improved Transformers
*Here're some resources about Improved Transformers*


#### RWKV: Reinventing RNNs for the Transformer Era

paper link: [here](https://arxiv.org/pdf/2305.13048)

citation: 
```bibtex
@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}
```


#### A Study on ReLU and Softmax in Transformer

paper link: [here](https://arxiv.org/pdf/2302.06461)

citation: 
```bibtex
@article{shen2023study,
  title={A Study on ReLU and Softmax in Transformer},
  author={Shen, Kai and Guo, Junliang and Tan, Xu and Tang, Siliang and Wang, Rui and Bian, Jiang},
  journal={arXiv preprint arXiv:2302.06461},
  year={2023}
}
```
    


#### Meta-transformer: A unified framework for multimodal learning

paper link: [here](https://arxiv.org/pdf/2307.10802)

citation: 
```bibtex
@article{zhang2023meta,
  title={Meta-transformer: A unified framework for multimodal learning},
  author={Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2307.10802},
  year={2023}
}
```

#### Perceiver io: A general architecture for structured inputs & outputs

paper link: [here](https://arxiv.org/pdf/2107.14795.pdf)

citation: 
```bibtex
@article{jaegle2021perceiver,
  title={Perceiver io: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}
```
    
    

#### Pay attention to mlps

paper link: [here](https://proceedings.neurips.cc/paper/2021/file/4cc05b35c2f937c5bd9e7d41d3686fff-Paper.pdf)

citation: 
```bibtex
@article{liu2021pay,
  title={Pay attention to mlps},
  author={Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9204--9215},
  year={2021}
}
```

#### Glu variants improve transformer

paper link: [here](https://arxiv.org/pdf/2002.05202.pdf)

citation: 
```bibtex
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
```
       
    

#### The evolved transformer

paper link: [here](http://proceedings.mlr.press/v97/so19a/so19a.pdf)

citation: 
```bibtex
@inproceedings{so2019evolved,
  title={The evolved transformer},
  author={So, David and Le, Quoc and Liang, Chen},
  booktitle={International conference on machine learning},
  pages={5877--5886},
  year={2019},
  organization={PMLR}
}
```
    


#### Convolutional self-attention networks

paper link: [here](https://arxiv.org/pdf/1904.03107)

citation: 
```bibtex
@article{yang2019convolutional,
  title={Convolutional self-attention networks},
  author={Yang, Baosong and Wang, Longyue and Wong, Derek and Chao, Lidia S and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1904.03107},
  year={2019}
}
```


#### Modeling recurrence for transformer

paper link: [here](https://arxiv.org/pdf/1904.03092)

citation: 
```bibtex
@article{hao2019modeling,
  title={Modeling recurrence for transformer},
  author={Hao, Jie and Wang, Xing and Yang, Baosong and Wang, Longyue and Zhang, Jinfeng and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1904.03092},
  year={2019}
}
```
    

#### Modeling localness for self-attention networks

paper link: [here](https://arxiv.org/pdf/1810.10182)

citation: 
```bibtex
@article{yang2018modeling,
  title={Modeling localness for self-attention networks},
  author={Yang, Baosong and Tu, Zhaopeng and Wong, Derek F and Meng, Fandong and Chao, Lidia S and Zhang, Tong},
  journal={arXiv preprint arXiv:1810.10182},
  year={2018}
}
```
    
    