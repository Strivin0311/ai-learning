# Theory on Transformers
*Here're some resources about Theory on Transformers, like scaling laws*


#### Do we need Attention? - Linear RNNs and State Space Models (SSMs) for NLP

slide link: [here](https://github.com/srush/do-we-need-attention/blob/main/DoWeNeedAttention.pdf)

citation:
```bibtex
@article{rush2023do,
  title={Do we need Attention? - Linear RNNs and State Space Models (SSMs) for NLP},
  author={sasha Rush},
  journal={Hugging Face},
  year={2023},
  url={https://www.youtube.com/watch?v=dKJEpOtVgXc}
}
```


#### Scaling Data-Constrained Language Models

paper link: [here](https://arxiv.org/pdf/2305.16264)

citation: 
```bibtex
@article{muennighoff2023scaling,
  title={Scaling Data-Constrained Language Models},
  author={Muennighoff, Niklas and Rush, Alexander M and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  journal={arXiv preprint arXiv:2305.16264},
  year={2023}
}
```

#### Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models

paper link: [here](https://arxiv.org/pdf/2305.12827)

citation: 
```bibtex
@article{ortiz2023task,
  title={Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models},
  author={Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal},
  journal={arXiv preprint arXiv:2305.12827},
  year={2023}
}
```
    

#### Attention is not all you need: Pure attention loses rank doubly exponentially with depth

paper link: [here](http://proceedings.mlr.press/v139/dong21a/dong21a.pdf)

citation: 
```bibtex
@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}
```

#### Data movement is all you need: A case study on optimizing transformers

paper link: [here](https://proceedings.mlsys.org/paper_files/paper/2021/file/bc86e95606a6392f51f95a8de106728d-Paper.pdf)

citation: 
```bibtex
@article{ivanov2021data,
  title={Data movement is all you need: A case study on optimizing transformers},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={711--732},
  year={2021}
}
```



#### Understanding the difficulty of training transformers

paper link: [here](https://arxiv.org/pdf/2004.08249)

citation: 
```bibtex
@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}
```
    
   

#### Low-rank bottleneck in multi-head attention models

paper link: [here](http://proceedings.mlr.press/v119/bhojanapalli20a/bhojanapalli20a.pdf)

citation: 
```bibtex
@inproceedings{bhojanapalli2020low,
  title={Low-rank bottleneck in multi-head attention models},
  author={Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International conference on machine learning},
  pages={864--873},
  year={2020},
  organization={PMLR}
}
```
    
    

#### A generalization of transformer networks to graphs

paper link: [here](https://arxiv.org/pdf/2012.09699)

citation: 
```bibtex
@article{dwivedi2020generalization,
  title={A generalization of transformer networks to graphs},
  author={Dwivedi, Vijay Prakash and Bresson, Xavier},
  journal={arXiv preprint arXiv:2012.09699},
  year={2020}
}
```

#### Scaling laws for autoregressive generative modeling

paper link: [here](https://arxiv.org/pdf/2010.14701)

citation: 
```bibtex
@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}
```


#### Scaling laws for neural language models

paper link: [here](https://arxiv.org/pdf/2001.08361.pdf)

citation: 
```bibtex
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
```
    
    

    
#### Are transformers universal approximators of sequence-to-sequence functions?

paper link: [here](https://arxiv.org/pdf/1912.10077)

citation: 
```bibtex
@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}
```

#### On the turing completeness of modern neural network architectures

paper link: [here](https://arxiv.org/pdf/1901.03429)

citation: 
```bibtex
@article{perez2019turing,
  title={On the turing completeness of modern neural network architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}
```
    
    

#### Transformer dissection: a unified understanding of transformer's attention via the lens of kernel

paper link: [here](https://arxiv.org/pdf/1908.11775)

citation: 
```bibtex
@article{tsai2019transformer,
  title={Transformer dissection: a unified understanding of transformer's attention via the lens of kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}
```


#### Online normalizer calculation for softmax

paper link: [here](https://arxiv.org/pdf/1805.02867)

citation: 
```bibtex
@article{milakov2018online,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}
```
     