# Survey on Transformers
*Here're some resources about Survey on Transformers*


#### Multimodal learning with transformers: A survey

paper link: [here](https://ieeexplore.ieee.org/iel7/34/4359286/10123038.pdf)

citation: 
```bibtex
@article{xu2023multimodal,
  title={Multimodal learning with transformers: A survey},
  author={Xu, Peng and Zhu, Xiatian and Clifton, David A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}
```
    


#### Video transformers: A survey

paper link: [here](https://arxiv.org/pdf/2201.05991)

citation: 
```bibtex
@article{selva2023video,
  title={Video transformers: A survey},
  author={Selva, Javier and Johansen, Anders S and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B and Clap{\'e}s, Albert},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}
```
    


#### Transformers in time series: A survey

paper link: [here](https://arxiv.org/pdf/2202.07125)

citation: 
```bibtex
@article{wen2022transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}
```

#### A survey of transformers

paper link: [here](https://www.sciencedirect.com/science/article/pii/S2666651022000146)

citation: 
```bibtex
@article{lin2022survey,
  title={A survey of transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI Open},
  year={2022},
  publisher={Elsevier}
}
```


#### On word embeddings - Part 2: Approximating the Softmax

blog link: [here](https://www.ruder.io/word-embeddings-softmax/)

citation:
```bibtex
@misc{ruder2016wordembeddingspart2,
  author = {Ruder, Sebastian},
  title = {{On word embeddings - Part 2: Approximating the Softmax}},
  year = {2016},
  howpublished = {\url{http://ruder.io/word-embeddings-softmax}},
}
```
     