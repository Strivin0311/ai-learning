# Gradient Descent Optimization
*Here's some resources about Gradient Descending*

## Paper

#### A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.18988)

citation: 
```bibtex
@article{curth2023u,
  title={A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning},
  author={Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2310.18988},
  year={2023}
}
```

#### Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2307.11007)

citation: 
```bibtex
@article{wen2023sharpness,
  title={Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  journal={arXiv preprint arXiv:2307.11007},
  year={2023}
}
```
    

#### Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows [`UNREAD`]

paper link: [here](https://inria.hal.science/hal-04150576/file/2307.00144.pdf)

citation: 
```bibtex
@article{marcotte2023abide,
  title={Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows},
  author={Marcotte, Sibylle and Gribonval, R{\'e}mi and Peyr{\'e}, Gabriel},
  year={2023}
}
```


#### Block Coordinate Descent on Smooth Manifolds: Convergence Theory and Twenty-One Examples [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.14744.pdf)

citation:
```bibtex
@misc{peng2023block,
      title={Block Coordinate Descent on Smooth Manifolds: Convergence Theory and Twenty-One Examples}, 
      author={Liangzu Peng and Ren√© Vidal},
      year={2023},
      eprint={2305.14744},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
```
    
#### Convergence of Stochastic Gradient Descent for PCA [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v48/shamirb16.pdf)

citation: 
```bibtex
@inproceedings{shamir2016convergence,
  title={Convergence of stochastic gradient descent for PCA},
  author={Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={257--265},
  year={2016},
  organization={PMLR}
}
```


#### An overview of gradient descent optimization algorithms [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1609.04747)

citation: 
```bibtex
@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}
```

#### Monte Carlo Sampling-Based Methods for Stochastic Optimization [`UNREAD`]
paper link: [here](https://optimization-online.org/wp-content/uploads/2013/06/3920.pdf)

citation: 
```bibtex
@article{homem2014monte,
  title={Monte Carlo sampling-based methods for stochastic optimization},
  author={Homem-de-Mello, Tito and Bayraksan, G{\"u}zin},
  journal={Surveys in Operations Research and Management Science},
  volume={19},
  number={1},
  pages={56--85},
  year={2014},
  publisher={Elsevier}
}
```

#### Adaptive Subgradient Methods for Online Learning and Stochastic Optimization [`UNREAD`]
paper link: [here](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)

citation: 
```bibtex
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
```

#### AdaDelta- An Adaptive Learning Rate Method [`UNREAD`]
paper link: [here](https://arxiv.org/pdf/1212.5701)

citation: 
```bibtex
@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
```

#### Adam- A Method for Stochastic Optimization [`UNREAD`]
paper link: [here](https://arxiv.org/pdf/1412.6980.pdf%5D)

citation: 
```bibtex
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
```

#### AdamW - Decoupled Weight Decay Regularization [`UNREAD`]
paper link: [here](https://arxiv.org/pdf/1711.05101)

citation: 
```bibtex
@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


## Lectures

### Dual Ascent [`UNREAD`]

slide link: [here](https://www.stat.cmu.edu/~ryantibs/convexopt-F18/lectures/dual-ascent.pdf)

#### Dual decomposition and dual algorithms [`UNREAD`]

slide link: [here](https://class.ece.uw.edu/546/2016spr/lectures/dualdecomp.pdf)






