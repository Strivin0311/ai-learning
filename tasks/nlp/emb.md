# Word Embedding
*Here's some resources about Word Embedding*
    


#### Skip-thought vectors [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper/2015/file/f442d33fa06832082290ad8544a8da27-Paper.pdf)

citation: 
```bibtex
@article{kiros2015skip,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Russ R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
```

#### Glove: Global vectors for word representation [`UNREAD`]

paper link: [here](https://aclanthology.org/D14-1162.pdf)

citation: 
```bibtex
@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}
```

#### Distributed representations of words and phrases and their compositionality [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

citation: 
```bibtex
@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
```
    

#### Efficient estimation of word representations in vector space (Word2Vec) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93)

citation: 
```bibtex
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
```
    